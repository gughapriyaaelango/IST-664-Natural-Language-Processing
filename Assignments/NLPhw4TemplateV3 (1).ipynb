{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#IST664/CIS668 - Homework 4 Template\n",
        "\n",
        "##My name: __________\n",
        "\n",
        "By adding your name to the space above, you attest that this work is all your own, except in those code and text blocks where you have given attribution to another author. You do not need to provide attribution for code copied from the labs or exercises for this class.\n",
        "\n",
        "Sometimes it is helpful to discuss the homework with other members of the class. This is fine as long as you do not share code. If you collaborated with one or more individuals, list their names here:\n",
        "\n",
        "###My collaborators: __________\n",
        "\n",
        "For this homework, you will be processing a dataset that contains a series of sentences, each of which is separated by a semicolon. The goal for the homework is to develop a matrix representation of the relationships among these sentences that can be processed by a CNN to make predictions. \n",
        "\n",
        "You will use a pre-trained sentence vectorization model to produce sentence embeddings for each sentence and then compute a square matrix of cosine similarities among the sentences. This matrix will serve as the input to the CNN predictive models.\n",
        "\n",
        "This template helps to pre-process the data with the following steps:\n",
        "\n",
        "1. Parse the data into sentences.\n",
        "2. Develop a vector summary for each sentence.\n",
        "3. Create a matrix of similarities among the sentences.\n",
        "4. Pad the matrix to a common (square) size. This will be the input to the CNN.\n",
        "5. Convert input matrices and output vectors into tensors.\n"
      ],
      "metadata": {
        "id": "qaLVwC1M-uOD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCpdXBqw-tMJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# Read in the data from Github\n",
        "url = \"https://raw.githubusercontent.com/jmstanto/ist664/main/paracoh.csv\"\n",
        "\n",
        "sentDB = pd.read_csv(url)\n",
        "\n",
        "sentDB.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentDB # Preview the data"
      ],
      "metadata": {
        "id": "RHBy9gkm_PPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The text field contains several sentences harvested from Wikipedia articles and separated by semicolons. The polys field is a readability index that is computed by looking at the ratio of polysyllabic words to total word count. cl is the Coleman-Liau index, another measure of readability that takes into account word length and sentence length.\n",
        "\n",
        "Both polys and cl are insensitive to the order of sentences in a paragraph and therefore do not measure paragraph coherence. In contrast, the incoh score indexes the number of ordering substitutions in a paragraph. So 0 indicates that the sentences are in their original order and 1 indicates that one sentence is in the wrong place, etc.\n",
        "\n",
        "The technique you are using from this lab, of capturing a matrix of similarity scores among sentences in a paragraph and analyzing it with CNN, should theoretically be able to detect when the sentences appear out of order in a paragraph."
      ],
      "metadata": {
        "id": "JmV4JCxdqsDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# These are the three metrics you will predict\n",
        "print(type(sentDB[\"polys\"])) # The polysylabbic score\n",
        "print(type(sentDB[\"cl\"])) # The Coleman-Liau Index\n",
        "print(type(sentDB[\"incoh\"])) # The number of sentence ordering incoherencies"
      ],
      "metadata": {
        "id": "q3FXF3N6bwYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1**\n",
        "\n",
        "Plot histograms that show the shape of the distribution for each of the three outcome variables. You don't need to take any action if the distributions look unusual, but doing this kind of diagnosis is an important part of becoming oriented to any data science problem."
      ],
      "metadata": {
        "id": "iwfHCI_iU0PL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HW4T1a\n",
        "# Task 1: Produce histograms for each the three metric outcome variables\n",
        "#"
      ],
      "metadata": {
        "id": "EOAVC0vxccNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How big does our padded (square) input matrix need to be?\n",
        "max_mat_size = max([len(sent.split(\";\")) for sent in sentDB[\"text\"]])\n",
        "max_mat_size"
      ],
      "metadata": {
        "id": "IyqsJaI-AZFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2**\n",
        "\n",
        "Show a few of the texts from the dataset. Note the semicolons that separate the sentences. Each row of the dataset has a set of sentences – the number of sentences varies per row."
      ],
      "metadata": {
        "id": "YFaPNXXRVn_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HW4T2a\n",
        "# Task 2: Examine a few of the texts.\n",
        "#\n"
      ],
      "metadata": {
        "id": "SwSHxs3l77yY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will need the library for loading sentence transformers\n",
        "# This generates a lot of output, but should run pretty fast.\n",
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "WccijZZ4_vSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this next step you have the opportunity to select one of three sentence vectorizers. These vary in terms of their dimensionality. The cosine similarities that you generate to measure the similarity of sentences will be affected by which model you choose."
      ],
      "metadata": {
        "id": "DnsjI_SCsM9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Task 3: Choose a Pretrained Sentence Summarizer\n",
        "\n",
        "model_name = 'Six Level Mini-LM V2 (d=384)'  #@param [\"Six Level Mini-LM V2 (d=384)\", \"Multilingual Sentence BERT (d=512)\", \"Multi-QA MPnet (d=768)\"]\n",
        "\n",
        "map_name_to_handle = {\n",
        "    'Six Level Mini-LM V2 (d=384)':\n",
        "        'sentence-transformers/all-MiniLM-L6-v2',\n",
        "    'Multilingual Sentence BERT (d=512)':\n",
        "        'sentence-transformers/distiluse-base-multilingual-cased-v2',\n",
        "    'Multi-QA MPnet (d=768)':\n",
        "        'sentence-transformers/multi-qa-mpnet-base-dot-v1'\n",
        "    \n",
        "}\n",
        "\n",
        "\n",
        "my_transformer = map_name_to_handle[model_name]\n",
        "\n",
        "print(f'Sentence model selected           : {my_transformer}')\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "kJu8WxNkWDc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now load the pre-trained sentence transformer, based on your selection above.\n",
        "# This downloads a lot of data to your virtual machine and takes half a minute or so.\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "\n",
        "model = SentenceTransformer(my_transformer)"
      ],
      "metadata": {
        "id": "DK5iGreGACVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This defines a function that takes an input matrix and puts it in the\n",
        "# upper left corner of a padded, standard-sized matrix\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def pad_matrix(in_mat, mat_size):\n",
        "\n",
        "  ret_mat = np.zeros(shape=(mat_size, mat_size))\n",
        "\n",
        "  # By using Python indices, we can target the upper left subset\n",
        "  # of the return matrix in the same shape as the input matrix.\n",
        "  ret_mat[0:in_mat.shape[0] , 0:in_mat.shape[1] ] = in_mat\n",
        "  \n",
        "  return(ret_mat)"
      ],
      "metadata": {
        "id": "VoL41bDjBzEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the padder\n",
        "M = np.arange(3*3).reshape((3,3))\n",
        "print(M)\n",
        "\n",
        "pad_matrix(M, 5)"
      ],
      "metadata": {
        "id": "EjyPxfO3E6eZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HWT3a\n",
        "# Exercise: Test the padder using a 4x4 input matrix that needs to be padded to 6x6\n",
        "#\n"
      ],
      "metadata": {
        "id": "AsvlFIsdskSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the individual texts: Computational time depends on the sentence summarizer selected,\n",
        "# but could take 15-20 minutes for nearly 5000 instances.\n",
        "from sentence_transformers.util import cos_sim\n",
        "\n",
        "mat_list = []\n",
        "\n",
        "for text in sentDB[\"text\"]:\n",
        "  items = text.split(\";\")\n",
        "  vect_list = model.encode(items)\n",
        "  sim_matrix = cos_sim(vect_list, vect_list)\n",
        "  mat_list.append(pad_matrix(sim_matrix, max_mat_size))"
      ],
      "metadata": {
        "id": "savDeA4L_Bhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Should have the same number of matrices as rows in the pandas df\n",
        "len(mat_list)"
      ],
      "metadata": {
        "id": "BD7LLu0CBukh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HW4T3b\n",
        "# (End of) Task 3: Review one of the padded matrices. Comment on what you see.\n",
        "#\n"
      ],
      "metadata": {
        "id": "bZebSsJnBxli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 4**\n",
        "\n",
        "Set up a CNN model to process the similarity matrices. The goal is to train models that can predict each of the three measures of quality using the similarity matrices as input. Use the same architecture for all three models. Don't forget that all three of these outcome variables are interval/metric data. You should therefore make the final layer of the model a single linear activation node. This is not the only possible option, but it is the most sensible one. Make sure to show a model summary that shows the layers and the shapes of data flowing between the layers."
      ],
      "metadata": {
        "id": "1EG8J3Zfb5lZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First set up testing and training splits that will be used by each of the three models\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Do random splits for testing and training\n",
        "PX_train, PX_test, Py_train, Py_test = train_test_split(mat_list, sentDB[\"polys\"], test_size=0.33, random_state=42)\n",
        "CX_train, CX_test, Cy_train, Cy_test = train_test_split(mat_list, sentDB[\"cl\"], test_size=0.33, random_state=42)\n",
        "IX_train, IX_test, Iy_train, Iy_test = train_test_split(mat_list, sentDB[\"incoh\"], test_size=0.33, random_state=42)\n",
        "\n",
        "# Convert to tensors for presenting to TF\n",
        "PX_train_tensor = tf.convert_to_tensor(PX_train)\n",
        "PX_test_tensor = tf.convert_to_tensor(PX_test)\n",
        "Py_train_tensor = tf.convert_to_tensor(Py_train)\n",
        "Py_test_tensor = tf.convert_to_tensor(Py_test)\n",
        "\n",
        "CX_train_tensor = tf.convert_to_tensor(CX_train)\n",
        "CX_test_tensor = tf.convert_to_tensor(CX_test)\n",
        "Cy_train_tensor = tf.convert_to_tensor(Cy_train)\n",
        "Cy_test_tensor = tf.convert_to_tensor(Cy_test)\n",
        "\n",
        "IX_train_tensor = tf.convert_to_tensor(IX_train)\n",
        "IX_test_tensor = tf.convert_to_tensor(IX_test)\n",
        "Iy_train_tensor = tf.convert_to_tensor(Iy_train)\n",
        "Iy_test_tensor = tf.convert_to_tensor(Iy_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "EbwY6wIjwnRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here are a few keras imports that will probably be needed. You can include\n",
        "# other kinds of layers appropriate to CNNs if you like. \n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Dense\n",
        "\n",
        "from keras.losses import MeanSquaredError\n",
        "\n",
        "# Hint: after running modelC = Sequential(), you can add each new layer using\n",
        "# modelC.add()"
      ],
      "metadata": {
        "id": "48HkHt4DWxj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point in the code, create three separate CNN models using Sequential(). Look to the Week 8 lecture and Lab 8 for information on configuring CNN models. The most basic model could consist of a 2D convolutional layer (because your input is a set of 2D matrices) followed by a max-pooling-2D layer, followed by a flattening layer, and concluding with a single unit dense layer with linear activation (because you are predicting a metric output rather than a categorical one). You should try more complex models as well. Note that your error function should be Mean Squared Error.\n",
        "\n",
        "Hint: Get the first model working first (the one where you will predict Py_train_tensor during training) making adjustments to the layers and hyperparameters as needed to improve the training. Once that first model is working well, you can simply copy it and change the training inputs and outputs "
      ],
      "metadata": {
        "id": "cuJBqERkMNop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# These are some hyperparameters that you may be helpful in specifying the layers\n",
        "input_shape = (max_mat_size, max_mat_size, 1) # Each input is a single layer square matrix \n",
        "num_filters = 6 # You can adjust this up or down to try to improve model fit\n",
        "kern_size = (6, 6) # You can adjust this up or down as needed to improve model fit\n",
        "max_pooling_size = (2, 2) # Making a pooling window larger than 2,2 can result in a loss of important data\n",
        "dense_size = 64 # How many neurons to receive the output of the max pooling layer\n",
        "dense_act = 'linear'\n",
        "val_split = 0.2"
      ],
      "metadata": {
        "id": "sO9sddKxPfSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember, because you are predicting metric outcomes (i.e., floating point numbers) rather than categories, you should use MeanSquaredError() as your loss function. Make sure to show a model summary that confirms the shapes of the various layers.  \n",
        "\n",
        "During training, carefully monitor the number of epochs needed to fully train the model without overtraining it. Keep your eye on the validation loss to make sure it does not start to climb far higher than the training loss.\n",
        "\n",
        "Once you have a trained model that is satisfactory for each of your outcome variables, make predictions using the model and the original input data and compute a regular correlation coefficient (e.g., with numpy corrcoef) to confirm how well the model performs."
      ],
      "metadata": {
        "id": "krGv2QhEQDeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# (End of) Task 4: Compile and summarize your models here.\n",
        "# Hint: Adam would be a reasonable optimizer to use: optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
        "# You can also experiment with a faster learning rate.\n",
        "#\n"
      ],
      "metadata": {
        "id": "nK-fBMi9RDSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 5**\n",
        "\n",
        "Train your CNN models. Experiment with the learning rate and the number of epochs to make your training as efficient as you can. Remember, get the first model working satisfactorily first, before you make copies for predicting the other two outcome variables."
      ],
      "metadata": {
        "id": "WfIdxvUne5nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HW4T5a\n",
        "# Train your models here"
      ],
      "metadata": {
        "id": "i3mxYSOSRPKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 6**\n",
        "\n",
        "Once each model is fully trained to your satisfaction, use the \"predict\" method on your test set to compute a set of predicted values from the set of input matrices. Use a Pearson's correlation (r) between predicted and actual values (from the test set) as a final model performance metric. You can use np.corrcoef() to calculate this. You may need to append .squeeze(axis=-1) to the predict function to get the predictions into the shape they need to be in for np.corrcoef().\n",
        "\n",
        "Report the value of Pearson's r for each of the three models."
      ],
      "metadata": {
        "id": "Bk3_662tfNfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HW4T6a\n",
        "# Task 6: Evaluate your models here\n",
        "#\n"
      ],
      "metadata": {
        "id": "Au9neth-fN8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 7** \n",
        "\n",
        "\n",
        "The very last block in the notebook should be a text block that documents and discusses your results. Make sure your discussion describes performance you achieved on each of the three different metrics. Give some thought to why the performance levels of the three models may differ from each other. Based on your results, what can be learned from vectorizing a series of sentences? Comment on whether you think that your models are overtrained – that is would these models generalize to new sentences extracted from Wikipedia?"
      ],
      "metadata": {
        "id": "P-u6XzomRUyG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "HWCC\n",
        "\n",
        "Replace this text with your comments."
      ],
      "metadata": {
        "id": "V7h9hs3qfscS"
      }
    }
  ]
}