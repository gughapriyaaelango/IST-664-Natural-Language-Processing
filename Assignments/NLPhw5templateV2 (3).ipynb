{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"11It2GNfnkCwIMC_dtVKUiOosMCTV_jxs","timestamp":1667672292376}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"VHcRoJfSC3A2"},"source":["\n","# IST664/CIS668 - Homework 5 Template\n","\n","In recent classes, we've examined sequence-to-sequence (S2S) models that use recurrent networks to process sequences of character or word tokens. In this final homework, you will process the SQUAD V2 dataset, extracting the most brief among the Q/A pairs and process them with an S2S model to explore the limits of recurrence.\n","\n","Most of this code has been borrowed from Labs 10 and 11. If you run into any problems, compare the code to what you used for the lab. \n","\n","Use this template to organize your code for Homework 5. Put your name and the names of your collaborators (if any) here:\n","\n","Your name: _______________________________\n","\n","Your collaborators: ________________________________\n"]},{"cell_type":"markdown","source":["*Task 1*: Install and process the SQUAD V2 dataset.\n"],"metadata":{"id":"VGs2fqLTCCGa"}},{"cell_type":"code","source":["# Get the datasets package and load the data from HuggingFace\n","\n","!pip install datasets # A package for creating a connection to HuggingFace data resources\n","from datasets import load_dataset\n","\n","raw_dataset = load_dataset(\"squad_v2\") # The names of the datasets can be obtained from the web-based discovery interface\n","raw_train_dataset = raw_dataset[\"train\"] # We will use the training data first\n","\n","type(raw_dataset), type(raw_train_dataset) # Display the types"],"metadata":{"id":"gIvAH71ZGgGm"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1v4UYHPkD913"},"source":["# We will be using numpy, tensorflow, and keras\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","tf.__version__"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The MAX_INP and MAX_TARG variables below are key to the operation of this S2S model. We know from Class 10 that LSTMs have limits on how far a training gradient can go back through a long sequence of LSTM cells. The loop below filters the dataset to only include question and answer pairs that fit the maximum sequence lengths. Low values mean short LSTMs that are easy to train, but not much training data. \n","\n","Start the model using the  sequence length suggestions below and then gradually increase them to try to recover more training data from the dataset. You MUST try at least one additional gradation of these two values to get full credit for this lab. For example, you could raise MAX_INP to 50 and MAX_TARG to 30."],"metadata":{"id":"2bHWSMwdC1Xb"}},{"cell_type":"code","source":["# Build a list of questions and answers plus character sets for each.\n","# Only include Q/A pairs where the message lengths fit within the\n","# maximums noted here:\n","MAX_INP = 30\n","MAX_TARG = 20\n","\n","input_texts = []\n","target_texts = []\n","input_characters = set()\n","target_characters = set()\n","\n","for entry in raw_train_dataset:\n","  # Some entries may not have an answer: Skip them\n","  if len(entry['answers']['text']) > 0:\n","    \n","    \n","    inp = entry['question'].strip()\n","    inp = inp.encode(\"ascii\", \"ignore\")\n","    inp = inp.decode()\n","    \n","    targ = \"\\t\" + entry['answers']['text'][0].strip() + \"\\n\"\n","    targ = targ.encode(\"ascii\",\"ignore\")\n","    targ = targ.decode()\n","\n","    if (len(inp) < MAX_INP) and (len(targ) < MAX_TARG):\n","\n","      input_texts.append(inp)\n","      for char in inp:\n","          if char not in input_characters:\n","              input_characters.add(char)\n","    \n","      target_texts.append(targ)\n","      for char in targ:\n","          if char not in target_characters:\n","              target_characters.add(char)\n","\n","len(input_texts), len(target_texts) # How much training data do we have"],"metadata":{"id":"LRJMPv6RGqIO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#\n","# Task 1a: Display the list of input characters\n","#\n"],"metadata":{"id":"7_x8hC4hIX-L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#\n","# Task 1b: Display the list of target characters\n","#\n"],"metadata":{"id":"RQzZSnqP2ClS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#\n","# Task 1c: Display the lengths of the two character sets.\n","# Leave a comment explaining the difference in lengths of the two sets.\n","#\n"],"metadata":{"id":"gW3a8ZhxDlPK"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y-VKjaDoDizz"},"source":["# Task 1d: Display the first 15 pairs of questions and answers\n","# \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ff0F8a6psDG9"},"source":["# Colab instances do not have much memory. This code provides an upper\n","# bound on how many training instances we will try to process.\n","num_samples = min(20000, len(input_texts))\n","input_texts = input_texts[:num_samples]\n","target_texts = target_texts[:num_samples]"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Randomize the order of the texts\n","arr = np.arange(num_samples)\n","np.random.shuffle(arr)\n","\n","input_texts = [input_texts[i] for i in arr]\n","target_texts = [target_texts[i] for i in arr]"],"metadata":{"id":"t-6Gf0MwC5vC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Task 1d: Display the first 15 pairs of questions and answers after randomization\n","# \n"],"metadata":{"id":"F1E1rWwWFB_i"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EdtsoeDzsuja"},"source":["# This code creates tokens lists (of character tokens)\n","input_characters = sorted(list(input_characters))\n","target_characters = sorted(list(target_characters))\n","num_encoder_tokens = len(input_characters)\n","num_decoder_tokens = len(target_characters)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Task 1e: Display the number of unique input and output tokens.\n","# Add a comment explaining why the number of tokens is different.\n","#\n"],"metadata":{"id":"Aumh4JnMGFSU"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S3zPygyhs65z"},"source":["# This code defines \"maximum message lengths\" calibrated in the number of characters.\n","# Because we are doing a character-level model these values define how far\n","# the encoder LSTM and the decoder LSTM (respectively) need to be \"unrolled\"\n","# in order to do the training. \n","#\n","# Also remember that shorter sequences will need to be padded.\n","max_encoder_seq_length = max([len(txt) for txt in input_texts])\n","max_decoder_seq_length = max([len(txt) for txt in target_texts])\n","\n","print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n","print(\"Max sequence length for outputs:\", max_decoder_seq_length)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Task 1f: Display the max sequence length for the encoder and the decoder.\n","# Explain how these values came to be.\n","#\n"],"metadata":{"id":"MmbmaveoGhGe"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9e1TGuHus_L6"},"source":["# Next we will vectorize all of the input and target messages\n","\n","# First, make Python dictionaries for the input and target messages\n","input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n","target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Remember that the seq2seq data is actually a 3-tuple. The encoder takes \n","# input messages as input but creates no output except for the hidden state.\n","# The decoder has both inputs and targets. These three lines fill vectors\n","# with zeroes to initialize them.\n","\n","# The encoder inputs - will hold character sequences for short English phrases\n","encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\")\n","\n","# The decoder inputs - will hold character sequences for short French phrases\n","decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n","\n","# Same size numpy array for the decoder targets\n","decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype = \"float32\")"],"metadata":{"id":"uKSBJg3i9P4N"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xNy5Ea5ctDdr"},"source":["# Now fill the vectors\n","\n","# Iterate over all of our phrase pairs\n","for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n","    \n","    # Iterate over all of the characters in the input phrase\n","    for t, char in enumerate(input_text):\n","        encoder_input_data[i, t, input_token_index[char]] = 1.0\n","\n","    # This adds padding with spaces\n","    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n","\n","    # Iterate over all of the characters in the target phrase. Here we are\n","    # filling two vectors \n","    for t, char in enumerate(target_text):\n","        # decoder_target_data is ahead of decoder_input_data by one timestep\n","        decoder_input_data[i, t, target_token_index[char]] = 1.0\n","        if t > 0:\n","            # decoder_target_data will be ahead by one timestep\n","            # and will not include the start character.\n","            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n","    \n","    # This adds padding with spaces\n","    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n","    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EO2i_wNfuo9Q"},"source":["# Reverse-lookup token index to decode sequences back to\n","# something readable.\n","reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n","reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This is a daignostic to check our work from above.\n","# Task 1g: Add comments to each line of code in this block\n","chars = \"\"\n","\n","for i, iv in enumerate(encoder_input_data[0]):\n","  for j, val in enumerate(iv):\n","    if val == 1.0:\n","      chars += reverse_input_char_index[j]\n","\n","print(chars)"],"metadata":{"id":"jK1peUq3tOPw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Task 1h: Create another block of code modeled on the above\n","# that translates one instance of decoder input data. \n","#\n"],"metadata":{"id":"KIUcXASAHDw7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Task 1i: Create another block of code modeled on the above\n","# that translates one instance of decoder target data. \n","#\n"],"metadata":{"id":"mIybf8mpHScI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Task 1j: Display the lengths of the encoder input data, decoder input data\n","# and decoder target data. Explain the results in a comment."],"metadata":{"id":"L6brm1ulHhip"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9HAVAWiAtSOR"},"source":["*Task 2*: Build a sequence to sequence LSTM model to train."]},{"cell_type":"code","source":["# Example hyperparameters to use for training the model: Tweak these to improve\n","# model performance. What does changing the batch_size do? Should the latent\n","# dimension of the \"though vector\" be larger or smaller?\n","\n","batch_size = 32  # Batch size for training.\n","\n","latent_dim = 256  # Latent dimensionality of the thought vector encoding space.\n"],"metadata":{"id":"Y0PCuA8xENVZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vXtcTJqQtQU_"},"source":["# 1. Define an input sequence and process it.\n","encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n","\n","# 2. Use a LSTM layer to process the input vectors. After today's lecture\n","# you should know what return_state does.\n","encoder = keras.layers.LSTM(latent_dim, return_state=True)\n","\n","# 3. Save the output from the encoder, but see step 4. Note the use of \n","# the functional programming interface here. For deep learning models that\n","# are not simple seqiential layers, this interface provides a stratightforward\n","# way of connecting one element of a model to the element that it should \n","# feed into.\n","encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n","\n","# 4. We discard `encoder_outputs` and only keep the states.\n","encoder_states = [state_h, state_c]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AkBZXwW-tWRF"},"source":["# Set up the decoder, using `encoder_states` as initial state.\n","\n","# This takes the target tokens as the input.\n","decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n","\n","# The LSTM later has the same internal dimensionality as for the encoder.\n","decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n","\n","# Save the decoder output: Note that this uses decoder_inputs\n","decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n","\n","# Dense with softmax allows us to predict categorical output (our list of French characters)\n","decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n","\n","# Output layer\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","# Define the overall model. This binds the encoder and decoder and will turn\n","# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n","model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nY_czog5tYpc"},"source":["# We use categorical_crossentropy because our prediction is multinomial: we\n","# are trying to predict which is the most likely character for the next time step.\n","model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"categorical_accuracy\"])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Task 2a: Display a model summary using the appropriate method.\n","# Add a comment describing the number of trainable gates and how \n","# this might compare to a CNN model.\n","#\n"],"metadata":{"id":"KhaQmEdgH_ld"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Task 2b: Display a plot of the model's shape using the appropriate method.\n","# Add a comment pointing out how and where the 3-tuple data are used in the model.\n"],"metadata":{"id":"7eaDmagXIG0h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vn-yOkzntb-3"},"source":["*Task 3*: Train the model, paying close attention to training time and the progress of validation loss and validation accuracy. Given the basic model hyperparameters used in the template together with the small amount of data this model trains very fast. You may need to raise the number of epochs or lower the batch size or both to try to improve the model accuracy."]},{"cell_type":"code","metadata":{"id":"Fb1eTlw_tahR"},"source":["# Train the model: Choose a value for epochs to balance training time and accuracy\n","\n","epochs = 35  # Number of epochs to train for. You may need to raise this after seeing initial results.\n","\n","history = model.fit(\n","    [encoder_input_data, decoder_input_data],\n","    decoder_target_data,\n","    batch_size=batch_size,\n","    epochs=epochs,\n","    validation_split=0.1,\n",")\n","\n","# You should be looking for a val_categorical_accuracy in excess of 0.60."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eE95wIM0tf-Z"},"source":["# Graphing code fragment modified from Rahul Verma on Stackoverflow\n","from matplotlib import pyplot as plt\n","\n","plt.plot(history.history['categorical_accuracy'])\n","plt.title('Model History')\n","plt.ylabel('Categorical Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Accuracy'], loc='upper right')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####Task 3: Add a comment about training success\n","\n","Replace this text with a comment  that describes what you see in the model history graph and the model training diagnostics. Have you reached the point in the training where not much additional improvement can be achieved? How do you know? Is the validation categorical accuracy sufficient?"],"metadata":{"id":"leud2lDYQusT"}},{"cell_type":"markdown","metadata":{"id":"H-gRv-rquhqO"},"source":["## Task 4: Use the trained model for inference\n","\n","The basic inference functions from Lab 11 are included here. \n","\n","Note: Add a code timer to the appropriate code to find out how long it takes to run inference. "]},{"cell_type":"code","metadata":{"id":"p3nbXkECumX6"},"source":["# Construct the encoder and decoder\n","\n","# Here's the encoder\n","encoder_inputs = model.input[0]  # Encoder input layer\n","encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # Encoder model LSTM\n","\n","# This is the \"thought vector\" the hidden state that is used to start the decoder\n","encoder_states = [state_h_enc, state_c_enc]\n","encoder_model = keras.Model(encoder_inputs, encoder_states)\n","\n","# This is the decoder, starting with the input layer \n","decoder_inputs = model.input[1]  \n","decoder_state_input_h = keras.Input(shape=(latent_dim,))\n","decoder_state_input_c = keras.Input(shape=(latent_dim,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","\n","decoder_lstm = model.layers[3] # Decoder model LSTM \n","\n","decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n","    decoder_inputs, initial_state=decoder_states_inputs\n",")\n","decoder_states = [state_h_dec, state_c_dec]\n","\n","decoder_dense = model.layers[4] # Dense output layer\n","\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","decoder_model = keras.Model(\n","    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Task 4a: Summarize the encoder model with the appropriate method.\n","#\n"],"metadata":{"id":"cKMMDlH6PfxV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Task 4b: Summarize the decoder model with the appropriate method.\n","#\n"],"metadata":{"id":"Bbll0hpDPrIS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####Task 4c: Add a comment explaining the encoder and decoder models\n","\n","Replace this text with a comment that describes what you see above in the model summaries. Comment on the number of trainable gates and how this corresponds to the size of the original model we trained. Based on the model builder just above, describe how the internal cell states are managed for the decoder and encoder. Make sure you use the correct terminology when referring to each of the two internal cell states.\n","\n","In these next three blocks, we have code to test whether our encoder is properly trained."],"metadata":{"id":"nx15x6KiPwyT"}},{"cell_type":"code","source":["# Make one prediction using the first data instance\n","states_value = encoder_model.predict(encoder_input_data[0 :  1])\n","\n","states_value[0].shape, states_value[1].shape"],"metadata":{"id":"i-EJt8anGvbB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Make one prediction using the second data instance\n","states_value1 = encoder_model.predict(encoder_input_data[1 :  2])\n","\n","states_value1[0].shape, states_value1[1].shape"],"metadata":{"id":"k7vRZGqRHVzQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We now have two instances of thought vectors, with states_value\n","# based on doing a prediction using the FIRST instance from the input\n","# data and the states_value1 based on doing a prediction using the SECOND \n","# instance from the input data. Because these two pieces of input data\n","# are very different, they should produce very different thought vectors.\n","# If the resulting thought vectors are similar or identical, this indicates that\n","# the encoder model is not properly trained.\n","\n","# Task 4d: Write a few lines of code to compare the two thought vectors. \n","# Confirm whether they are notably different from one another. If they are \n","# too similar, the model tests below will tend to generate the same answers\n","# for many questions.\n","#"],"metadata":{"id":"W1oXR2m0RSZX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Task 4d, just above, is critical for diagnosing whether the encoder stage of the model is sufficiently trained. If the two thought vectors are too similar, then the encoder has not trained properly. Go back and make adjustments to the model hyperparameters to improve the training.\n","\n","Add a comment in this text block documenting where you ended up."],"metadata":{"id":"2dzkZnDVx5Wd"}},{"cell_type":"code","metadata":{"id":"FUHDc7IKurhE"},"source":["# Here's a function to decode sequences. This takes a one-hot encoded \n","# input sequence as the input. It runs the encoder model with that input to\n","# generate the \"thought vector.\" The thought vector is fed to the decoder\n","# model and a \\t is issued to the decoder to start producing a sequence.\n","\n","def decode_sequence(input_seq):\n","    # Encode the input as state vectors.\n","    states_value = encoder_model.predict(input_seq)\n","\n","    # Generate an empty target sequence of length 1.\n","    target_seq = np.zeros((1, 1, num_decoder_tokens))\n","    \n","    # Populate the first character of target sequence with the start character.\n","    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n","\n","    # Sampling loop for a batch of sequences\n","    stop_condition = False\n","    decoded_sentence = \"\"\n","    \n","    # Always a little risky to use a while loop, but we don't know what \n","    # length of sentence the decoder will issue - that's pretty much the \n","    # whole point of a sequence-to-sequence model, right?\n","    while not stop_condition:\n","        output_tokens, h, c = decoder_model.predict([target_seq] + states_value, verbose=False)\n","\n","        # Sample a token: Find the index of the most probable output character\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","\n","        # Use our reversing dictionary to decode the predicted character\n","        sampled_char = reverse_target_char_index[sampled_token_index]\n","        decoded_sentence += sampled_char\n","\n","        # Exit condition: either hit max length of a decoder output sequence\n","        # or find that the decoder has issued a stop character.\n","        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n","            stop_condition = True\n","\n","        # Update the target sequence (of length 1).\n","        target_seq = np.zeros((1, 1, num_decoder_tokens))\n","\n","        # This one-hot encodes the current character to use as input for the next iteration\n","        target_seq[0, 0, sampled_token_index] = 1.0\n","\n","        # Update states\n","        states_value = [h, c]\n","    \n","    return decoded_sentence\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The range() call controls how many input sequences we will test. \n","# Task 4e: Change to review at least 10 decoded sequences\n","\n","for seq_index in range(1):\n","    # Take one sequence (part of the training set)\n","    # for trying out decoding.\n","    input_seq = encoder_input_data[seq_index : seq_index + 1]\n","\n","    # Here's where we call our custom function\n","    decoded_sentence = decode_sequence(input_seq)\n","\n","    print(\"Input sentence:\", input_texts[seq_index], \"Decoded sentence:\", decoded_sentence)"],"metadata":{"id":"02j91Acw22He"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yh077fldu5Ek"},"source":["# Here's a function that put's one more layer of convenience on top of\n","# decode_sequences. This function takes plain text as input and converts\n","# it to the one hot encoding needed to get the prediction model to work.\n","\n","def answer(text_2):\n","  text_2 = text_2.replace(\"\\n\",\"\")\n","  if len(text_2) <= max_encoder_seq_length:\n","    # Fill up a single one-hot encodign vector with zeroes\n","    my_input_data =  np.zeros((1, max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\")\n","\n","    # Iterate over all of the characters in the input phrase\n","    for t, char in enumerate(text_2):\n","      my_input_data[0, t, input_token_index[char]] = 1.0\n","\n","    # This adds padding with spaces\n","    my_input_data[0, t + 1 :, input_token_index[\" \"]] = 1.0\n","    translated_sentence = decode_sequence(my_input_data)\n","    print (text_2, \" \", translated_sentence)\n","    return translated_sentence\n","\n","  else:\n","    print(\"Input phrase is longer than the maximum encoder sequence length.\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Task 4f: Test the answer() function with at least two different examples\n","# of short questions. Make sure not to exceed the maximum encoder sequence length.\n","#\n"," "],"metadata":{"id":"4zWZSuejThOM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["*Task 5*: In this final task we are going to use the validation data from SQUAD V2 to make some predictions and compare what the model produces to what the real answers are in the validation data."],"metadata":{"id":"HU6A0W7xTwyu"}},{"cell_type":"code","source":["# This is similar to the code at the top of the notebook.\n","raw_test_dataset = raw_dataset[\"validation\"] # We will use the validation data now\n","\n","type(raw_dataset), type(raw_test_dataset) # Display the types"],"metadata":{"id":"jFG6SaZ9Nlem"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Task 5a: Display how much data is in the raw_test_dataset\n","#\n"],"metadata":{"id":"q0i2PWQmUBCF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This is similar to the code at the top of the notebook. We're going to make\n","# the assumption, based on reusing the same ASCII conversion as before, that\n","# the character sets produced in these questions and answers will be the same\n","# as the ones our model was set up to handle.\n"," \n","# Build a list of questions and answers plus character sets for each test instance\n","test_input_texts = []\n","test_target_texts = []\n","\n","for entry in raw_test_dataset:\n","  # Some entries may not have an answer: Skip them\n","  if len(entry['answers']['text']) > 0:\n","    \n","    inp = entry['question'].strip()\n","    inp = inp.encode(\"ascii\", \"ignore\")\n","    inp = inp.decode()\n","    \n","    targ = \"\\t\" + entry['answers']['text'][0].strip() + \"\\n\"\n","    targ = targ.encode(\"ascii\",\"ignore\")\n","    targ = targ.decode()\n","\n","    # Remember that we can only use short questions and answers, same as\n","    # the original model\n","    if (len(inp) < MAX_INP) and (len(targ) < MAX_TARG):\n","      test_input_texts.append(inp)\n","      test_target_texts.append(targ)\n","\n"],"metadata":{"id":"mwX5lK7xnFFL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Task 5b: Display how much data is in test_input_texts and test_target_texts\n","#\n"],"metadata":{"id":"I6HPwD3EUq7v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Task 5c: Display the first 15 question/answer pairs\n","#\n"],"metadata":{"id":"GtchT3qCUzYv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Task 5d: Display one model prediction generated by answer() and compare that\n","# to the actual, correct response in test_target_texts\n","#\n"],"metadata":{"id":"WMShakBXU7AV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To complete task 5 you will need to generate cosine similarities between the predictions created by answer() and the actual, correct responses in test_target_texts. We'll use a sentence summarizer to do the job."],"metadata":{"id":"HpJ2TGbIVKrz"}},{"cell_type":"code","source":["!pip install sentence-transformers"],"metadata":{"id":"FbfTHbxkoWSE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Now load a pre-trained sentence transformer. There are hundreds to choose from.\n","# This downloads a lot of data to your virtual machine and takes half a minute or so.\n","from sentence_transformers import SentenceTransformer\n","\n","sent_model = SentenceTransformer('distiluse-base-multilingual-cased-v2')\n","# Why is it sometimes a good idea to use a multilingual model?"],"metadata":{"id":"U259_EjUqQCh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Now evaluate all of the text items in a loop generating a list of cosine similarities\n","from sklearn.metrics.pairwise import cosine_similarity\n","cos_sim_list = []\n","\n","for test_text, correct_answer in zip(test_input_texts, test_target_texts):\n","  a = sent_model.encode([answer(test_text)])\n","  b = sent_model.encode([correct_answer])\n","  cos_sim_list.append(cosine_similarity(a, b))"],"metadata":{"id":"UShHC0daPMgl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Task 5e: Display some of cos_sim_list and say why the values make sense.\n","#\n"],"metadata":{"id":"Yc-YYrOvVfxg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Task 5f: Plot a histogram of the values in cos_sim_list\n","#\n"],"metadata":{"id":"BEXdD50cVxJ4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Task 5g: Show the mean cosine similarity of the values in cos_sim_list. The\n","# basic code template generates a mean cosine similarity of about 0.35. Your\n","# job in adjusting the model training data and hyperparameters is to  \n","# beat this value.\n","#\n"],"metadata":{"id":"7TwfUpZ5V2FN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Concluding Comments\n","\n","Replace this text with answers to the following questions based on your exploration of model and data parameters.\n","\n","1. What's the upper limit of sequence length for encoder and decoder models that still produces a trainable model.\n","\n","2. Why can't you properly train a sequence to sequence model with longer sequences?\n","\n","3. Given the best model that you were able to train, is the model producing sensible answers? Why or why not? \n","\n","4. What are one or two of the main challenges involved in working with a question and answer dataset like SQUAD V2?\n","\n","5. Based on everything you have learned in the class, does it seem like a sequence to sequence model is the best approach to question answering? If you answered \"no\", then mention one or two other models that you think would perform better."],"metadata":{"id":"A8F7eaDCWKmP"}}]}