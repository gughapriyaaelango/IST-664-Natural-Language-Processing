{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t09eeeR5prIJ"
      },
      "source": [
        "#Week 9 Lab - RNNs for Text Generation\n",
        "\n",
        "This lab was modified from an example originally written by \"The TensorFlow Authors\" in 2019 and distributed under Apache License 2.0. This lab creates a model that can generate text using a character-based RNN. A character-based RNN learns sequences of characters from a corpus of text. \n",
        "\n",
        "Given a character, or a sequence of characters, what is the most probable next character? This is the task that our RNN model will learn and then perform. The input to the model will be sequences of characters from the book we ingest at the top of this notebook.\n",
        "\n",
        "Once the model is trained, one can present an input character sequence and the model will generate the character that it predicts would be most likely to appear next. By repeatedly calling the model for new predictions with the previously built sequence, one can create a string of text that \"looks like\" sentences from the original training text. Note that depending upon the amount of training material and the details of the training procedure, this generated text may look more or less like gibberish.\n",
        "\n",
        "Also note that, depending on the specific types of layers you are training, you might want to try enabling GPU acceleration to execute this notebook faster. In Colab: *Runtime > Change runtime type > Hardware accelerator > GPU*. Do this before you start to run any code, because it generally restarts the runtime and your local variables will be lost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srXC6pLGLwS6"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGyKZj3bzf9p"
      },
      "source": [
        "### Import TensorFlow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:24:50.972847Z",
          "iopub.status.busy": "2021-08-11T18:24:50.972275Z",
          "iopub.status.idle": "2021-08-11T18:24:52.595725Z",
          "shell.execute_reply": "2021-08-11T18:24:52.596142Z"
        },
        "id": "yG_n40gFzf9s"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import numpy as np\n",
        "import os # To access local files; for saving checkpoints\n",
        "import time\n",
        "\n",
        "from urllib import request # We will need this to read from an URL"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auQWy3dpCB8S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0bbae52-a09b-4b9e-f425-dedbefb3d2f2"
      },
      "source": [
        "# TensorFlow 2.0 offers \"Eager Execution,\" a more practical model for \n",
        "# running tf code. Are we using it here?\n",
        "tf.executing_eagerly()"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHDoRoc5PKWz"
      },
      "source": [
        "### Download the Text Data\n",
        "\n",
        "Load a plain text book from Project Gutenberg:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Xp9CQJwqnd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e5357c5-b1bd-42c6-99d5-4018cc280dcd"
      },
      "source": [
        "# Some collected plays by Anton Chekhov\n",
        "url = \"https://www.gutenberg.org/files/7986/7986-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "text = response.read().decode('utf8')\n",
        "type(text), len(text)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(str, 411576)"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbVhoqcCM0Sz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "376f9224-16fe-496b-979e-164c93543a46"
      },
      "source": [
        "#\n",
        "# Exercise 9.1: Examine some of the contents of text using slicing\n",
        "# Write a comment saying what you see. Add code to discard the front\n",
        "# matter so as to start where the book begins: somewhere around text[1090:]. \n",
        "#\n",
        "text[1390:1690]\n",
        "#From the book, we can see few sentences selected above\n",
        "#The text contains words along with tags like new lines \\n. Punctuations and stop words are all intact.\n",
        "#There is no preprocessing is done to the text."
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'nsystematic mass of\\r\\ntranslations from the Russian flung at the heads and hearts of English\\r\\nreaders. The ready acceptance of Chekhov has been one of the few\\r\\nsuccessful features of this irresponsible output. He has been welcomed\\r\\nby British critics with something like affection. Bernard Shaw has\\r\\ns'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g53PCZD3v__O"
      },
      "source": [
        "# Additional URLs of plain text Chekhov plays for possible later use\n",
        "uncle_vanya = \"https://www.gutenberg.org/cache/epub/1756/pg1756.txt\"\n",
        "the_seagull = \"https://www.gutenberg.org/files/1754/1754-0.txt\""
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHjdCjDuSvX_"
      },
      "source": [
        "### Read the data\n",
        "\n",
        "First, look in the text to see what we have. Note that this is a character-based model, so we are interested to know what different characters are used throughout the whole text. Note that this should make the process largely language independent: Any language where words are formed through sequences of characters should work in training this kind of model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:24:52.658460Z",
          "iopub.status.busy": "2021-08-11T18:24:52.657812Z",
          "iopub.status.idle": "2021-08-11T18:24:52.660268Z",
          "shell.execute_reply": "2021-08-11T18:24:52.660598Z"
        },
        "id": "IlCgQBRVymwR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9836be9-9491-4ece-bfea-bd0e2893b22f"
      },
      "source": [
        "# The unique text characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "94 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIPbpCWFNJoT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6671ca82-64c3-4288-abd1-44cf378c6a20"
      },
      "source": [
        "#\n",
        "# Exercise 9.2: Display the list of unique text characters.\n",
        "# Write a comment saying what you see. What are the advantages of \n",
        "# having such a small vocabulary?\n",
        "#\n",
        "vocab\n",
        "\n",
        "#The list includes all the alphabets (lower and upper), tags like new line, symbols, numbers\n",
        "#Having a small vocabulory helps the RNN model in faster training and less computational power\n",
        "#They would be better at generalizing and can prevent overfitting"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n',\n",
              " '\\r',\n",
              " ' ',\n",
              " '!',\n",
              " '#',\n",
              " '$',\n",
              " '%',\n",
              " '(',\n",
              " ')',\n",
              " '*',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '/',\n",
              " '0',\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '7',\n",
              " '8',\n",
              " '9',\n",
              " ':',\n",
              " ';',\n",
              " '=',\n",
              " '?',\n",
              " '@',\n",
              " 'A',\n",
              " 'B',\n",
              " 'C',\n",
              " 'D',\n",
              " 'E',\n",
              " 'F',\n",
              " 'G',\n",
              " 'H',\n",
              " 'I',\n",
              " 'J',\n",
              " 'K',\n",
              " 'L',\n",
              " 'M',\n",
              " 'N',\n",
              " 'O',\n",
              " 'P',\n",
              " 'Q',\n",
              " 'R',\n",
              " 'S',\n",
              " 'T',\n",
              " 'U',\n",
              " 'V',\n",
              " 'W',\n",
              " 'X',\n",
              " 'Y',\n",
              " 'Z',\n",
              " '[',\n",
              " ']',\n",
              " '_',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z',\n",
              " '£',\n",
              " 'à',\n",
              " 'è',\n",
              " 'é',\n",
              " 'ê',\n",
              " '‘',\n",
              " '’',\n",
              " '“',\n",
              " '”',\n",
              " '\\ufeff']"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNnrKn_lL-IJ"
      },
      "source": [
        "## Process the text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFjSVAlWzf-N"
      },
      "source": [
        "### Vectorize the text\n",
        "\n",
        "Before training, we will convert the strings to a numerical representation. The approach in this notebook uses a so-called \"ragged tensor.\" Here's an excerpt from the TensorFlow documentation:\n",
        "\n",
        "\"Ragged tensors are the TensorFlow equivalent of nested variable-length lists. They make it easy to store and process data with non-uniform shapes, including: Variable-length features, such as the set of actors in a movie; Batches of variable-length sequential inputs, such as sentences or video clips; Hierarchical inputs, such as text documents that are subdivided into sections, paragraphs, sentences, and words.\"\n",
        "\n",
        "In the cells below, the notebook uses `preprocessing.StringLookup` from TensorFlow. This function can convert each character into a numeric ID. The input to this is a set of \"byte code\" ID numbers.\n",
        "\n",
        "Try the character encoding with a small example first:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:24:54.194902Z",
          "iopub.status.busy": "2021-08-11T18:24:54.194072Z",
          "iopub.status.idle": "2021-08-11T18:24:54.208515Z",
          "shell.execute_reply": "2021-08-11T18:24:54.208926Z"
        },
        "id": "a86OoYtO01go",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "004af531-721b-45f2-da10-819487ca4a10"
      },
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBTMDuH8H2Mx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "900e8d05-294c-4634-f843-ce9c77c23967"
      },
      "source": [
        "print(chars) # In TF 2.0, the print() function can also show tensors"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6XQ95pQN4EB"
      },
      "source": [
        "We have now created a ragged tensor. Now or later, you might want to read some of the TensorFlow documentation describing what a tensor actually is and what it means to be ragged. There's a nice tutorial with code here: https://www.tensorflow.org/guide/tensor\n",
        "\n",
        "Note that each element in the structure above begins with b. That means that the function has returned a \"byte-coded\" version of the string. This is an alternative representation to plain text. Also, the idea of \"ragged\" is demonstrated by this example. The first nested list has seven items, whereas the second list has three. Ragged means that different numbers of elements are permissible within each element of a list. The term ragged comes from publishing, where a common way of justifying text in a book is called \"ragged right\" (meaning that different lines are different lengths).\n",
        "\n",
        "Next, create a preprocessing.StringLookup layer. Note this initial diagnostic test to show what preprocessing.StringLookup actually is. It is an object reflecting a relatively new feature of Python called an \"abstract base class.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3ye5l9gZPie",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69fb715c-566a-4c5e-cb67-93bdc0676a83"
      },
      "source": [
        "# Confirms that preprocessing.StringLookup is an ABC and not an instance\n",
        "type(preprocessing.StringLookup), isinstance(preprocessing.StringLookup, preprocessing.StringLookup)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(abc.ABCMeta, False)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:24:54.216550Z",
          "iopub.status.busy": "2021-08-11T18:24:54.215976Z",
          "iopub.status.idle": "2021-08-11T18:24:54.222826Z",
          "shell.execute_reply": "2021-08-11T18:24:54.223217Z"
        },
        "id": "6GMlCe3qzaL9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94abbb84-ae81-4909-c610-ade436433f94"
      },
      "source": [
        "# Note that we are passing in the vocab from parsing the whole book in an \n",
        "# earlier cell. Examine the first argument closely:\n",
        "ids_from_chars = preprocessing.StringLookup(vocabulary=list(vocab), mask_token=None)\n",
        "\n",
        "# Shows the resulting class type and that we now have an instance\n",
        "type(ids_from_chars), isinstance(ids_from_chars, preprocessing.StringLookup)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(keras.layers.preprocessing.string_lookup.StringLookup, True)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0I4rdMNbepDZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64f3ca51-3da2-4008-afe6-009f21d39f46"
      },
      "source": [
        "#\n",
        "# Exercise 9.3: Display the vocabulary associated with ids_from_chars. Hint: Use\n",
        "# the bound method get_vocabulary(). What is UNK?\n",
        "#\n",
        "ids_from_chars.get_vocabulary()\n",
        "\n",
        "#[UNK] is a special token that represent out of vocabulary.\n",
        "#If there is a character that StringLookup ABC cannot recognize, it is given [UNK] token\n",
        "#indicating it is unknown"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[UNK]',\n",
              " '\\n',\n",
              " '\\r',\n",
              " ' ',\n",
              " '!',\n",
              " '#',\n",
              " '$',\n",
              " '%',\n",
              " '(',\n",
              " ')',\n",
              " '*',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '/',\n",
              " '0',\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '7',\n",
              " '8',\n",
              " '9',\n",
              " ':',\n",
              " ';',\n",
              " '=',\n",
              " '?',\n",
              " '@',\n",
              " 'A',\n",
              " 'B',\n",
              " 'C',\n",
              " 'D',\n",
              " 'E',\n",
              " 'F',\n",
              " 'G',\n",
              " 'H',\n",
              " 'I',\n",
              " 'J',\n",
              " 'K',\n",
              " 'L',\n",
              " 'M',\n",
              " 'N',\n",
              " 'O',\n",
              " 'P',\n",
              " 'Q',\n",
              " 'R',\n",
              " 'S',\n",
              " 'T',\n",
              " 'U',\n",
              " 'V',\n",
              " 'W',\n",
              " 'X',\n",
              " 'Y',\n",
              " 'Z',\n",
              " '[',\n",
              " ']',\n",
              " '_',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z',\n",
              " '£',\n",
              " 'à',\n",
              " 'è',\n",
              " 'é',\n",
              " 'ê',\n",
              " '‘',\n",
              " '’',\n",
              " '“',\n",
              " '”',\n",
              " '\\ufeff']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmX_jbgQqfOi"
      },
      "source": [
        "Our instances of the preprocessing.StringLookup ABC can convert from byte coded character tokens to numeric character IDs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:24:54.227577Z",
          "iopub.status.busy": "2021-08-11T18:24:54.226961Z",
          "iopub.status.idle": "2021-08-11T18:24:54.231289Z",
          "shell.execute_reply": "2021-08-11T18:24:54.230753Z"
        },
        "id": "WLv5Q_2TC2pc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52d71d03-1d00-4b02-8669-709b906094da"
      },
      "source": [
        "print(chars)\n",
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[59, 60, 61, 62, 63, 64, 65], [82, 83, 84]]>"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_omwej_M2jl"
      },
      "source": [
        "# Just out of curiosity: Are these ASCII codes or something else?\n",
        "\n",
        "#The stringlookup function maps each unique string value to integer id in ascending order\n",
        "#They necessarily need not be in ASCII all the time\n",
        "#This code is not ASCII codes, these are something else.\n"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJMVBHuFcvNu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a32ef3b8-b729-4dac-d9ec-dfeacc0fccd5"
      },
      "source": [
        "#\n",
        "# Exercise 9.4: Compare the IDs from the ragged tensor with the vocabulary\n",
        "# list from exercise 9.3. Do this by adding code to the print statement that \n",
        "# looks up the ids using the get_vocabulary() method of ids_from_chars().\n",
        "#\n",
        "# Also write a comment describing what you see.\n",
        "#\n",
        "for nest_list in ids.to_list():\n",
        "  [print(i,ids_from_chars.get_vocabulary()) for i in nest_list]\n",
        "\n",
        "#The ragged tensor creates vector IDs as integer codes\n",
        "#whereas the vocabulary list contains the whole list of vocabulory"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "59 ['[UNK]', '\\n', '\\r', ' ', '!', '#', '$', '%', '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '£', 'à', 'è', 'é', 'ê', '‘', '’', '“', '”', '\\ufeff']\n",
            "60 ['[UNK]', '\\n', '\\r', ' ', '!', '#', '$', '%', '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '£', 'à', 'è', 'é', 'ê', '‘', '’', '“', '”', '\\ufeff']\n",
            "61 ['[UNK]', '\\n', '\\r', ' ', '!', '#', '$', '%', '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '£', 'à', 'è', 'é', 'ê', '‘', '’', '“', '”', '\\ufeff']\n",
            "62 ['[UNK]', '\\n', '\\r', ' ', '!', '#', '$', '%', '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '£', 'à', 'è', 'é', 'ê', '‘', '’', '“', '”', '\\ufeff']\n",
            "63 ['[UNK]', '\\n', '\\r', ' ', '!', '#', '$', '%', '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '£', 'à', 'è', 'é', 'ê', '‘', '’', '“', '”', '\\ufeff']\n",
            "64 ['[UNK]', '\\n', '\\r', ' ', '!', '#', '$', '%', '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '£', 'à', 'è', 'é', 'ê', '‘', '’', '“', '”', '\\ufeff']\n",
            "65 ['[UNK]', '\\n', '\\r', ' ', '!', '#', '$', '%', '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '£', 'à', 'è', 'é', 'ê', '‘', '’', '“', '”', '\\ufeff']\n",
            "82 ['[UNK]', '\\n', '\\r', ' ', '!', '#', '$', '%', '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '£', 'à', 'è', 'é', 'ê', '‘', '’', '“', '”', '\\ufeff']\n",
            "83 ['[UNK]', '\\n', '\\r', ' ', '!', '#', '$', '%', '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '£', 'à', 'è', 'é', 'ê', '‘', '’', '“', '”', '\\ufeff']\n",
            "84 ['[UNK]', '\\n', '\\r', ' ', '!', '#', '$', '%', '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '£', 'à', 'è', 'é', 'ê', '‘', '’', '“', '”', '\\ufeff']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZfqhkYCymwX"
      },
      "source": [
        "In the previous exercise you retrieved the character representations through direct indexing into the vocabulary list, but you can also use `preprocessing.StringLookup(..., invert=True)`. Note: Make sure to use the get_vocabulary() method of the preprocessing.StringLookup layer so that the [UNK] tokens (if any) are appropriately labeled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:24:54.235659Z",
          "iopub.status.busy": "2021-08-11T18:24:54.235049Z",
          "iopub.status.idle": "2021-08-11T18:24:54.240952Z",
          "shell.execute_reply": "2021-08-11T18:24:54.241336Z"
        },
        "id": "Wd2m3mqkDjRj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "435f0ebe-df07-4b05-cf82-78e458bbe70b"
      },
      "source": [
        "# This creates an instance of the \"inverter\"\n",
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        "\n",
        "chars = chars_from_ids(ids) # Now use the inverter to process our tiny example\n",
        "chars"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqTDDxS-s-H8"
      },
      "source": [
        "Note that we have gotten back the byte codes of the characters from the vectors of IDs, and we are seeing them as a `tf.RaggedTensor` of characters. We can now use another utility, tf.strings.reduce_join, to join the characters back into strings. This is demonstrated here because it will be helpful later when we are ready to use our model to generate new text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:24:54.252648Z",
          "iopub.status.busy": "2021-08-11T18:24:54.252070Z",
          "iopub.status.idle": "2021-08-11T18:24:54.263588Z",
          "shell.execute_reply": "2021-08-11T18:24:54.263101Z"
        },
        "id": "zxYI-PeltqKP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82a6679a-4eee-403f-8d3b-ec7a215881af"
      },
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:24:54.267710Z",
          "iopub.status.busy": "2021-08-11T18:24:54.267101Z",
          "iopub.status.idle": "2021-08-11T18:24:54.268757Z",
          "shell.execute_reply": "2021-08-11T18:24:54.269127Z"
        },
        "id": "w5apvBDn9Ind"
      },
      "source": [
        "# Let's turn that utility into a function that we can call.\n",
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd6ur7AamylJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d0c7527-9406-45df-cdef-7b6a267346a3"
      },
      "source": [
        "#\n",
        "# Exercise 9.5: Test the utility function on the small example to make sure\n",
        "# it does what we expect.\n",
        "#\n",
        "text_from_ids(ids)\n",
        "#Yes this is what we expected"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'abcdefg', b'xyz'], dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoNIbPZom7Sw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eb35b1c-edeb-4f40-ceff-300705d51615"
      },
      "source": [
        "#\n",
        "# Exercise 9.6: Create a new small example of example_texts with at least\n",
        "# three character strings in the list. Include some upper and lower case and\n",
        "# some numerals. Run the text through the string preprocessor and then use\n",
        "# the function from exercise 9.5 to recover the original strings.\n",
        "#\n",
        "example_texts_2 = ['hI9Kl', 'mN1p','0Lki']\n",
        "chars_2 = tf.strings.unicode_split(example_texts_2, input_encoding='UTF-8')\n",
        "ids_2 = ids_from_chars(chars_2)\n",
        "chars_from_ids_2 = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        "chars_2 = chars_from_ids_2(ids_2) # Now use the inverter to process our tiny example\n",
        "chars_2\n",
        "text_from_ids(ids_2)\n",
        "\n",
        "#We have taken a new example, split it and use preprocessing.stringLoookup.\n",
        "#Then we inverted to get the letters back and used the text_from_ids method\n",
        "#to recombine them to string"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3,), dtype=string, numpy=array([b'hI9Kl', b'mN1p', b'0Lki'], dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgsVvVxnymwf"
      },
      "source": [
        "### Create training examples and targets\n",
        "\n",
        "Next, we will divide the text into example sequences. Each input sequence will contain `seq_length` characters from the text. For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
        "\n",
        "So break the text into chunks of `seq_length+1`. For example, say `seq_length` is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\". This ensures that we have tons of training examples and that each training example captures context both around the input string and the output string. This is a somewhat different strategy than the word-based example we examined in class, but it uses the same principle: A input sequence is processed by the RNN to predict a target (in this case a target sequence, rather than just one word).\n",
        "\n",
        "To do this first use the `tf.data.Dataset.from_tensor_slices` function to convert the text vector into a stream of character indices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:24:54.275470Z",
          "iopub.status.busy": "2021-08-11T18:24:54.274838Z",
          "iopub.status.idle": "2021-08-11T18:24:54.760641Z",
          "shell.execute_reply": "2021-08-11T18:24:54.761013Z"
        },
        "id": "UopbsKi88tm5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4203256b-f5b0-4b1a-f6a3-4ccc3392e1fb"
      },
      "source": [
        "# Here's where we process all of the characters from the book, which are stored in text.\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "type(all_ids)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.framework.ops.EagerTensor"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnUCkvLa9zgN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1665bfe1-1f24-44d5-e23b-dd5948252f74"
      },
      "source": [
        "#\n",
        "# Exercise 9.7: Use the get_shape() bound method to reveal the shape of the resulting tensor\n",
        "#\n",
        "all_ids.get_shape()\n",
        "#Shape of the tensor is 411576"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([411576])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:24:54.766177Z",
          "iopub.status.busy": "2021-08-11T18:24:54.765609Z",
          "iopub.status.idle": "2021-08-11T18:24:54.767493Z",
          "shell.execute_reply": "2021-08-11T18:24:54.767862Z"
        },
        "id": "qmxrYDCTy-eL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "545452bd-1f89-49f1-f76e-d41cf96ae255"
      },
      "source": [
        "# This creates a dataset whose elements are slices from the original tensor.\n",
        "# This would be a great moment to look at the TF documentation for the \n",
        "# from_tensor_slices() method.\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "type(ids_dataset)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.data.ops.from_tensor_slices_op.TensorSliceDataset"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_loXBnKp-YKC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c34c2f6e-4a1b-4dbb-be5b-796ab74fde2e"
      },
      "source": [
        "#\n",
        "# Exercise 9.8: Run this code and look at the output that it \n",
        "# generates. Then comment each line of the code to say what it is doing.\n",
        "# Also comment on the output. What does it mean?\n",
        "\n",
        "#getting the unique values for ids_dataset\n",
        "temp = ids_dataset.unique()\n",
        "\n",
        "#For loop within the unique values of ids_dataset\n",
        "for element in temp:\n",
        "\n",
        "  #Create the ids as a numpy element\n",
        "  print(element.numpy())\n",
        "\n",
        "#The code loops through the unique set of ids and prints the elements in the id list\n",
        "#"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "94\n",
            "45\n",
            "76\n",
            "73\n",
            "68\n",
            "63\n",
            "61\n",
            "78\n",
            "3\n",
            "36\n",
            "79\n",
            "72\n",
            "60\n",
            "65\n",
            "91\n",
            "77\n",
            "70\n",
            "59\n",
            "83\n",
            "32\n",
            "66\n",
            "69\n",
            "80\n",
            "11\n",
            "48\n",
            "62\n",
            "67\n",
            "30\n",
            "2\n",
            "1\n",
            "49\n",
            "31\n",
            "64\n",
            "81\n",
            "71\n",
            "13\n",
            "54\n",
            "74\n",
            "12\n",
            "41\n",
            "25\n",
            "44\n",
            "37\n",
            "47\n",
            "52\n",
            "38\n",
            "33\n",
            "17\n",
            "15\n",
            "20\n",
            "56\n",
            "34\n",
            "5\n",
            "22\n",
            "24\n",
            "23\n",
            "21\n",
            "57\n",
            "50\n",
            "16\n",
            "35\n",
            "10\n",
            "39\n",
            "43\n",
            "40\n",
            "51\n",
            "82\n",
            "18\n",
            "42\n",
            "92\n",
            "93\n",
            "8\n",
            "9\n",
            "75\n",
            "19\n",
            "26\n",
            "84\n",
            "28\n",
            "46\n",
            "27\n",
            "14\n",
            "55\n",
            "4\n",
            "88\n",
            "85\n",
            "53\n",
            "58\n",
            "87\n",
            "90\n",
            "86\n",
            "89\n",
            "7\n",
            "29\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:24:54.772220Z",
          "iopub.status.busy": "2021-08-11T18:24:54.771623Z",
          "iopub.status.idle": "2021-08-11T18:24:54.789291Z",
          "shell.execute_reply": "2021-08-11T18:24:54.789695Z"
        },
        "id": "cjH5v45-yqqH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d8636e0-469a-4e65-cf9e-01be53ce1ced"
      },
      "source": [
        "# The take() bound method is helpful for examining data in a tensor. It is \n",
        "# modeled after the take() methods from numpy. We can use it in a similar fashion\n",
        "# to head() to extract a small slice of a tensor. \n",
        "for ids in ids_dataset.take(18):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿\n",
            "P\n",
            "r\n",
            "o\n",
            "j\n",
            "e\n",
            "c\n",
            "t\n",
            " \n",
            "G\n",
            "u\n",
            "t\n",
            "e\n",
            "n\n",
            "b\n",
            "e\n",
            "r\n",
            "g\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMCUbYM_4d3-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c93de6df-f69e-4904-bf52-e5e55da6f35e"
      },
      "source": [
        "#\n",
        "# Exercise 9.9: Add the skip() bound method to the expression in the for loop\n",
        "# in the cell just above. To demonstrate how it works, skip 18 elements and then\n",
        "# take the next 10 elements. The invocation of skip() in the expression should\n",
        "# precede the invocation of take().\n",
        "#\n",
        "for ids in ids_dataset.skip(18).take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))\n",
        "\n",
        "#Skip() method is skipping the first 18 elements and taking the next 10 elements of the ids"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "’\n",
            "s\n",
            " \n",
            "P\n",
            "l\n",
            "a\n",
            "y\n",
            "s\n",
            " \n",
            "b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:24:54.793684Z",
          "iopub.status.busy": "2021-08-11T18:24:54.793095Z",
          "iopub.status.idle": "2021-08-11T18:24:54.795288Z",
          "shell.execute_reply": "2021-08-11T18:24:54.795652Z"
        },
        "id": "C-G2oaTxy6km"
      },
      "source": [
        "# While an RNN can theoretically handle a continuous stream of data\n",
        "# here we are considering the data in small groupings whose length\n",
        "# is controlled by seq_length. Leave it at its current value for now, but \n",
        "# in the future you might consider making it either shorter or longer in the\n",
        "# training run.\n",
        "seq_length = 80 # About one line of standard text\n",
        "examples_per_epoch = len(text)//(seq_length+1)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWnRJp-dEpqa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38a05fa8-6181-4573-8c62-f389a8612fb5"
      },
      "source": [
        "#\n",
        "# Exercise 9.10: Display how many examples are used per epoch. What does the\n",
        "# // operator do in Python? Write a comment explaining it.\n",
        "#\n",
        "\n",
        "examples_per_epoch\n",
        "#len(text) - 411576\n",
        "#seq length+1 - 81\n",
        "#5081 examples per epoch\n",
        "#// floors the division result to lowest significant digit"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5081"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZSYAcQV8OGP"
      },
      "source": [
        "The `batch` method lets you easily convert these individual characters to sequences of the desired size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:24:54.801632Z",
          "iopub.status.busy": "2021-08-11T18:24:54.801025Z",
          "iopub.status.idle": "2021-08-11T18:24:54.809747Z",
          "shell.execute_reply": "2021-08-11T18:24:54.810106Z"
        },
        "id": "BpdjRO2CzOfZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f09882c0-1211-4163-a821-3ceb146c5cc0"
      },
      "source": [
        "#\n",
        "# Exercise 9.11: Explain why seq_length+1 is used in the next line of code\n",
        "#\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "#seq_length os 80 is the input target size\n",
        "#seq_length+1 is used to create 81 as the desired size of the output target size.\n",
        "#We are predicting one character from the previous 80 characters.\n",
        "\n",
        "for seq in sequences.take(2):\n",
        "  print(chars_from_ids(seq))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'\\xef\\xbb\\xbf' b'P' b'r' b'o' b'j' b'e' b'c' b't' b' ' b'G' b'u' b't'\n",
            " b'e' b'n' b'b' b'e' b'r' b'g' b'\\xe2\\x80\\x99' b's' b' ' b'P' b'l' b'a'\n",
            " b'y' b's' b' ' b'b' b'y' b' ' b'C' b'h' b'e' b'k' b'h' b'o' b'v' b','\n",
            " b' ' b'S' b'e' b'c' b'o' b'n' b'd' b' ' b'S' b'e' b'r' b'i' b'e' b's'\n",
            " b',' b' ' b'b' b'y' b' ' b'A' b'n' b't' b'o' b'n' b' ' b'C' b'h' b'e'\n",
            " b'k' b'h' b'o' b'v' b'\\r' b'\\n' b'\\r' b'\\n' b'T' b'h' b'i' b's' b' ' b'e'\n",
            " b'B'], shape=(81,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'o' b'o' b'k' b' ' b'i' b's' b' ' b'f' b'o' b'r' b' ' b't' b'h' b'e'\n",
            " b' ' b'u' b's' b'e' b' ' b'o' b'f' b' ' b'a' b'n' b'y' b'o' b'n' b'e'\n",
            " b' ' b'a' b'n' b'y' b'w' b'h' b'e' b'r' b'e' b' ' b'a' b't' b' ' b'n'\n",
            " b'o' b' ' b'c' b'o' b's' b't' b' ' b'a' b'n' b'd' b' ' b'w' b'i' b't'\n",
            " b'h' b'\\r' b'\\n' b'a' b'l' b'm' b'o' b's' b't' b' ' b'n' b'o' b' ' b'r'\n",
            " b'e' b's' b't' b'r' b'i' b'c' b't' b'i' b'o' b'n' b's'], shape=(81,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PHW902-4oZt"
      },
      "source": [
        "It's easier to see what this is doing if you join the tokens back into strings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:24:54.814385Z",
          "iopub.status.busy": "2021-08-11T18:24:54.813718Z",
          "iopub.status.idle": "2021-08-11T18:24:54.824355Z",
          "shell.execute_reply": "2021-08-11T18:24:54.824698Z"
        },
        "id": "QO32cMWu4a06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea296109-ac94-49e6-ac3e-5edbdeaa2515"
      },
      "source": [
        "for seq in sequences.take(2):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'\\xef\\xbb\\xbfProject Gutenberg\\xe2\\x80\\x99s Plays by Chekhov, Second Series, by Anton Chekhov\\r\\n\\r\\nThis eB'\n",
            "b'ook is for the use of anyone anywhere at no cost and with\\r\\nalmost no restrictions'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbLcIPBj_mWZ"
      },
      "source": [
        "For training you'll need a dataset of `(input, label)` pairs, where both `input` and `label` are sequences. At each time step the input is the current character and the label is the next character. Here's a function that takes a sequence as input, duplicates it, and shifts it to align the input and label for each timestep:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:24:54.829238Z",
          "iopub.status.busy": "2021-08-11T18:24:54.828497Z",
          "iopub.status.idle": "2021-08-11T18:24:54.830467Z",
          "shell.execute_reply": "2021-08-11T18:24:54.830891Z"
        },
        "id": "9NGu-FkO_kYU"
      },
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:24:54.835585Z",
          "iopub.status.busy": "2021-08-11T18:24:54.834851Z",
          "iopub.status.idle": "2021-08-11T18:24:54.838059Z",
          "shell.execute_reply": "2021-08-11T18:24:54.837600Z"
        },
        "id": "WxbDTJTw5u_P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3ac5bcb-6538-476d-8079-e5d32212a7df"
      },
      "source": [
        "# Let's test the function - We can do the test using\n",
        "# a regular character string converted to a list.\n",
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3GUPHoPIR6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99ebf924-7a0e-4889-fbe8-b36ed4b5e8a6"
      },
      "source": [
        "#\n",
        "# Exercise 9.12: Retest the function with a list of numbers\n",
        "#\n",
        "split_input_target([1,2,3,7,8])\n",
        "\n",
        "#It is splitting into input and label targets"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([1, 2, 3, 7], [2, 3, 7, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:24:54.864027Z",
          "iopub.status.busy": "2021-08-11T18:24:54.841876Z",
          "iopub.status.idle": "2021-08-11T18:24:54.884561Z",
          "shell.execute_reply": "2021-08-11T18:24:54.884061Z"
        },
        "id": "B9iKPXkw5xwa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b0e959d-a1af-41b4-9ad6-7f45896d741d"
      },
      "source": [
        "# This is a curious construction - i.e., passing a function into the\n",
        "# sequences.map() bound method:\n",
        "dataset = sequences.map(split_input_target)\n",
        "#\n",
        "# Exercise 9.13: Look up the documentation for the map() bound \n",
        "# method and then add a comment to explain the line of code in this block.\n",
        "#\n",
        "\n",
        "#sequences.map() - applies a function to each element in the sequence\n",
        "#To each element in the sequence, we are applying the split_input_target function"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<MapDataset element_spec=(TensorSpec(shape=(80,), dtype=tf.int64, name=None), TensorSpec(shape=(80,), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:24:54.889080Z",
          "iopub.status.busy": "2021-08-11T18:24:54.888454Z",
          "iopub.status.idle": "2021-08-11T18:24:54.907170Z",
          "shell.execute_reply": "2021-08-11T18:24:54.907541Z"
        },
        "id": "GNbw-iR0ymwj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f090538-13fc-4709-cc30-1b09fc111408"
      },
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'\\xef\\xbb\\xbfProject Gutenberg\\xe2\\x80\\x99s Plays by Chekhov, Second Series, by Anton Chekhov\\r\\n\\r\\nThis e'\n",
            "Target: b'Project Gutenberg\\xe2\\x80\\x99s Plays by Chekhov, Second Series, by Anton Chekhov\\r\\n\\r\\nThis eB'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJdfPmdqzf-R"
      },
      "source": [
        "### Create training batches\n",
        "\n",
        "We have used `tf.data` to split the text into manageable sequences. Before using these data to train the model, we need to shuffle the data and pack it into batches. Remember from class that batching, AKA mini-batching, is a method of processing a group of input-output pairs together in the same epoch. Mini-batching facilitates parallelization and can prevent overfitting. Mini-batching also reduces the total number of weight updates that need to occur during a given epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:24:54.912261Z",
          "iopub.status.busy": "2021-08-11T18:24:54.911677Z",
          "iopub.status.idle": "2021-08-11T18:24:54.916643Z",
          "shell.execute_reply": "2021-08-11T18:24:54.916978Z"
        },
        "id": "p2pGotuNzf-S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f97705d-d7d5-4742-f827-6b1866fe84ab"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset:\n",
        "# TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements.\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 80), dtype=tf.int64, name=None), TensorSpec(shape=(64, 80), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TT8t8k-g_27m"
      },
      "source": [
        "#\n",
        "# Exercise 9.14: Examine the documentation for shuffle, batch, and prefetch.\n",
        "# Try starting your exploration here: https://www.tensorflow.org/guide/data_performance\n",
        "# Write a one line comment explaining each concept. Make sure you mention\n",
        "# what AUTOTUNE is.\n",
        "#\n",
        "\n",
        "#These are transformations applied to the map transformation done above\n",
        "#Shuffle - To maintain an internal buffer of elements, it shuffles the elements of dataset \n",
        "#samples are randomly shuffled and randomly sampled like using seed\n",
        "#Batch - it groups the elements into batches of the size we specify - \n",
        "#Prefetch - this is used to overlap the preprocessing and model execution of a training step\n",
        "#It prefetches a number of elements from input and buffers them\n",
        "#This makes them easily available for next iteration\n",
        "#AUTOTUNE - We could manually tune the number of elements to prefetch \n",
        "#the number of batches consumed by a single training step or we could \n",
        "#choose to autotune to get the number of elements"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6oUuElIMgVx"
      },
      "source": [
        "#Build The Model\n",
        "\n",
        "In this section we will create an architecturally simple vanilla RNN model. Make sure you can explain why we have an embedding layer, what the dimensions of the RNN layer are, and how the vocabular size fits into the picture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8gPwEjRzf-Z"
      },
      "source": [
        "This section defines the model as a `keras.Model` subclass (For details see [Making new Layers and Models via subclassing](https://www.tensorflow.org/guide/keras/custom_layers_and_models)). \n",
        "\n",
        "This model has three layers:\n",
        "\n",
        "* `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map each character-ID to a vector with `embedding_dim` dimensions;\n",
        "* `tf.keras.layers.SimpleRNN`: A type of RNN with size `units=rnn_units` (You could also use a GRU or LSTM layer here.)\n",
        "* `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:24:54.921163Z",
          "iopub.status.busy": "2021-08-11T18:24:54.920603Z",
          "iopub.status.idle": "2021-08-11T18:24:54.922081Z",
          "shell.execute_reply": "2021-08-11T18:24:54.922401Z"
        },
        "id": "zHT8cLh7EAsg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a44cc89-a414-471b-d28c-9b6559508c10"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256 # 256 is kind of a generic choice that should work well\n",
        "# for various-sized character vocabularies. There is a danger of overfitting \n",
        "# when the size of the embedding layer exceeds the size of the vocabulary.\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024 # This is tunable. Remember that each RNN unit has a little\n",
        "# bit of memory for what came before. Here 1024 provides four nodes for every\n",
        "# node in the embedding layer. Later, you might want to experiment with half\n",
        "# as many and twice as many.\n",
        "\n",
        "vocab_size, embedding_dim, rnn_units"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(94, 256, 1024)"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q--wMnxH8s4w"
      },
      "source": [
        "# This builds a custom class for instantiating the Keras model\n",
        "#\n",
        "# Exercise 9.15: Add comments on the appropriate lines of code to\n",
        "# document each layer of the model.\n",
        "#\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "\n",
        "    # What's this layer? # Embedding layer\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    \n",
        "    # What's this layer? # RNN layer\n",
        "    self.rnn = tf.keras.layers.SimpleRNN(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    # Why do we need return_sequences=True and return_state=True?\n",
        "    # How will these be used after the model is trained?\n",
        "\n",
        "    # What's this layer? #Dense layer\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.rnn.get_initial_state(x)\n",
        "    x, states = self.rnn(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gz4r3FUMBY9W"
      },
      "source": [
        "We could have used a `keras.Sequential` model here, as this architecture is quite simple. However, to  generate text later we will need to manage the RNN's internal state. It's simpler to include the state input and output options upfront, than it is to rearrange the model architecture later. For more details see the [Keras RNN guide](https://www.tensorflow.org/guide/keras/rnn#rnn_state_reuse)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:24:54.933900Z",
          "iopub.status.busy": "2021-08-11T18:24:54.933342Z",
          "iopub.status.idle": "2021-08-11T18:24:54.945286Z",
          "shell.execute_reply": "2021-08-11T18:24:54.945609Z"
        },
        "id": "IX58Xj9z47Aw"
      },
      "source": [
        "# Now instantiate the class defined above.\n",
        "\n",
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkA5upJIJ7W7"
      },
      "source": [
        "For each character the model looks up the embedding, runs the RNN one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character. A logit can be turned into an odds ratio with exponentiation. Statisticians like to work with logits because they behave linearly. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ubPo0_9Prjb"
      },
      "source": [
        "## Try the model\n",
        "\n",
        "Now run the model to see that it behaves as expected.\n",
        "\n",
        "First check the shape of the output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:24:54.950074Z",
          "iopub.status.busy": "2021-08-11T18:24:54.949469Z",
          "iopub.status.idle": "2021-08-11T18:24:57.944427Z",
          "shell.execute_reply": "2021-08-11T18:24:57.944816Z"
        },
        "id": "C-_70kKAPrPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7fc70b2-4a45-4c48-e23c-1c1a83cb3931"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 80, 95) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6NzLBi4VM4o"
      },
      "source": [
        "In the above example the sequence length of the input is `80` but the model can be run on inputs of any length:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:24:57.950716Z",
          "iopub.status.busy": "2021-08-11T18:24:57.949988Z",
          "iopub.status.idle": "2021-08-11T18:24:57.953371Z",
          "shell.execute_reply": "2021-08-11T18:24:57.953741Z"
        },
        "id": "vPGmAAXmVLGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a440c2c3-2efe-4795-d2ac-7994bd49668e"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  24320     \n",
            "                                                                 \n",
            " simple_rnn (SimpleRNN)      multiple                  1311744   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  97375     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,433,439\n",
            "Trainable params: 1,433,439\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwv0gEkURfx1"
      },
      "source": [
        "To get actual predictions from the model we should sample from the output nodes. The distribution of output node values for any given input is defined by the logits over the character vocabulary. The TF documentation says that it is important to _sample_ from this distribution as taking the _argmax_ of the distribution because that can easily get the model stuck in a loop. \n",
        "\n",
        "Taking the argmax would cause the text generator to make the exact same prediction every time when provided with a particular input. As a result, we could present \"to be\" as the input and get \"to be or not to be or not to be or not to be. . .\" as the output. By sampling from the output instead, we can get a variety of somewhat random (yet high probability) responses each time we present the input, so \"to be\" could generate the response, \"to be or not to be, that is the snorgle.\"\n",
        "\n",
        "Try it for the first example in the batch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:24:57.959137Z",
          "iopub.status.busy": "2021-08-11T18:24:57.958400Z",
          "iopub.status.idle": "2021-08-11T18:24:57.961128Z",
          "shell.execute_reply": "2021-08-11T18:24:57.961509Z"
        },
        "id": "4V4MfFg0RQJg"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM1Vbxs_URw5"
      },
      "source": [
        "This gives us, at each timestep, a prediction of the next character index. You should run the cell above and the cell below several times:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:24:57.967110Z",
          "iopub.status.busy": "2021-08-11T18:24:57.966383Z",
          "iopub.status.idle": "2021-08-11T18:24:57.968952Z",
          "shell.execute_reply": "2021-08-11T18:24:57.969334Z"
        },
        "id": "YqFMUQc_UFgM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58224efa-8132-4543-ec57-86c6aacc8534"
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([67, 33, 65,  0, 36, 53, 15, 51, 71, 13,  5, 12,  9, 34, 43, 46, 10,\n",
              "       92, 84, 20, 15, 63, 68, 76,  2, 12, 47, 86,  1, 26, 32, 10,  4, 93,\n",
              "       56, 22, 13, 88, 39, 89, 54,  1, 68, 82, 82, 65, 36, 69, 57, 79,  0,\n",
              "       60, 47, 79, 19, 72, 73, 77, 69, 66, 63, 74, 51, 45, 28, 24, 35, 47,\n",
              "       29, 44,  5, 61, 37, 42, 75, 70, 12, 57, 41, 15])"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfLtsP3mUhCG"
      },
      "source": [
        "Decode these to see the text predicted by this untrained model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:24:57.974520Z",
          "iopub.status.busy": "2021-08-11T18:24:57.973890Z",
          "iopub.status.idle": "2021-08-11T18:24:57.978884Z",
          "shell.execute_reply": "2021-08-11T18:24:57.979283Z"
        },
        "id": "xWcFwPwLSo05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da420b4e-36a4-4f26-b372-fa6e3618f23c"
      },
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'n hanging about\\r\\nwith you people, going rusty without work. I can\\xe2\\x80\\x99t live without'\n",
            "\n",
            "Next Char Predictions:\n",
            " b'iDg[UNK]GX0Vm.#-)ENQ*\\xe2\\x80\\x9cz50ejr\\r-R\\xc3\\xa0\\n;C*!\\xe2\\x80\\x9d[7.\\xc3\\xa9J\\xc3\\xaaY\\njxxgGk]u[UNK]bRu4noskhepVP?9FR@O#cHMql-]L0'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD17Y8LP9QkS"
      },
      "source": [
        "Naturally, the predicted text is nonsense because the untrained model essentially makes random predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6Afuy77WGND"
      },
      "source": [
        "#\n",
        "# Exercise 9.15: Rerun the previous three code cells. Comment on what you\n",
        "# observe. In particular, why is the input different each time if you are \n",
        "# calling the same code?\n",
        "#\n",
        "\n",
        "#Every time, the input is the same\n",
        "#Output changes everytime because it samples from the highest probability distributions\n",
        "#Keeping the input base same, but the input batch will change every time\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJL0Q0YPY6Ee"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCbHQHiaa4Ic"
      },
      "source": [
        "At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trpqTWyvk0nr"
      },
      "source": [
        "### Attach an optimizer, and a loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAjbjY03eiQ4"
      },
      "source": [
        "The standard `tf.keras.losses.sparse_categorical_crossentropy` loss function works in this case because it is applied across the last dimension of the predictions. Sparse categorical means that the category options are considered mutually exclusive. After all the next character cannot be both a and b - one of those two options should have a higher probability.\n",
        "\n",
        "Because your model returns logits, you need to set the `from_logits` flag.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:24:57.984396Z",
          "iopub.status.busy": "2021-08-11T18:24:57.983751Z",
          "iopub.status.idle": "2021-08-11T18:24:57.985611Z",
          "shell.execute_reply": "2021-08-11T18:24:57.985987Z"
        },
        "id": "ZOeWdgxNFDXq"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:24:57.991174Z",
          "iopub.status.busy": "2021-08-11T18:24:57.990533Z",
          "iopub.status.idle": "2021-08-11T18:24:57.997194Z",
          "shell.execute_reply": "2021-08-11T18:24:57.996735Z"
        },
        "id": "4HrXTACTdzY-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a500cb70-dec8-40a3-ab6b-5f67a4eae645"
      },
      "source": [
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", mean_loss)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 80, 95)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         4.572419\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkvUIneTFiow"
      },
      "source": [
        "A newly initialized model shouldn't be too sure of itself, the output logits should all have similar magnitudes. To confirm this you can check that the exponential of the mean loss is approximately equal to the vocabulary size:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:24:58.001780Z",
          "iopub.status.busy": "2021-08-11T18:24:58.001138Z",
          "iopub.status.idle": "2021-08-11T18:24:58.005003Z",
          "shell.execute_reply": "2021-08-11T18:24:58.004544Z"
        },
        "id": "MAJfS5YoFiHf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2badacb-d02d-42c8-b518-3b089792bad4"
      },
      "source": [
        "tf.exp(mean_loss).numpy()"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "96.77795"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Checkpoint! Report the untrained model (exponentiated) mean_loss from the previous cell\n",
        "\n",
        "The expression tf.exp(mean_loss).numpy() takes the mean loss value, which is expressed in logits, and exponentiates it (effectively creating an odds ratio). Write the value from the previous cell next to your name on the whiteboard. You can round off to one or two significant digits."
      ],
      "metadata": {
        "id": "aI5GqAEeiVXw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTP3PpO5W6JU"
      },
      "source": [
        "#\n",
        "# Exercise 9.16: Add some code that compares the exponentiated mean loss with\n",
        "# the size of the vocabulary (look in earlier cells for this value). If the \n",
        "# exp(mean_loss) is more than 10% larger than the vocab size, print a warning.\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeOXriLcymww"
      },
      "source": [
        "Configure the training procedure using the `tf.keras.Model.compile` method. Use `tf.keras.optimizers.Adam` with default arguments and the loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:24:58.013648Z",
          "iopub.status.busy": "2021-08-11T18:24:58.012966Z",
          "iopub.status.idle": "2021-08-11T18:24:58.019077Z",
          "shell.execute_reply": "2021-08-11T18:24:58.018481Z"
        },
        "id": "DDl1_Een6rL0"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieSJdchZggUj"
      },
      "source": [
        "### Configure checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6XBUUavgF56"
      },
      "source": [
        "Use a `tf.keras.callbacks.ModelCheckpoint` to ensure that checkpoints are saved during training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:24:58.024393Z",
          "iopub.status.busy": "2021-08-11T18:24:58.023777Z",
          "iopub.status.idle": "2021-08-11T18:24:58.025672Z",
          "shell.execute_reply": "2021-08-11T18:24:58.026012Z"
        },
        "id": "W6fWTriUZP-n"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ky3F_BhgkTW"
      },
      "source": [
        "### Execute the training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxdOA-rgyGvs"
      },
      "source": [
        "To keep training time reasonable, use 20 epochs to train the model. In Colab, set the runtime to GPU for faster training. Note that with all of the initial hyperparameters in this notebook, each epoch should take about 10 seconds, so only about 3 minutes to train this model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:24:58.030422Z",
          "iopub.status.busy": "2021-08-11T18:24:58.029840Z",
          "iopub.status.idle": "2021-08-11T18:24:58.031719Z",
          "shell.execute_reply": "2021-08-11T18:24:58.032055Z"
        },
        "id": "7yGBE2zxMMHs"
      },
      "source": [
        "EPOCHS = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:24:58.036492Z",
          "iopub.status.busy": "2021-08-11T18:24:58.035869Z",
          "iopub.status.idle": "2021-08-11T18:26:49.222172Z",
          "shell.execute_reply": "2021-08-11T18:26:49.222645Z"
        },
        "id": "UK-hmKjYVoll"
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_g3O3-KZE-M"
      },
      "source": [
        "#\n",
        "# Exercise 9.17: Click on the file folder in the left hand control bar.\n",
        "# Open up the training_checkpoints folder. How many training checkpoints\n",
        "# do you see. Add a comment saying why there are that many checkpoints.\n",
        "# Try downloading one of those checkpoint files to your computer. How \n",
        "# large is it? Why does it take so much space?\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "## Generate text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIdQ8c8NvMzV"
      },
      "source": [
        "The simplest way to generate text with this model is to run a set of predictions in a loop, while keeping track of the model's internal state as it runs. Each time we call the model we pass in a slice of text and an internal state. \n",
        "\n",
        "The model returns a prediction for the next character as well as its new state. Pass the prediction and state back in to continue generating text. The class defined below accomplishes one step in this chain of model runs. Besides the class initialization, there is only one bound method, called generate_one_step(). When the generate_one_step() bound method is called, it makes a single step prediction. Note the temperature argument on the class initializer. Temperature controls the degree of randomness in the predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:26:49.231962Z",
          "iopub.status.busy": "2021-08-11T18:26:49.231408Z",
          "iopub.status.idle": "2021-08-11T18:26:49.233437Z",
          "shell.execute_reply": "2021-08-11T18:26:49.232954Z"
        },
        "id": "iSBU1tHmlUSs"
      },
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    # The model also returns its internal state so that we can use that\n",
        "    # the next time around the loop.\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:26:49.238176Z",
          "iopub.status.busy": "2021-08-11T18:26:49.237539Z",
          "iopub.status.idle": "2021-08-11T18:26:49.244921Z",
          "shell.execute_reply": "2021-08-11T18:26:49.245283Z"
        },
        "id": "fqMOuDutnOxK"
      },
      "source": [
        "# Now we can instantiate the class. Take note of the arguments we are \n",
        "# passing in. What do the last two arguments do?\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a8hSzw6aiy8"
      },
      "source": [
        "#\n",
        "# Exercise 9.18: Display the type() of each of the agruments passed into\n",
        "# the class initializer in the previous cell. Explain why each of the \n",
        "# three arguments is needed to initialize the OneStep class.\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9yDoa0G3IgQ"
      },
      "source": [
        "Run it in a loop to generate some text. Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Chekhov-like  vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:26:49.251455Z",
          "iopub.status.busy": "2021-08-11T18:26:49.250898Z",
          "iopub.status.idle": "2021-08-11T18:26:51.562788Z",
          "shell.execute_reply": "2021-08-11T18:26:51.562298Z"
        },
        "id": "ST7PSyk9t1mT"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['NATALYA STEPANOVNA.'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Discuss with Your Partner\n",
        "\n",
        "Examine the generated output above closely. What linguistic tasks can the model do correctly? What does it have trouble with? Can you explain why there are spelling errors? Why do you think the model has difficulty producing a grammatically correct sentence?\n",
        "\n",
        "Finally, make sure you can both answer the question of why we used a character generation model for this lab rather than a word-based generation model."
      ],
      "metadata": {
        "id": "uu115Ko3i7Cc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The easiest thing you can do to improve the results is to train it for longer. You can run model.fit() just as you did above and the model will continue training where it left off. Try doing 10 or 20 more epochs."
      ],
      "metadata": {
        "id": "G7IS2X9i4OxK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CrVOOuQbMBx"
      },
      "source": [
        "#\n",
        "# Exercise 9.19: Run 20 more training epochs and then generate additional \n",
        "# characters using the code above. Use the same seed text to start the\n",
        "# model. \n",
        "#\n",
        "# Add a comment indicating if the model's generated text is better than before.\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Exercise 9.20: Plot the training history from the most recent training run.\n",
        "# comment on whether you think that even more training would improve the model.\n",
        "#\n"
      ],
      "metadata": {
        "id": "Pc_n8ZRt4jXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM2Uma_-yVIq"
      },
      "source": [
        "\n",
        "If you want the model to generate text faster the easiest thing you can do is batch the text generation. In the example below the model generates three texts in about the same time it took to generate just one above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:26:51.568807Z",
          "iopub.status.busy": "2021-08-11T18:26:51.568077Z",
          "iopub.status.idle": "2021-08-11T18:26:53.770322Z",
          "shell.execute_reply": "2021-08-11T18:26:53.770785Z"
        },
        "id": "ZkLu7Y8UCMT7"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['NATALYA STEPANOVNA.', 'NATALYA STEPANOVNA.', 'NATALYA STEPANOVNA.'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVqwkgl2b65d"
      },
      "source": [
        "#\n",
        "# Exercise 9.21: Add a comment describing why the model produces different text\n",
        "# even though you have provided the same seed three times.\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVGQ33Tqe_s1"
      },
      "source": [
        "**Improving the Model**\n",
        "\n",
        "There are several strategies that might improve the performance of the model. Try them in the following order:\n",
        "\n",
        "* Reduce the temperature in the initialization of the OneStep class to reduce the randomness in generation of new characters \n",
        "* Increase the number of nodes in the existing SimpleRNN layer to give the model more \"intelligence\"\n",
        "* Add an additional dense layer after the RNN layer to improve the model's capability to make sense of the output of the RNN layer\n",
        "\n",
        "Try these techniques one at a time in the order stated. For the moment we don't have a way of documenting model quality other than the final model loss value and your own read of the generated text to see whether it is creating real words and sensible sentences. Make sure to add comments documenting what you find out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHefPZi-gWHx"
      },
      "source": [
        "#\n",
        "# Exercise 9.21: Make one change at a time to the model to try to improve it.\n",
        "# Run a text using the OneStep predictor class as shown in the code above\n",
        "# to generate new predictions and judge for yourself whether they look more\n",
        "# sensible. You may want to create some additional functions to encapsulate \n",
        "# some of the earlier steps and make it easier to run experiments.\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4QwTjAM6A2O"
      },
      "source": [
        "## Advanced: Customized Training\n",
        "\n",
        "The following material is for strudents who want to dive more deeply into TensorFlow. If you have run out of options for modifying and testing the simple model above and there is time left in the lab, you may want to learn more about improving the quality of a character-based text generation model. Examine the customized training class provided below and spend some time thinking about how it works. \n",
        "\n",
        "All the training we have done to this point is very straightforward and is no different from outher TF/Keras models we've experimented with. Some would call this an open-loop model, because prediction mistakes are not fed back into the model to improve it. In future weeks, we will use \"teacher forcing\" to address this problem. You might want to consult this article for a preview: https://towardsdatascience.com/what-is-teacher-forcing-3da6217fed1c\n",
        "\n",
        "The custom class below improves over our previous training method by closing the \"open loop\" that allowed prediction errors to accumulate. The most important part of a custom training loop is the train step function.\n",
        "\n",
        "The loop uses `tf.GradientTape` to track the gradients. You can learn more about this approach by reading the [eager execution guide](https://www.tensorflow.org/guide/eager).\n",
        "\n",
        "The basic procedure is:\n",
        "\n",
        "1. Execute the model and calculate the loss under a `tf.GradientTape`.\n",
        "2. Calculate the updates and apply them to the model using the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:27:00.454438Z",
          "iopub.status.busy": "2021-08-11T18:27:00.453865Z",
          "iopub.status.idle": "2021-08-11T18:27:00.455636Z",
          "shell.execute_reply": "2021-08-11T18:27:00.455995Z"
        },
        "id": "x0pZ101hjwW0"
      },
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Oc-eJALcK8B"
      },
      "source": [
        "The above implementation of the `train_step` method follows [Keras' `train_step` conventions](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit). This is optional, but it allows you to change the behavior of the train step and still use keras' `Model.compile` and `Model.fit` methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:27:00.460174Z",
          "iopub.status.busy": "2021-08-11T18:27:00.459566Z",
          "iopub.status.idle": "2021-08-11T18:27:00.466960Z",
          "shell.execute_reply": "2021-08-11T18:27:00.467318Z"
        },
        "id": "XKyWiZ_Lj7w5"
      },
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:27:00.473671Z",
          "iopub.status.busy": "2021-08-11T18:27:00.472987Z",
          "iopub.status.idle": "2021-08-11T18:27:00.476534Z",
          "shell.execute_reply": "2021-08-11T18:27:00.476868Z"
        },
        "id": "U817KUm7knlm"
      },
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:27:00.480945Z",
          "iopub.status.busy": "2021-08-11T18:27:00.480244Z",
          "iopub.status.idle": "2021-08-11T18:27:07.814866Z",
          "shell.execute_reply": "2021-08-11T18:27:07.814333Z"
        },
        "id": "o694aoBPnEi9"
      },
      "source": [
        "model.fit(dataset, epochs=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8nAtKHVoInR"
      },
      "source": [
        "Or if you need more control, you can write your own complete custom training loop:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-11T18:27:07.822775Z",
          "iopub.status.busy": "2021-08-11T18:27:07.822150Z",
          "iopub.status.idle": "2021-08-11T18:28:01.685316Z",
          "shell.execute_reply": "2021-08-11T18:28:01.684742Z"
        },
        "id": "d4tSNwymzf-q"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqlxo_UAeujG"
      },
      "source": [
        "#\n",
        "# This training code created a trained \"model\" object, just as the earlier, \n",
        "# simpler code did. So you can use all of the predictive machinery that\n",
        "# appeared earlier in this notebook to test the results.\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}