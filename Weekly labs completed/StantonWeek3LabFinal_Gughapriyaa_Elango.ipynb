{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRWAxlXmZ3qG"
      },
      "source": [
        "**IST664/CIS668: Week 3 Lab: Analyzing Syntax**\n",
        "\n",
        "In the realm of natural language processing, syntax is one level up from morphology. Whereas morphology pertains to the components of words, syntax examines how words are sequenced together.  \n",
        "\n",
        "Although contemporary deep learning methods tend to hide a lot of these details behind the veil of the neural network, syntactical analysis remains a key part of effective NLP solutions - which is why it is such a core process in spaCy. Your ability to create, debug, and successfully modify a natural language system will be enhanced by deepening your understanding of how we use code to assign meaning to various parts of speech as well as the ways that sentences fit together.\n",
        "\n",
        "This lab begins by reading a complete text from the Project Gutenberg website. We are downloading Dostoevsky's Crime and Punishment, as plain text, in a translation by Constance Garnett. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9wxZxwNb3dN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f333f77b-8f2f-4253-b7ae-76a680aaea4e"
      },
      "source": [
        "import nltk # We'll be using lots of facilities from this\n",
        "nltk.download('punkt') # Download, as not included in basic colab\n",
        "\n",
        "# text from online gutenberg\n",
        "from urllib import request # We will need this to read from the URL\n",
        "\n",
        "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "raw = response.read().decode('utf8')\n",
        "type(raw), len(raw)\n",
        "#1176812 characters present in the text"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(str, 1176812)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iw0XuitLcSiR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "eec75fdb-3b26-41c6-9d2a-f50e50ee05aa"
      },
      "source": [
        "# Over one million characters. Let's look at the first few.\n",
        "raw[:178]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ufeffThe Project Gutenberg eBook of Crime and Punishment, by Fyodor Dostoevsky\\r\\n\\r\\nThis eBook is for the use of anyone anywhere in the United States and\\r\\nmost other parts of the world'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVfL-VuX9rNH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1494a9fe-0557-43ce-ac07-940da1fde633"
      },
      "source": [
        "# We'll begin our processing with tokenization \n",
        "crimetokens = nltk.word_tokenize(raw)\n",
        "crimetokens[112:122]\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Release', 'Date', ':', 'March', ',', '2001', '[', 'eBook', '#', '2554']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVVkw7kFfIYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4db2aae-bff8-48f8-e52c-09b7fbfe2847"
      },
      "source": [
        "# Let's keep track of how many unique tokens we're starting with.\n",
        "len(set(crimetokens))\n",
        "#11516 unique tokens"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11516"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Sb09Wv-ekV_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30b51ef0-ce69-433c-e05f-5c2c3c09bb90"
      },
      "source": [
        "# Let's normalize to lower case to reduce the number of unique tokens\n",
        "crimetokens = [w.lower() for w in crimetokens]\n",
        "crimetokens[112:122]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['release', 'date', ':', 'march', ',', '2001', '[', 'ebook', '#', '2554']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptGA8TN4erc4"
      },
      "source": [
        "We discussed stemmers in class last week. Let's compare three stemmers provided by NLTK."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSK7UQRn9rNQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49dfef69-5935-444e-89fb-cb9e1744a963"
      },
      "source": [
        "porter = nltk.PorterStemmer()\n",
        "lancaster = nltk.LancasterStemmer()\n",
        "snowball = nltk.stem.SnowballStemmer('english')\n",
        "type(porter), type(lancaster), type(snowball)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(nltk.stem.porter.PorterStemmer,\n",
              " nltk.stem.lancaster.LancasterStemmer,\n",
              " nltk.stem.snowball.SnowballStemmer)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_iPNGkai58S"
      },
      "source": [
        "Computer scientist Martin Porter wrote and published the Porter Stemmer more than 40 years ago. The Porter stemmer is a rule-based algorithm (i.e., no dictionary) for \"suffix stripping.\" The algorithm was subsequently implemented by other coders in more than two dozen different computer languages. Eventually, Porter got tired of hearing about the implementation errors in some of these other versions, so he rewrote the algorithm in C about 20 years ago. He also created a programming framework, called \"Snowball\" that can be used to create additional stemmers including the third one above, which is also known as the Porter2 stemmer. \n",
        "\n",
        "The Lancaster stemmer, also known as the Paice/Husk stemmer, was created at Lancaster University and has the advantage that the \"rule book\" it uses is external to the algorithm itself and can therefore be adapted to languages other than English."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMgJIRW-eMVL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20c4a8a9-87cc-4985-c0fb-ba8949fa393c"
      },
      "source": [
        "# From a data reduction standpoint, which stemmer results in the greatest\n",
        "# reduction in the number of unique tokens? Remember that we started with 11539.\n",
        "crimePstem = [porter.stem(t) for t in crimetokens]\n",
        "crimeLstem = [lancaster.stem(t) for t in crimetokens]\n",
        "crimeSstem = [snowball.stem(t) for t in crimetokens]\n",
        "\n",
        "len(set(crimePstem)), len(set(crimeLstem)), len(set(crimeSstem))\n",
        "\n",
        "#Lancaster stemmer gives the greatest reduction in the number of unique tokens\n",
        "#lowest - 6399"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7363, 6399, 7174)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mgq8DbXpPE8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "148ef234-6b74-42e4-be13-da0e613e3c91"
      },
      "source": [
        "# What proportion of reduction have we achieved with the Porter stemmer?\n",
        "len(set(crimePstem))/len(set(crimetokens))\n",
        "#Porter stemmer - 68.9% reduction"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6890323788134007"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pv2hXaupp-Ia",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f644454a-9cf5-4998-e4f8-5dcc0fc3e5af"
      },
      "source": [
        "# The Porter stemmer produced a set roughly 69% of the size of the original\n",
        "# vocabulary. Now calculate and show the percent reduction in the number of \n",
        "# tokens for the other two stemmers.\n",
        "\n",
        "# 3.1: Compute and display reduction ratio for the Lancaster stemmer\n",
        "len(set(crimeLstem))/len(set(crimetokens))\n",
        "#Lancaster stemmer - 59.8% reduction"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5988208871420551"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# 3.2: Compute and display reduction ratio for the Snowball stemmer\n",
        "len(set(crimeSstem))/len(set(crimetokens))\n",
        "#Snowball Stemmer - 67.1% reduction"
      ],
      "metadata": {
        "id": "SrE5Be08yg--",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "371dc23c-6713-407d-8275-26653a5839b4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6713456859442261"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRdy05ubPszU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60c84281-aa61-4366-b561-8f3867e319d7"
      },
      "source": [
        "# Let's compare the highest frequency tokens from the three stemmers\n",
        "from tabulate import tabulate\n",
        "from nltk import FreqDist\n",
        "pdist = FreqDist(crimePstem)\n",
        "ldist = FreqDist(crimeLstem)\n",
        "sdist = FreqDist(crimeSstem)\n",
        "\n",
        "# zip() is a cool built-in function for zipping together two or\n",
        "# more lists/tuples into a single iterator.\n",
        "compare = zip(pdist.most_common(20),\n",
        "              ldist.most_common(20),\n",
        "              sdist.most_common(20))\n",
        "\n",
        "print(tabulate(compare, headers=[\"Porter\", \"Lancaster\", \"Snowball\"]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter          Lancaster       Snowball\n",
            "--------------  --------------  --------------\n",
            "(',', 16177)    (',', 16177)    (',', 16177)\n",
            "('.', 8908)     ('.', 8908)     ('.', 8908)\n",
            "('the', 8006)   ('the', 8038)   ('the', 8006)\n",
            "('and', 7031)   ('and', 7031)   ('and', 7031)\n",
            "('to', 5350)    ('to', 5350)    ('to', 5350)\n",
            "('he', 4769)    ('he', 4769)    ('he', 4769)\n",
            "('a', 4651)     ('a', 4651)     ('a', 4651)\n",
            "('i', 4397)     ('i', 4397)     ('i', 4397)\n",
            "('you', 4086)   ('you', 4094)   ('you', 4086)\n",
            "('’', 4039)     ('’', 4039)     ('’', 4039)\n",
            "('“', 3980)     ('“', 3980)     ('“', 3980)\n",
            "('”', 3929)     ('”', 3929)     ('”', 3929)\n",
            "('of', 3927)    ('of', 3927)    ('of', 3927)\n",
            "('it', 3474)    ('it', 3474)    ('it', 3474)\n",
            "('that', 3282)  ('that', 3282)  ('that', 3282)\n",
            "('in', 3248)    ('in', 3261)    ('in', 3248)\n",
            "('wa', 2826)    ('was', 2826)   ('was', 2826)\n",
            "('!', 2364)     ('on', 2606)    ('!', 2364)\n",
            "('?', 2275)     ('!', 2364)     ('?', 2275)\n",
            "('hi', 2114)    ('?', 2275)     ('his', 2113)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Comments from discussion\n",
        "Lancaster stemmer stems words like \"these\" to \"the\" which could explain why Lancaster has more \"the\" compared to other stemmers. Lancaster ignores the demonstrative pronouns like \"these\". It clubs the part of speeches to bigger chunks, so we acheive less unique words. \n",
        "Porter stemmer stems all the -s in words thinking it is a plural form. Hence was gets converted to -wa. It is not able to detect past verbs and stems them.\n",
        "Snowball appears to be a better stemmer than the other two. Even though it doesnot reduce tokens as much, it can stem with better part of speech. "
      ],
      "metadata": {
        "id": "xLNiNkLKtw2v"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7hFZwVJRbYr"
      },
      "source": [
        "#Discuss with Your Partner\n",
        "\n",
        "There's a lot going on in the display just above. All three stemmers agree on commas, periods, the word \"and,\" and the close double quote. Can you think of some reasons why the word \"the\" has a different count for the Lancaster stemmer? Discuss this point with your lab partner.\n",
        "\n",
        "What's going on near the end of the list where we have the following output:\n",
        "\n",
        "(('wa', 2825), ('was', 2825), ('was', 2825))\n",
        "\n",
        "The counts match, but what has the Porter stemmer done differently? Even based on the small amount of evidence above, what conclusions can you draw about the advantages and disadvantages of various stemmers?\n",
        "\n",
        "\n",
        "\n",
        "Because stemming is quite variable in the results it produces, some NLP processing methods use lemmatization instead. A lemma is the root form on a word. In English one of the most striking set of lemmas comes from the verb \"to be.\" The words \"am,\" \"is,\" \"are,\" and \"be,\" despite their unique spellings and pronunciations, all lemmatize to \"be.\" Let's try this with the Wordnet Lemmatizer: \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjBF4aidUt0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8230e82-eaae-4dc5-e7eb-142289e3d808"
      },
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "wnl = nltk.WordNetLemmatizer()\n",
        "type(wnl)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nltk.stem.wordnet.WordNetLemmatizer"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kV6ONqbUz1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3cf477da-3227-45bc-cec5-fe16724631e2"
      },
      "source": [
        "wnl.lemmatize(\"am\", pos =\"v\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'be'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_Ab7tmyWKZm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5978fdc8-e622-4ccb-8943-e1080ee215da"
      },
      "source": [
        "# Now lemmatize \"is\" and \"are\" using wnl.lemmatize(). \n",
        "# Also test what happens if you leave out the pos argument? \n",
        "# Write a comment describing what the pos argument does.\n",
        "\n",
        "# 3.3: Lemmatize is, using pos=\"v\"\n",
        "print(\"is : \",wnl.lemmatize(\"is\", pos=\"v\"))\n",
        "\n",
        "#is : be"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "is :  be\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# 3.4: Lemmatize are, using pos=\"v\"\n",
        "print(\"are : \",wnl.lemmatize(\"are\", pos=\"v\"))\n",
        "#are : be"
      ],
      "metadata": {
        "id": "swTN4DXmzSmO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fae401ff-1bc3-44c1-8a6c-322d7e012c96"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "are :  be\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# 3.5: Test the lemmatize method without the pos argument\n",
        "#\n",
        "print(\"is : \",wnl.lemmatize(\"is\"))\n",
        "\n",
        "#is returns is when given without pos = v"
      ],
      "metadata": {
        "id": "rGwN7sw6zXzG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83d92b4f-7622-45f4-ac21-1ef604a24287"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "is :  is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuGJQYjMcs_a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49f411ce-ff6d-4ef8-839b-b5172e52920e"
      },
      "source": [
        "# Let's lemmatize Crime and Punishment to see what we get:\n",
        "crimelemma = [wnl.lemmatize(t) for t in crimetokens]\n",
        "\n",
        "len(set(crimelemma))\n",
        "#Unique lemmatized tokens :9793"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9793"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRndRboHc-E0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b2308c0-85e3-427d-ebb3-6f128c94dc83"
      },
      "source": [
        "# What proportion of reduction have we achieved with the lemmatizer?\n",
        "len(set(crimelemma))/len(set(crimetokens))\n",
        "\n",
        "#We have acheived less reduction compared to other stemmers\n",
        "#0.9164327157027887"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9164327157027887"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At about 92%, the WordNet lemmatizer does not achieve as much data reduction as the stemming methods. WordNet really seems to require that the user specify the part of speech. Without that specification, there are likely to be errors.\n",
        "\n",
        "Switching gears for a moment, one way of capturing more contextual information in our token lists is to analyze tokens in sets of two or more. Two tokens together is called a bigram, three is called a trigram, and more generally any number \"n\" is called an ngram. NLTK and other language packages contain numerous tools for working with bigrams. Let's look at the output of the NLTK ngrams() function:"
      ],
      "metadata": {
        "id": "oFxkTiX50X9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rather than a whole book, let's begin by working with one sentence:\n",
        "sentence = \"thomas jefferson began building monticello at the age of twenty-six.\"\n",
        "len(sentence)"
      ],
      "metadata": {
        "id": "r1LV66cE0Xla",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d5229a1-0cf2-4a3d-acbb-cdbb7a76e6f5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "68"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "import re # Regular expressions library\n",
        "pattern = re.compile('[a-z]+')\n",
        "tokens = pattern.findall(sentence)\n",
        "list(ngrams(tokens, 2))"
      ],
      "metadata": {
        "id": "LPhN_iydrW1M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3012d67c-2aad-4e67-dd39-2ba68d4a4375"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('thomas', 'jefferson'),\n",
              " ('jefferson', 'began'),\n",
              " ('began', 'building'),\n",
              " ('building', 'monticello'),\n",
              " ('monticello', 'at'),\n",
              " ('at', 'the'),\n",
              " ('the', 'age'),\n",
              " ('age', 'of'),\n",
              " ('of', 'twenty'),\n",
              " ('twenty', 'six')]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can easily repeat the process for trigrams:\n",
        "list(ngrams(tokens, 3))"
      ],
      "metadata": {
        "id": "MxxAEH6e0j7C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a12b27b-6295-4a3f-8513-1f1f3d03a61d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('thomas', 'jefferson', 'began'),\n",
              " ('jefferson', 'began', 'building'),\n",
              " ('began', 'building', 'monticello'),\n",
              " ('building', 'monticello', 'at'),\n",
              " ('monticello', 'at', 'the'),\n",
              " ('at', 'the', 'age'),\n",
              " ('the', 'age', 'of'),\n",
              " ('age', 'of', 'twenty'),\n",
              " ('of', 'twenty', 'six')]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It should be pretty clear what's happening in both of the previous cells. Also, it may not seem especially useful, but research has shown that there is  value in understanding the context around words - i.e., the other words that occur nearby. In fact, this was an idea called \"the distributional hypothesis\" imagined by linguist Zellig Harris, that words with similar meanings tend to occur in similar contexts."
      ],
      "metadata": {
        "id": "J-BiCdAM0xBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can create ngram token strings from these lists.\n",
        "bigrams = [\" \".join(w) for w in ngrams(tokens, 2)]\n",
        "print(bigrams) # If we were going to use CountVectorizer, this would be the input"
      ],
      "metadata": {
        "id": "xhCwSXoG0nz6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dddea48d-b16d-4b97-de52-78c5eb93e0b4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['thomas jefferson', 'jefferson began', 'began building', 'building monticello', 'monticello at', 'at the', 'the age', 'age of', 'of twenty', 'twenty six']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# 3.5: Build trigram tokens from the Thomas Jefferson tokens.\n",
        "# Make sure the trigram tokens have spaces between the component words as shown\n",
        "# in the bigram example in the code block just above.\n",
        "#\n",
        "trigrams = [\" \".join(w) for w in ngrams(tokens, 3)]\n",
        "print(trigrams) "
      ],
      "metadata": {
        "id": "TGG43sOl3sDp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6da1e41d-d6e9-408b-9713-a6e772bb9066"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['thomas jefferson began', 'jefferson began building', 'began building monticello', 'building monticello at', 'monticello at the', 'at the age', 'the age of', 'age of twenty', 'of twenty six']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Given only one sentence, that result is not very exciting, but what if\n",
        "# we did a whole book?\n",
        "nltk.download('stopwords')\n",
        "nltk_stops = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "crimenopunct = [w for w in crimetokens if w.isalnum()]\n",
        "crimenostops = [w for w in crimenopunct if w not in nltk_stops]\n",
        "crimebigrams = [\" \".join(w) for w in ngrams(crimenostops, 2)]\n",
        "\n",
        "fdist = FreqDist(crimebigrams) # This creates a list of frequencies for bigrams\n",
        "len(fdist) # This is the total number of unique bigrams\n",
        "\n",
        "#75363 number of unique bigrams in the book"
      ],
      "metadata": {
        "id": "Rm1YYKtF0-QV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f8b101a-53a3-49a5-f993-dc1b47d7bb21"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "75363"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fdist.most_common(10) # What do you notice about the most frequent bigrams\n",
        "\n",
        "#Most frequent bigrams are Names of the characters in book. \n",
        "#Most likely first and last name appears together"
      ],
      "metadata": {
        "id": "UWX0Lle-1Fg8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dca37fbb-0d22-4d40-d287-68b51624d54c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('katerina ivanovna', 215),\n",
              " ('pyotr petrovitch', 172),\n",
              " ('pulcheria alexandrovna', 123),\n",
              " ('avdotya romanovna', 112),\n",
              " ('old woman', 91),\n",
              " ('rodion romanovitch', 82),\n",
              " ('porfiry petrovitch', 81),\n",
              " ('marfa petrovna', 76),\n",
              " ('sofya semyonovna', 71),\n",
              " ('amalia ivanovna', 54)]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dayl1Ab3kYSv"
      },
      "source": [
        "**Part Two**\n",
        "\n",
        "At this point we know that the WordNet lemmatizer does not work very well unless we already know the part of speech of the word we are trying to lemmatize. This is a significant limitation, which is also reflected in the fact that we only achieved an 8% reduction in the number of unique tokens using this lemmatizer.\n",
        "\n",
        "Given the limitations of stemmers and simple lemmatizers, it is time to take a more serious look at part of speech tagging. For this, we are going to graduate from NLTK to our first effort with spaCy. Whereas NLTK was designed for teaching and research, spaCy was architected so that it can serve as the basis of a production-grade NLP pipeline. Unlike other NLP toolkits (e.g., Stanford core NLP) spaCy was written in Python and Cython, so it is convenient for use directly from the Jupyter notebook environment. We will do a thorough examination of many of spaCy's capabilities in the Week 5 lab. For now, we will just try out a few basic techniques.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwIUu68mwY4Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05e16b8d-f752-458e-ceff-c6ef08a90605"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm') # sm means small - some pipeline capabilities not loaded\n",
        "type(nlp) # This is our pipeline: an instantiated class that we can use to process any string\n",
        "# You can ignore this warning if you see it: \"UserWarning: Can't initialize NVML\""
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.lang.en.English"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hV4_95-Vy7Qg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b430917-0f27-48b5-ec7d-c34c0646b847"
      },
      "source": [
        "# Let's process a small example from NLPIA first\n",
        "sentence = \"The faster Harry got to the store, the faster Harry would get home.\"\n",
        "spsent = nlp(sentence)\n",
        "type(spsent), len(spsent)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(spacy.tokens.doc.Doc, 15)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rVaDDfizZqn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd0950f5-79ae-4682-8487-db62b1361753"
      },
      "source": [
        "# But this is no ordinary set of string tokens:\n",
        "# What bound methods and attributes are available for this parsed object?\n",
        "[m for m in dir(spsent) if m[0] != '_']"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cats',\n",
              " 'char_span',\n",
              " 'copy',\n",
              " 'count_by',\n",
              " 'doc',\n",
              " 'ents',\n",
              " 'extend_tensor',\n",
              " 'from_array',\n",
              " 'from_bytes',\n",
              " 'from_dict',\n",
              " 'from_disk',\n",
              " 'from_docs',\n",
              " 'from_json',\n",
              " 'get_extension',\n",
              " 'get_lca_matrix',\n",
              " 'has_annotation',\n",
              " 'has_extension',\n",
              " 'has_unknown_spaces',\n",
              " 'has_vector',\n",
              " 'is_nered',\n",
              " 'is_parsed',\n",
              " 'is_sentenced',\n",
              " 'is_tagged',\n",
              " 'lang',\n",
              " 'lang_',\n",
              " 'mem',\n",
              " 'noun_chunks',\n",
              " 'noun_chunks_iterator',\n",
              " 'remove_extension',\n",
              " 'retokenize',\n",
              " 'sentiment',\n",
              " 'sents',\n",
              " 'set_ents',\n",
              " 'set_extension',\n",
              " 'similarity',\n",
              " 'spans',\n",
              " 'tensor',\n",
              " 'text',\n",
              " 'text_with_ws',\n",
              " 'to_array',\n",
              " 'to_bytes',\n",
              " 'to_dict',\n",
              " 'to_disk',\n",
              " 'to_json',\n",
              " 'to_utf8_array',\n",
              " 'user_data',\n",
              " 'user_hooks',\n",
              " 'user_span_hooks',\n",
              " 'user_token_hooks',\n",
              " 'vector',\n",
              " 'vector_norm',\n",
              " 'vocab']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wg6tH-vrzvAI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96052870-e6b5-4179-90f7-bf0f0456ec13"
      },
      "source": [
        "# So there are quite a number of attributes and bound methods for\n",
        "# this collection of tokens. We will learn more of them eventually\n",
        "# but for now, let's just look at one attribute.\n",
        "spsent.has_annotation(\"TAG\") # What does this one tell us?\n",
        "\n",
        "#There is a method called \"tag\""
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HprBAeam5R80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b0540ce-1415-4d04-dd74-b115a43ce89f"
      },
      "source": [
        "# So spaCy has guessed the part of speech for each token. We can easily list\n",
        "# all of the tags.\n",
        "tags = [(i, i.pos_) for i in spsent]\n",
        "print(tabulate(tags, headers=[\"Token\", \"POS Tag\"]))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token    POS Tag\n",
            "-------  ---------\n",
            "The      DET\n",
            "faster   ADJ\n",
            "Harry    PROPN\n",
            "got      VERB\n",
            "to       ADP\n",
            "the      DET\n",
            "store    NOUN\n",
            ",        PUNCT\n",
            "the      PRON\n",
            "faster   ADJ\n",
            "Harry    PROPN\n",
            "would    AUX\n",
            "get      VERB\n",
            "home     ADV\n",
            ".        PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zLxoqQH6HJf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4f7a477-2298-4774-fe7b-5fd2f28446fb"
      },
      "source": [
        "# SpaCy has also stored the lemmas for each token\n",
        "# Let's show the lemmas and clean up our output. We can use the\n",
        "# tabulate package to make clean, simple display tables.\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Make a little dataset for tabulate() to work on.\n",
        "poslist = [ (i, i.lemma_, i.pos_) for i in spsent]\n",
        "\n",
        "print(tabulate(poslist,  headers=[\"Token\", \"Lemma\", \"Tag\"]))\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token    Lemma    Tag\n",
            "-------  -------  -----\n",
            "The      the      DET\n",
            "faster   fast     ADJ\n",
            "Harry    Harry    PROPN\n",
            "got      get      VERB\n",
            "to       to       ADP\n",
            "the      the      DET\n",
            "store    store    NOUN\n",
            ",        ,        PUNCT\n",
            "the      the      PRON\n",
            "faster   fast     ADJ\n",
            "Harry    Harry    PROPN\n",
            "would    would    AUX\n",
            "get      get      VERB\n",
            "home     home     ADV\n",
            ".        .        PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipfZfKyp8zAZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21644731-8a46-441e-f116-fc1ffc34fe16"
      },
      "source": [
        "# Take a peek at those tags. Some of them should be pretty obvious. For how many\n",
        "# of them can you guess what part of speech it is referring to? Also take a \n",
        "# look at the lemmas.\n",
        "\n",
        "# There's even more info in there for every token. In particular, pay attention\n",
        "# to the \"is_\" tests: There are 18 tests that you can do on any token to\n",
        "# see how spaCy has categorized it.\n",
        "[m for m in dir(spsent[0]) if m[0:2] == 'is']"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['is_alpha',\n",
              " 'is_ancestor',\n",
              " 'is_ascii',\n",
              " 'is_bracket',\n",
              " 'is_currency',\n",
              " 'is_digit',\n",
              " 'is_left_punct',\n",
              " 'is_lower',\n",
              " 'is_oov',\n",
              " 'is_punct',\n",
              " 'is_quote',\n",
              " 'is_right_punct',\n",
              " 'is_sent_end',\n",
              " 'is_sent_start',\n",
              " 'is_space',\n",
              " 'is_stop',\n",
              " 'is_title',\n",
              " 'is_upper']"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5DfLI85mOUH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f6b45a0-d45b-4eae-ffac-f276e87fb204"
      },
      "source": [
        "# Let's make a more detailed table with a few of these fields.\n",
        "poslist = [ (i, i.head, i.lemma_, i.pos_, i.tag_, i.is_alpha) for i in spsent]\n",
        "\n",
        "print(tabulate(poslist,  headers=[\"Token\", \"Head\", \"Lemma\", \"Tag\", \"Details\",\"Alpha?\"]))\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token    Head    Lemma    Tag    Details    Alpha?\n",
            "-------  ------  -------  -----  ---------  --------\n",
            "The      Harry   the      DET    DT         True\n",
            "faster   Harry   fast     ADJ    JJR        True\n",
            "Harry    got     Harry    PROPN  NNP        True\n",
            "got      get     get      VERB   VBD        True\n",
            "to       got     to       ADP    IN         True\n",
            "the      store   the      DET    DT         True\n",
            "store    to      store    NOUN   NN         True\n",
            ",        get     ,        PUNCT  ,          False\n",
            "the      faster  the      PRON   DT         True\n",
            "faster   Harry   fast     ADJ    JJR        True\n",
            "Harry    get     Harry    PROPN  NNP        True\n",
            "would    get     would    AUX    MD         True\n",
            "get      get     get      VERB   VB         True\n",
            "home     get     home     ADV    RB         True\n",
            ".        get     .        PUNCT  .          False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-XJZVvXVv4TN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZo9P_EJoUsr"
      },
      "source": [
        "The table above just scratches the surface, but there's still a lot of interesting stuff happening there. In the first column we have the token itself, which can be a word, a number, or punctuation. The second column starts to unpack the idea of dependency grammar - that each word in a sentence represents a portion of a tree, with \"ancestors\" that it depends on and \"children\" that depend on it. \"Head\" refers to the immediate ancestor of a word. So for instance, the proper noun \"Harry\" depends on the corresponding verb \"got.\" Next we have the lemmas and the simple part of speech tag as before. By the way, you can find an explanation of these tags here:\n",
        "\n",
        "https://universaldependencies.org/docs/u/pos/\n",
        "\n",
        "Finally, there is a fine-grained part of speech - a more complicated tag provided by spaCy. These are unique to each language model, but there is a function call that will provide information about any of the tags:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJA12TMdoI1C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fa2d273c-563f-4b36-ffae-40d05158b54d"
      },
      "source": [
        "spacy.explain(\"JJR\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'adjective, comparative'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Checkpoint! Use spacy.explain(\"RB\")\n",
        "\n",
        "In the empty code box below, add and run spacy.explain(\"RB\"). The tag \"RB\" is from the detail field of the last word (home) in the tabular output just above. The method will return the part of speech that \"RB\" refers to. Write that part of speech on the whiteboard next to your name."
      ],
      "metadata": {
        "id": "3UIPOTKM1u2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.explain(\"RB\")"
      ],
      "metadata": {
        "id": "tCAlMrxH1o3p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bb6a1ed2-2e55-4143-dfd0-b70ca342d49d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'adverb'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjmOSyCrr5WE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38ffc327-8959-433a-b32d-05ecada7681a"
      },
      "source": [
        "# Let's practice by tagging another sentence. Here's some text extracted from\n",
        "# Wikipedia's article on kites.\n",
        "kites = \"\"\"A kite is a tethered heavier-than-air or lighter-than-air craft with wing surfaces that react against the air to create lift and drag forces. \n",
        "A kite consists of wings, tethers and anchors. Kites often have a bridle and tail to guide the face of the kite so the wind can lift it. \n",
        "Some kite designs don’t need a bridle; box kites can have a single attachment point. \n",
        "A kite may have fixed or moving anchors that can balance the kite. \n",
        "One technical definition is that a kite is “a collection of tether-coupled wing sets“.\n",
        "The name derives from its resemblance to a hovering bird.\"\"\"\n",
        "\n",
        "spkites = nlp(kites)\n",
        "type(spkites), len(spkites)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(spacy.tokens.doc.Doc, 130)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTE4tWNIsoFd"
      },
      "source": [
        "# Add code to conduct the following analyses:\n",
        "\n",
        "# 3.6: Display tokens, lemmas, and parts of speech for spkites. Try using a\n",
        "# nice, neat tabular format for the output.\n",
        "\n",
        "poslist2 = [ (i, i.lemma_, i.pos_) for i in spkites]\n",
        "\n",
        "print(tabulate(poslist2,  headers=[\"Token\", \"Lemma\", \"Tag\"]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSdCtkO-uvX7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "097d467c-918d-46d3-bce8-86d8fb6a42fa"
      },
      "source": [
        "# It might be more convenient to work with individual sentences:\n",
        "kitespans = list(spkites.sents)\n",
        "\n",
        "kitespans[0] # Let's view just the first sentence"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "A kite is a tethered heavier-than-air or lighter-than-air craft with wing surfaces that react against the air to create lift and drag forces. "
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wm-eThSIvV6g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "outputId": "e62cdd69-a6a7-4df2-b02a-b611e8a66cc2"
      },
      "source": [
        "# One other neat trick: We can use spaCy to display a graphical\n",
        "# version of the dependence tree for any sentence or document.\n",
        "# You saw this in an exercise in class.\n",
        "from spacy import displacy \n",
        "displacy.render(kitespans[0], style=\"dep\", jupyter=True)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"bbcd3a17abb04cdcafc42b9b08460b4c-0\" class=\"displacy\" width=\"4950\" height=\"749.5\" direction=\"ltr\" style=\"max-width: none; height: 749.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">A</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">kite</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">is</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">a</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">tethered</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">heavier-</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">than-</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">air</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">or</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">CCONJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">lighter-</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">than-</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">air</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">craft</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">with</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2500\">wing</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2500\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2675\">surfaces</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2675\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2850\">that</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2850\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3025\">react</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3025\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3200\">against</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3200\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3375\">the</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3375\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3550\">air</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3550\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3725\">to</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3725\">PART</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3900\">create</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3900\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4075\">lift</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4075\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4250\">and</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4250\">CCONJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4425\">drag</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4425\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4600\">forces.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4600\">PUNCT</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"659.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4775\">\n",
              "</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4775\">SPACE</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-0\" stroke-width=\"2px\" d=\"M70,614.5 C70,527.0 195.0,527.0 195.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,616.5 L62,604.5 78,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-1\" stroke-width=\"2px\" d=\"M245,614.5 C245,527.0 370.0,527.0 370.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,616.5 L237,604.5 253,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-2\" stroke-width=\"2px\" d=\"M595,614.5 C595,177.0 2140.0,177.0 2140.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M595,616.5 L587,604.5 603,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-3\" stroke-width=\"2px\" d=\"M770,614.5 C770,264.5 2135.0,264.5 2135.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M770,616.5 L762,604.5 778,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-4\" stroke-width=\"2px\" d=\"M945,614.5 C945,352.0 2130.0,352.0 2130.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M945,616.5 L937,604.5 953,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-5\" stroke-width=\"2px\" d=\"M945,614.5 C945,527.0 1070.0,527.0 1070.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1070.0,616.5 L1078.0,604.5 1062.0,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-6\" stroke-width=\"2px\" d=\"M1120,614.5 C1120,527.0 1245.0,527.0 1245.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1245.0,616.5 L1253.0,604.5 1237.0,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-7\" stroke-width=\"2px\" d=\"M1295,614.5 C1295,527.0 1420.0,527.0 1420.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1420.0,616.5 L1428.0,604.5 1412.0,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-8\" stroke-width=\"2px\" d=\"M1295,614.5 C1295,439.5 1600.0,439.5 1600.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1600.0,616.5 L1608.0,604.5 1592.0,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-9\" stroke-width=\"2px\" d=\"M1645,614.5 C1645,527.0 1770.0,527.0 1770.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1770.0,616.5 L1778.0,604.5 1762.0,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-10\" stroke-width=\"2px\" d=\"M1820,614.5 C1820,527.0 1945.0,527.0 1945.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1945.0,616.5 L1953.0,604.5 1937.0,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-11\" stroke-width=\"2px\" d=\"M420,614.5 C420,89.5 2145.0,89.5 2145.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2145.0,616.5 L2153.0,604.5 2137.0,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-12\" stroke-width=\"2px\" d=\"M2170,614.5 C2170,527.0 2295.0,527.0 2295.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2295.0,616.5 L2303.0,604.5 2287.0,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-13\" stroke-width=\"2px\" d=\"M2520,614.5 C2520,527.0 2645.0,527.0 2645.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2520,616.5 L2512,604.5 2528,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-14\" stroke-width=\"2px\" d=\"M2345,614.5 C2345,439.5 2650.0,439.5 2650.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2650.0,616.5 L2658.0,604.5 2642.0,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-15\" stroke-width=\"2px\" d=\"M2870,614.5 C2870,527.0 2995.0,527.0 2995.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2870,616.5 L2862,604.5 2878,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-16\" stroke-width=\"2px\" d=\"M2170,614.5 C2170,352.0 3005.0,352.0 3005.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-16\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">relcl</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M3005.0,616.5 L3013.0,604.5 2997.0,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-17\" stroke-width=\"2px\" d=\"M3045,614.5 C3045,527.0 3170.0,527.0 3170.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-17\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M3170.0,616.5 L3178.0,604.5 3162.0,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-18\" stroke-width=\"2px\" d=\"M3395,614.5 C3395,527.0 3520.0,527.0 3520.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-18\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M3395,616.5 L3387,604.5 3403,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-19\" stroke-width=\"2px\" d=\"M3220,614.5 C3220,439.5 3525.0,439.5 3525.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-19\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M3525.0,616.5 L3533.0,604.5 3517.0,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-20\" stroke-width=\"2px\" d=\"M3745,614.5 C3745,527.0 3870.0,527.0 3870.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-20\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M3745,616.5 L3737,604.5 3753,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-21\" stroke-width=\"2px\" d=\"M3045,614.5 C3045,352.0 3880.0,352.0 3880.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-21\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advcl</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M3880.0,616.5 L3888.0,604.5 3872.0,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-22\" stroke-width=\"2px\" d=\"M3920,614.5 C3920,527.0 4045.0,527.0 4045.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-22\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M4045.0,616.5 L4053.0,604.5 4037.0,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-23\" stroke-width=\"2px\" d=\"M4095,614.5 C4095,527.0 4220.0,527.0 4220.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-23\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M4220.0,616.5 L4228.0,604.5 4212.0,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-24\" stroke-width=\"2px\" d=\"M4445,614.5 C4445,527.0 4570.0,527.0 4570.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-24\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M4445,616.5 L4437,604.5 4453,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-25\" stroke-width=\"2px\" d=\"M420,614.5 C420,2.0 4600.0,2.0 4600.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-25\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">punct</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M4600.0,616.5 L4608.0,604.5 4592.0,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-26\" stroke-width=\"2px\" d=\"M4620,614.5 C4620,527.0 4745.0,527.0 4745.0,614.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bbcd3a17abb04cdcafc42b9b08460b4c-0-26\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M4745.0,616.5 L4753.0,604.5 4737.0,604.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eN2GoLJYvrT5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "df050431-b688-49f5-c4e4-ad47c37a5cec"
      },
      "source": [
        "# Apply displacy to another sentence from the same Wikipedia article.\n",
        "\n",
        "# 3.7: Add a dependency structure graph for the second sentence in kites.\n",
        "displacy.render(kitespans[1], style=\"dep\", jupyter=True)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"5ca8fc78878342af867db405b68b73ff-0\" class=\"displacy\" width=\"1450\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">A</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">kite</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">consists</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">of</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">wings,</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">tethers</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">and</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">CCONJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">anchors.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-5ca8fc78878342af867db405b68b73ff-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-5ca8fc78878342af867db405b68b73ff-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-5ca8fc78878342af867db405b68b73ff-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-5ca8fc78878342af867db405b68b73ff-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-5ca8fc78878342af867db405b68b73ff-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-5ca8fc78878342af867db405b68b73ff-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M570.0,179.0 L578.0,167.0 562.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-5ca8fc78878342af867db405b68b73ff-0-3\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-5ca8fc78878342af867db405b68b73ff-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M745.0,179.0 L753.0,167.0 737.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-5ca8fc78878342af867db405b68b73ff-0-4\" stroke-width=\"2px\" d=\"M770,177.0 C770,89.5 920.0,89.5 920.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-5ca8fc78878342af867db405b68b73ff-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M920.0,179.0 L928.0,167.0 912.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-5ca8fc78878342af867db405b68b73ff-0-5\" stroke-width=\"2px\" d=\"M945,177.0 C945,89.5 1095.0,89.5 1095.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-5ca8fc78878342af867db405b68b73ff-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1095.0,179.0 L1103.0,167.0 1087.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-5ca8fc78878342af867db405b68b73ff-0-6\" stroke-width=\"2px\" d=\"M945,177.0 C945,2.0 1275.0,2.0 1275.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-5ca8fc78878342af867db405b68b73ff-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1275.0,179.0 L1283.0,167.0 1267.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbgHBwm66EM_"
      },
      "source": [
        "Let's close the loop on the idea of data reduction by seeing how many unique lemmas spaCy creates for Crime and Punishment. Recall that we were unsatisfied with the lemmatizer from NLTK because - in order for it to work efficiently - we needed to know the POS for each token before calling the lemmatizer. The spaCy nlp() call ingests our whole text, applies tags, and determines lemmas, all based on a swappable language model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0t6ne0lS6A1N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f70c3919-e255-4542-af3b-c71e2564528c"
      },
      "source": [
        "# Process Crime and Punishment with spaCy: takes a minute!\n",
        "nlp.max_length = 1200000 # Increase from the default of 1 million characters\n",
        "\n",
        "# Note that this call takes about a minute to complete.\n",
        "crimespacy = nlp(raw) # We're going back to the original raw text data!\n",
        "\n",
        "type(crimespacy), len(crimespacy)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(spacy.tokens.doc.Doc, 274697)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-BTBbW-7_nZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "506a7ecc-63e9-4ffa-afef-39715c24ba86"
      },
      "source": [
        "# Let's count unique lemmas\n",
        "newcrimelemma = [l.lemma_ for l in crimespacy]\n",
        "len(set(newcrimelemma))\n",
        "#7844 unique tokens"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7844"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRC0_cIT8mnW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b791f1aa-bfd5-4fde-c20b-a8fc8a99a9f6"
      },
      "source": [
        "# What percentage reduction have we achieved with the lemmatizer? How does that\n",
        "# compare with the stemmers we tested at the beginning of this lab?\n",
        "len(set(newcrimelemma))/len(set(crimetokens))\n",
        "\n",
        "#This has acheived about 73% reduction\n",
        "#Reduction is lower than the three stemmers and higher than the lemmatizer"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7340445442635224"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-5oEhkb8141"
      },
      "source": [
        "**Part Three**\n",
        "\n",
        "So using the spaCy text preprocessing we have generated a complete sequence of approximately 275,000 tokens, considerably more than the NLTK word tokenizer. Can you guess why there are so many more? Even starting from this larger base, however, the spaCy lemmatizer - with the advantage of knowing the POS for each token - has cut things down to about 8000 unique tokens. This is not as aggresive as the Lancaster stemmer, but on the other hand, all of the lemmatized words in the spaCy list are real words (note that the spaCy list will also include tokens that were not lemmatized, such as proper names).\n",
        "\n",
        "What can we do with this large collection of lemmatized tokens? One essential way of representing a corpus is to transform the token counts into a \"document term matrix\" (DTM) or a transposed version of the same thing, a \"term document matrix.\" The most basic DTM contains word frequencies in each cell. A more advanced DTM contains adjusted values known as TF-IDF (term frequency, inverse document frequency). Creating a DTM, either with counts or with TF-IDF values begins with a process called vectorization. Let's vectorize Crime and Punishment, treating each sentence as a document. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-8PqwJhAOrH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94dcb58c-0b1b-41f2-e796-4e98bf6c3c53"
      },
      "source": [
        "# It might be more convenient to work with individual sentences:\n",
        "crimespans = list(crimespacy.sents)\n",
        "\n",
        "crimespans[42:45] # Let's view three sample sentences"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[A few months later Dostoevsky died., He was followed to the grave by a\n",
              " vast multitude of mourners, who “gave the hapless man the funeral of a\n",
              " king.”, He is still probably the most widely read writer in Russia.\n",
              " ]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAHlVE8AA_eF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c7181b9-3dca-468f-af9f-919553af86de"
      },
      "source": [
        "type(crimespans[42]) # Check on the type of a single sentence\n",
        "\n",
        "#Type is spacy tokens span"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.span.Span"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9ilXNA1BYUJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "473f3eef-b201-4a98-b566-84fa0c1b3cfd"
      },
      "source": [
        "# Create a vectorizer using the powerful Sci-Kit Learn\" library.\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Instantiate a vectorizer, removing stopwords, setting min doc frequency\n",
        "vectorizer = CountVectorizer(min_df=1, stop_words='english', lowercase=True) \n",
        "\n",
        "crimesparse = vectorizer.fit_transform([ t.text for t in crimespans])\n",
        "type(crimesparse)\n",
        "\n",
        "#Converted to scipy sparse matrix to represent document term frequency -DTM\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "scipy.sparse.csr.csr_matrix"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwO1fkeWGCpv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "259f1241-f99d-472c-c30d-902a61da8e44"
      },
      "source": [
        "# A sparse matrix DTM is excellent for efficient storage, but to do useful \n",
        "# manipulations, we will need to blow it up into a data frame.\n",
        "import pandas as pd\n",
        "dtmDF = pd.DataFrame(crimesparse.toarray(),\n",
        "                      columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "dtmDF.shape # Make sure you know what these numbers are: Confirm with your partner!\n",
        "\n",
        "#14723  rows or sentences\n",
        "#9557 columns or wods"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14713, 9557)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dtmDF # We can get a preview of the data frame: first five and last five rows\n",
        "# And remember that the magic wand tool lets you take a closer look."
      ],
      "metadata": {
        "id": "Lv7BDlLX6DJh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 673
        },
        "outputId": "1ba2421c-93a3-4719-afa8-788a5002b94d"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       000  14  1500  1849  1859  1861  1864  1880  1887  20  ...  zest  zeus  \\\n",
              "0        0   0     0     0     0     0     0     0     0   0  ...     0     0   \n",
              "1        0   0     0     0     0     0     0     0     0   0  ...     0     0   \n",
              "2        0   0     0     0     0     0     0     0     0   0  ...     0     0   \n",
              "3        0   0     0     0     0     0     0     0     0   0  ...     0     0   \n",
              "4        0   0     0     0     0     0     0     0     0   0  ...     0     0   \n",
              "...    ...  ..   ...   ...   ...   ...   ...   ...   ...  ..  ...   ...   ...   \n",
              "14708    0   0     0     0     0     0     0     0     0   0  ...     0     0   \n",
              "14709    0   0     0     0     0     0     0     0     0   0  ...     0     0   \n",
              "14710    0   0     0     0     0     0     0     0     0   0  ...     0     0   \n",
              "14711    0   0     0     0     0     0     0     0     0   0  ...     0     0   \n",
              "14712    0   0     0     0     0     0     0     0     0   0  ...     0     0   \n",
              "\n",
              "       zigzags  zimmerman  zossimov  æsthetic  æsthetically  æsthetics  \\\n",
              "0            0          0         0         0             0          0   \n",
              "1            0          0         0         0             0          0   \n",
              "2            0          0         0         0             0          0   \n",
              "3            0          0         0         0             0          0   \n",
              "4            0          0         0         0             0          0   \n",
              "...        ...        ...       ...       ...           ...        ...   \n",
              "14708        0          0         0         0             0          0   \n",
              "14709        0          0         0         0             0          0   \n",
              "14710        0          0         0         0             0          0   \n",
              "14711        0          0         0         0             0          0   \n",
              "14712        0          0         0         0             0          0   \n",
              "\n",
              "       éternelle_  êtes  \n",
              "0               0     0  \n",
              "1               0     0  \n",
              "2               0     0  \n",
              "3               0     0  \n",
              "4               0     0  \n",
              "...           ...   ...  \n",
              "14708           0     0  \n",
              "14709           0     0  \n",
              "14710           0     0  \n",
              "14711           0     0  \n",
              "14712           0     0  \n",
              "\n",
              "[14713 rows x 9557 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cb0dc4f9-67ae-4710-8549-cb59b94bc9f5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>000</th>\n",
              "      <th>14</th>\n",
              "      <th>1500</th>\n",
              "      <th>1849</th>\n",
              "      <th>1859</th>\n",
              "      <th>1861</th>\n",
              "      <th>1864</th>\n",
              "      <th>1880</th>\n",
              "      <th>1887</th>\n",
              "      <th>20</th>\n",
              "      <th>...</th>\n",
              "      <th>zest</th>\n",
              "      <th>zeus</th>\n",
              "      <th>zigzags</th>\n",
              "      <th>zimmerman</th>\n",
              "      <th>zossimov</th>\n",
              "      <th>æsthetic</th>\n",
              "      <th>æsthetically</th>\n",
              "      <th>æsthetics</th>\n",
              "      <th>éternelle_</th>\n",
              "      <th>êtes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14708</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14709</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14710</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14711</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14712</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14713 rows × 9557 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cb0dc4f9-67ae-4710-8549-cb59b94bc9f5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cb0dc4f9-67ae-4710-8549-cb59b94bc9f5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cb0dc4f9-67ae-4710-8549-cb59b94bc9f5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 63
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Total number of columns (9557) exceeds max_columns (20) limiting to first (20) columns.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5doBSS5MKoW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3c2f5f1-f5a3-477f-d889-81c4a436aaae"
      },
      "source": [
        "# The output above shows us that we have about 2800 terms (columns) in our\n",
        "# data frame and nearly 15000 sentences (rows). We can look up any word in\n",
        "# the DTM by name and find out how frequently it occurs.\n",
        "dtmDF['priest'].sum() # We're computing the column sum of word counts\n",
        "\n",
        "#The word priest occurs 19 times"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh910qxKMmNE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6db8477e-7afc-4850-fc44-034979469e22"
      },
      "source": [
        "# Choose another word that you think should be in the DTM. What \n",
        "# happens if you try a stop word?\n",
        "\n",
        "# 3.8: Get a total frequency count for a different word.\n",
        "print(\"happy : \",dtmDF['happy'].sum())\n",
        "\n",
        "#Stopwords dont exist in the dataframe"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "happy :  30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wofzzc3ENDKb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "402249ba-e1b4-45b9-8ca0-a961eed4fc48"
      },
      "source": [
        "# Let's make a complete frequency list of all words (columns)\n",
        "wordfreqs = [ (word, dtmDF[word].sum()) for word in vectorizer.get_feature_names_out()] \n",
        "\n",
        "# Now we can sort, using the count as a key. This code uses a lambda function,\n",
        "# an anonymous temporary function, to choose the second element of each tuple \n",
        "# as the basis for the sort. You can read this lambda function as saying, \n",
        "# \"I receive as input one of the tuples and I call it w. Given w, I will \n",
        "# return the second element (the count) to be used as the sorting key.\"\n",
        "wordfreqs.sort(key=lambda w: w[1], reverse=True)\n",
        "\n",
        "# Show the top 20 items\n",
        "wordfreqs[0:20]"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('raskolnikov', 785),\n",
              " ('know', 530),\n",
              " ('said', 519),\n",
              " ('did', 497),\n",
              " ('come', 480),\n",
              " ('man', 479),\n",
              " ('don', 464),\n",
              " ('like', 453),\n",
              " ('sonia', 402),\n",
              " ('time', 385),\n",
              " ('went', 356),\n",
              " ('razumihin', 347),\n",
              " ('dounia', 325),\n",
              " ('thought', 306),\n",
              " ('ivanovna', 304),\n",
              " ('say', 296),\n",
              " ('looked', 293),\n",
              " ('suddenly', 293),\n",
              " ('little', 288),\n",
              " ('petrovitch', 287)]"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNuwmdAzRTth",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "15169a7b-a921-4267-a614-3a05f541005b"
      },
      "source": [
        "# Rodio Raskolnikov and Dmitri Prokofych Razumikhin are focal characters in the book,\n",
        "# so it is pretty cool that their names are among the most frequently\n",
        "# appearing terms in our DTM.\n",
        "\n",
        "# Next, make a list of *row sums* from our dtm using dtmDF.sum(axis=1).  \n",
        "# Examine this list to see if there are any documents that have a row \n",
        "# sum of zero. What would this imply, if you found it?\n",
        "\n",
        "# 3.9: Count the number of rows where row sum equals zero\n",
        "dtmDF[dtmDF.sum(axis=1)==0]\n",
        "# Write a comment indicating what (if anything) you should do with \n",
        "# rows whose sum is zero?\n",
        "\n",
        "#If a rows sum is zero, it means the sentence is empty. We can remove those sentences.\n",
        "#They majorly contain only stop words or words that were stemmed and lemmatized.\n",
        "#The words of those sentences contain what is considered stop words by SpaCy "
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       000  14  1500  1849  1859  1861  1864  1880  1887  20  ...  zest  zeus  \\\n",
              "72       0   0     0     0     0     0     0     0     0   0  ...     0     0   \n",
              "176      0   0     0     0     0     0     0     0     0   0  ...     0     0   \n",
              "213      0   0     0     0     0     0     0     0     0   0  ...     0     0   \n",
              "269      0   0     0     0     0     0     0     0     0   0  ...     0     0   \n",
              "298      0   0     0     0     0     0     0     0     0   0  ...     0     0   \n",
              "...    ...  ..   ...   ...   ...   ...   ...   ...   ...  ..  ...   ...   ...   \n",
              "14664    0   0     0     0     0     0     0     0     0   0  ...     0     0   \n",
              "14667    0   0     0     0     0     0     0     0     0   0  ...     0     0   \n",
              "14672    0   0     0     0     0     0     0     0     0   0  ...     0     0   \n",
              "14679    0   0     0     0     0     0     0     0     0   0  ...     0     0   \n",
              "14681    0   0     0     0     0     0     0     0     0   0  ...     0     0   \n",
              "\n",
              "       zigzags  zimmerman  zossimov  æsthetic  æsthetically  æsthetics  \\\n",
              "72           0          0         0         0             0          0   \n",
              "176          0          0         0         0             0          0   \n",
              "213          0          0         0         0             0          0   \n",
              "269          0          0         0         0             0          0   \n",
              "298          0          0         0         0             0          0   \n",
              "...        ...        ...       ...       ...           ...        ...   \n",
              "14664        0          0         0         0             0          0   \n",
              "14667        0          0         0         0             0          0   \n",
              "14672        0          0         0         0             0          0   \n",
              "14679        0          0         0         0             0          0   \n",
              "14681        0          0         0         0             0          0   \n",
              "\n",
              "       éternelle_  êtes  \n",
              "72              0     0  \n",
              "176             0     0  \n",
              "213             0     0  \n",
              "269             0     0  \n",
              "298             0     0  \n",
              "...           ...   ...  \n",
              "14664           0     0  \n",
              "14667           0     0  \n",
              "14672           0     0  \n",
              "14679           0     0  \n",
              "14681           0     0  \n",
              "\n",
              "[531 rows x 9557 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bd119417-810f-4fd1-b457-f5fd682fc57a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>000</th>\n",
              "      <th>14</th>\n",
              "      <th>1500</th>\n",
              "      <th>1849</th>\n",
              "      <th>1859</th>\n",
              "      <th>1861</th>\n",
              "      <th>1864</th>\n",
              "      <th>1880</th>\n",
              "      <th>1887</th>\n",
              "      <th>20</th>\n",
              "      <th>...</th>\n",
              "      <th>zest</th>\n",
              "      <th>zeus</th>\n",
              "      <th>zigzags</th>\n",
              "      <th>zimmerman</th>\n",
              "      <th>zossimov</th>\n",
              "      <th>æsthetic</th>\n",
              "      <th>æsthetically</th>\n",
              "      <th>æsthetics</th>\n",
              "      <th>éternelle_</th>\n",
              "      <th>êtes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>213</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14664</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14667</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14672</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14679</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14681</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>531 rows × 9557 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bd119417-810f-4fd1-b457-f5fd682fc57a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bd119417-810f-4fd1-b457-f5fd682fc57a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bd119417-810f-4fd1-b457-f5fd682fc57a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# As a hint about the previous exercise, let's find out all of the words\n",
        "# that are included in the default list of stop words used by CountVectorizer\n",
        "from sklearn.feature_extraction import text \n",
        "\n",
        "stop_words = text.ENGLISH_STOP_WORDS\n",
        "len(stop_words)\n",
        "\n",
        "#318 number of stop words"
      ],
      "metadata": {
        "id": "xSI1Fxe8tm-O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34975508-6140-4b4b-c8e8-79d7be57d1b8"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "318"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# \n",
        "# 3.9a: Print out the contents of stop_words. Review it carefully. Are \n",
        "# there any surprises?\n",
        "#\n",
        "stop_words\n",
        "#There are words like \"together\",\"twelve\",\"two\",\"beforehand\" - which are considered \n",
        "#as stopwords, which could mean the sentence has become empty."
      ],
      "metadata": {
        "id": "bSHKdjhot-k3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1q4Vc_uUYfR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffe67268-76bc-4d52-c7e2-8a842eeb1cae"
      },
      "source": [
        "# Let's conclude with a primitive analysis of the dtm. First we'll make two \n",
        "# subsets of our data, based on mentions of characters:\n",
        "\n",
        "raskolDF = dtmDF[dtmDF.raskolnikov > 0]\n",
        "razumihinDF = dtmDF[dtmDF.razumihin > 0]\n",
        "raskolDF.shape, razumihinDF.shape\n",
        "\n",
        "#raskolnikov is present in 777 sentences\n",
        "#razumihin is present in 343 sentences"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((777, 9557), (343, 9557))"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOisk1ZUVU9H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea23a594-42ab-46ef-c758-e00090f61f24"
      },
      "source": [
        "# This creates a ratio of the number of times the word good is mentioned \n",
        "# in each of the two data subsets. What's another word we could probe\n",
        "# to get a sense of how these two characters are discussed.\n",
        "raskolDF['good'].sum()/razumihinDF['good'].sum()\n",
        "\n",
        "#raskolDF contains 2 times more frequency of \"good\" than razumihin"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.0"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# 3.10: Now obtain a ratio of total word frequency for a word other than good\n",
        "#\n",
        "raskolDF['mother'].sum()/razumihinDF['mother'].sum()\n",
        "\n",
        "#raskolDF contains 3.3 times more frequency of \"mother\" than razumihin"
      ],
      "metadata": {
        "id": "_EWnAHHTglVt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "169f54d5-f8d5-4a92-e9c4-7a849079bdc5"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.3333333333333335"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBW6vb3CXb8S"
      },
      "source": [
        "There are many more things we can do with our vectorized sentences, and we will learn some more of them in future weeks. Note that unigram vectorization based on word frequencies is really the most primitive numeric representation of a document. Where we are headed is to use contemporary vector representations (first, word by word, and later with full sentences) of document contents.\n",
        "\n",
        "You are probably near the end of the lab period, so don't forget to submit your lab file AND make a note of how far you got.\n",
        "\n",
        "If there is some remaining time in the lab session, go back to the cell where CountVectorizer is invoked and use that code as the starting point for a new set of code cells that use *TfidfVectorizer* from the scikit.learn package. You can find the documentation for the TfidfVectorizer here: \n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
        "\n",
        "After vectorizing with TF-IDF, convert your sparse matrix to a data frame and do some diagnostics on it as we did above. Finally, break your data frame into subsets, with one for Raskolnikov and one for Razhumihin. Then repeat your ratio tests on good and other words to see if anything has changed as compared with using raw frequency counts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwa0NAn0ZDLH"
      },
      "source": [
        "#\n",
        "# 3.11: Revectorize Crime and Punishment sentences with TF-IDF\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# 3.12: Convert vectorization results (the TF-IDF DTM) to pandas data frame\n",
        "#"
      ],
      "metadata": {
        "id": "Sq7aU1_Vg6R_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# 3.13: Repeat one or more of diagnostic tests demonstrated for the \n",
        "# count vectorization.\n",
        "#"
      ],
      "metadata": {
        "id": "2pDtQ3REg6Du"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# 3.14: Repeat ratio tests, comparing the contents of the DTMs for the two characters.\n",
        "#"
      ],
      "metadata": {
        "id": "jLuQET7hg5g_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bonus Content!##\n",
        "\n",
        "One last advanced topic for this lab is to consider the concept of \"pointwise mutual information\" (PMI) mentioned in today's presentation. When we are dealing with data such as lists of bigrams or trigrams, we often would like to have a method of sifting the data to find the most interesting examples. PMI calculates the probability of the co-occurence of two words using the probability of each word independently as a baseline. Here's an example: Let's say that \"fish\" occurs five times in 100 words, while \"cake\" appears eight times. The combination \"fish cake\" appears 3 times. Now run the code below:"
      ],
      "metadata": {
        "id": "Zh5P4HwR5Aml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pfish = 5/100\n",
        "pcake = 8/100\n",
        "pfishcake = 3/100\n",
        "\n",
        "import math # We will need the log2() function\n",
        "pmi = math.log2( pfishcake / (pfish * pcake))\n",
        "print(pmi)"
      ],
      "metadata": {
        "id": "h4IpmsGU5ECY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So based on this result, Fish and Cake are occuring together somewhat more frequently than would be expected based on how often they appear independently. You can fiddle around with the probability values to see how it affects the PMI calculation.\n",
        "\n",
        "To to this kind of analysis at scale, we'll pull in some code from the nltk.collocations module."
      ],
      "metadata": {
        "id": "5ci9nHHtiQzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.collocations import BigramAssocMeasures # We need two modules\n",
        "from nltk.collocations import BigramCollocationFinder\n",
        "\n",
        "# Here we are creating instances of two classes. The first is a\n",
        "# bigram measurer of the class nltk.metrics.association.BigramAssocMeasures.\n",
        "# The second is a locator function that is initialized with the tokens from Crime and Punishment.\n",
        "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "finder = BigramCollocationFinder.from_words(crimetokens)\n",
        "type(bigram_measures), type(finder)"
      ],
      "metadata": {
        "id": "LTw6dkhA5Gd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The NLTK Pointwise Mutual Information scoring function, PMI, scores\n",
        "# the bigrams by taking into account the frequency of the two\n",
        "# component words. When infrequent words make a bigram they get\n",
        "# a boost in PMI. A higher score thus means a more interesting\n",
        "# bigram.\n",
        "finder.apply_freq_filter(2) # Let's ignore hapaxes\n",
        "scored = finder.score_ngrams(bigram_measures.pmi)\n",
        "\n",
        "# Examine the pairs with PMI greater than 16.5 (an arbitrary number chosen\n",
        "# simply to keep the list short).\n",
        "[bg for bg in scored if bg[1] > 16.5]"
      ],
      "metadata": {
        "id": "GxTr-WcA5mKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \n",
        "# 3.15: Lower the PMI threshold to 14 or 15 and examine some of the \n",
        "# additional bigrams. What do you see? Are high PMI bigrams useful\n",
        "# at telling us something about the corpus?"
      ],
      "metadata": {
        "id": "hhdAo8Yn7GBt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}