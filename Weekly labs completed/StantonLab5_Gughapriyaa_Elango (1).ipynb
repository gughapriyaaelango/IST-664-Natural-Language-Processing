{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P4qX2BhcVYy"
      },
      "source": [
        "# IST664 - Week 5 Lab: Introduction to spaCy\n",
        "\n",
        "SpaCy is an open source natural language processing library written by  Matthew Honnibal and Ines Montani. Most of spaCy is written natively in Python. Unlike NLTK, which was designed for teaching and research, spaCy was created from the start to support production applications - real world activities that require natural language processing. SpaCy uses a \"pipeline\" metaphor such that input documents and data go through a variety of typical processing stages where each stage feeds into the next one. Examples of these stages include tokenization, part of speech tagging, named entity recognition, and transformation into word vectors. \n",
        "\n",
        "Try searching for \"spaCy\" on Kaggle.com. At this writing there were more than 4600 projects that used spaCy. Part of the appeal is that spaCy makes it easy to get started with a project. SpaCy contains support for dozens of different languages and its integration with word- and sentence-embedding approaches provides access to the advantages of pre-trained deep learning models. \n",
        "\n",
        "Although you have seen spaCy briefly before in previous labs, in this lab you will get a more comprehensive view of the architecture and capabilities of spaCy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFA8yhi5cVY0"
      },
      "source": [
        "Sections of this lab:\n",
        "- Basics: Getting Started\n",
        "- Lemmatization\n",
        "- Token Extracting / Removing / Transforming\n",
        "- Sentence Segmentation\n",
        "- Part of Speech Tagging\n",
        "- Named Entity Recognition\n",
        "- Dependency Parsing\n",
        "- Word Vectors\n",
        "- Sentence Similarity\n",
        "- Customizing pipeline components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI5UX3AicVY5"
      },
      "source": [
        "# Basics: Getting Started"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOF7T14dcVY5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2d5e83d-dd02-4565-db6e-f36d5060b576"
      },
      "source": [
        "# Every spaCy project begins with importing the package and \n",
        "# instantiating a processing object that is initialized with a particular\n",
        "# language model. In this case we will start with a small English pipeline\n",
        "# trained from text harvested from the web. \n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# That's equivalent to:\n",
        "# import en_core_web_sm\n",
        "# nlp = en_core_web_sm.load()\n",
        "\n",
        "type(nlp)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.lang.en.English"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5kmYa0aiLxY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cb1aafa-9d1f-4cf4-b28a-8356c335e8f6"
      },
      "source": [
        "# There are lots of things this object can do. Let's use\n",
        "# dir to get a list of them:\n",
        "\n",
        "[m for m in dir(nlp) if m[0] != \"_\"]"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Defaults',\n",
              " 'add_pipe',\n",
              " 'analyze_pipes',\n",
              " 'batch_size',\n",
              " 'begin_training',\n",
              " 'component',\n",
              " 'component_names',\n",
              " 'components',\n",
              " 'config',\n",
              " 'create_optimizer',\n",
              " 'create_pipe',\n",
              " 'create_pipe_from_source',\n",
              " 'default_config',\n",
              " 'default_error_handler',\n",
              " 'disable_pipe',\n",
              " 'disable_pipes',\n",
              " 'disabled',\n",
              " 'enable_pipe',\n",
              " 'evaluate',\n",
              " 'factories',\n",
              " 'factory',\n",
              " 'factory_names',\n",
              " 'from_bytes',\n",
              " 'from_config',\n",
              " 'from_disk',\n",
              " 'get_factory_meta',\n",
              " 'get_factory_name',\n",
              " 'get_pipe',\n",
              " 'get_pipe_config',\n",
              " 'get_pipe_meta',\n",
              " 'has_factory',\n",
              " 'has_pipe',\n",
              " 'initialize',\n",
              " 'lang',\n",
              " 'make_doc',\n",
              " 'max_length',\n",
              " 'meta',\n",
              " 'path',\n",
              " 'pipe',\n",
              " 'pipe_factories',\n",
              " 'pipe_labels',\n",
              " 'pipe_names',\n",
              " 'pipeline',\n",
              " 'rehearse',\n",
              " 'remove_pipe',\n",
              " 'rename_pipe',\n",
              " 'replace_listeners',\n",
              " 'replace_pipe',\n",
              " 'resume_training',\n",
              " 'select_pipes',\n",
              " 'set_error_handler',\n",
              " 'set_factory_meta',\n",
              " 'to_bytes',\n",
              " 'to_disk',\n",
              " 'tokenizer',\n",
              " 'update',\n",
              " 'use_params',\n",
              " 'vocab']"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that all of the methods shown above pertain to pipelines - a modular sequence of processing steps. In general these follow a standard order:\n",
        "\n",
        "* Tokenizer\n",
        "* Part of speech tagger\n",
        "* Dependency parser (organizes each sentence into its constituent parts\n",
        "* Named entity recognizer\n",
        "* Lemmatizer\n",
        "* Additional elements (including document classification)\n",
        "\n",
        "You know enough of the essential foundations of NLP to know why these pipeline elements appear in this order. For example, you could not apply part of speech tags to words without first tokenizing the raw text.  \n"
      ],
      "metadata": {
        "id": "3TVcwVkpbOuX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJbnfdzZcVY6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fb45b44-0b54-429d-d1d8-e1f77d747178"
      },
      "source": [
        "# At the most basic level, and at the beginning of most\n",
        "# NLP pipelines, we tokenize a document:\n",
        "doc = nlp(\"Hello World!\") # This is the most basic way to use the instance\n",
        "type(doc), len(doc) # What is the result?"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(spacy.tokens.doc.Doc, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTLkJurQi9U1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e92efb9e-bc99-4486-e2a8-f8eea25ed69a"
      },
      "source": [
        "# A spaCy \"tokens-doc\" behaves like a list, such that \n",
        "# we can use a list comprehension to access the individual\n",
        "# tokens in the document:\n",
        "[token.text for token in doc]"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', 'World', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uayw2T4rcVY7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f62e3edd-1cc6-4880-eb87-d35c1e6eebd9"
      },
      "source": [
        "# And because it behaves like a list, we can also use\n",
        "# slicing to get access to the individual tokens.\n",
        "first_token = doc[0] # Slice the first token\n",
        "print(type(first_token)) # What is its type?\n",
        "print(first_token.text) # Show the text of the token"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'spacy.tokens.token.Token'>\n",
            "Hello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbALUS2kcVY7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9453ead6-959d-4693-ddb1-df7d2f35aab0"
      },
      "source": [
        "# In spaCy terminology, a span is any contiguous set of tokens.\n",
        "# Spans are often used to break up a document into sentences. Here\n",
        "# we are just using slicing to create a span with the first two \n",
        "# of our three tokens.\n",
        "span = doc[0:2]\n",
        "[token.text for token in span]"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', 'World']"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8cGZAmUj_vx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0882133f-0a53-44a0-9c3b-4caa2363e023"
      },
      "source": [
        "# For this first exercise, tokenize a longer text excerpted from Wikipedia. \n",
        "# Use slicing to show the first five tokens:\n",
        "\n",
        "longtext = \"\"\"A neural network is either a biological neural network or an \n",
        "artificial neural network for solving artificial intelligence (AI) problems. \n",
        "The connections of the biological neuron are modeled as weights. A positive \n",
        "weight reflects an excitatory connection, while negative values mean \n",
        "inhibitory connections.\"\"\"\n",
        "\n",
        "# 5.1: Tokenize longtext\n",
        "token_longtext = nlp(longtext) # This is the most basic way to use the instance\n",
        "print(type(token_longtext))\n",
        "print(len(token_longtext))\n"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'spacy.tokens.doc.Doc'>\n",
            "53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.2: Display the *texts* of tokens in a span consisting of the first 5 tokens\n",
        "span1 = token_longtext[0:5]\n",
        "[token.text for token in span1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1Vkl_yGy7vD",
        "outputId": "61e72a85-ca56-4d47-9705-10c7a18ce388"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A', 'neural', 'network', 'is', 'either']"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.2a: (Challenge) Use Python slicing notation to show the *last* 5 tokens\n",
        "[token.text for token in token_longtext][-5:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8nHDyAwy8ze",
        "outputId": "ea25490e-760e-44f5-a223-ffe30dfcb313"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mean', '\\n', 'inhibitory', 'connections', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5irACZSv7pA7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b0040a2-3dbf-498c-c0bf-0431f571a441"
      },
      "source": [
        "# SpaCy uses the language model to make better tokenization decisions. Let's \n",
        "# compare spaCy tokenization with the primitive use of split(). Remember that\n",
        "# split() defaults to splitting on spaces.\n",
        "headline = \"Rare Bird’s Detection Highlights Promise of ‘Environmental DNA’\"\n",
        "\n",
        "splitspacy = nlp(headline) # Use spaCy tokenization\n",
        "splitspace = headline.split() # Use simple splitting on spaces\n",
        "\n",
        "print(\"SpaCy tokens:\")\n",
        "print([t.text for t in splitspacy])\n",
        "print(len(splitspacy), \"tokens.\")\n",
        "\n",
        "print(\"\\nSimple splitting:\")\n",
        "print([s for s in splitspace])\n",
        "print(len(splitspace), \"tokens.\")"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SpaCy tokens:\n",
            "['Rare', 'Bird', '’s', 'Detection', 'Highlights', 'Promise', 'of', '‘', 'Environmental', 'DNA', '’']\n",
            "11 tokens.\n",
            "\n",
            "Simple splitting:\n",
            "['Rare', 'Bird’s', 'Detection', 'Highlights', 'Promise', 'of', '‘Environmental', 'DNA’']\n",
            "8 tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kC32D3YIKbZl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f4b5a51-d446-44a4-eedd-579f33e6b522"
      },
      "source": [
        "# Why do you think it might be helpful to tokenize the possessive \"Bird's\" into\n",
        "# two tokens? Add a comment that explains your reasoning. Then find or write\n",
        "# a new sentence that contains a hyphenated noun phrase. How does spaCy treat that?\n",
        "\n",
        "#Dectecting Bird's as \"Bird\" and \"'s\" will be useful for lexical chaining and coreference entities\n",
        "#This will help in making better linkages within and between sentences.\n",
        "\n",
        "# 5.2b: Use spaCy to tokenize a sentence that contains a hyphenated phrase.\n",
        "\n",
        "new_sent = \"The puppy's collar is red.\"\n",
        "splitspacy = nlp(new_sent)\n",
        "print(\"SpaCy tokens:\")\n",
        "print([t.text for t in splitspacy])\n",
        "print(len(splitspacy), \"tokens.\")\n",
        "\n",
        "#Puppy's is split as \"Puppy\" and \"'s\""
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SpaCy tokens:\n",
            "['The', 'puppy', \"'s\", 'collar', 'is', 'red', '.']\n",
            "7 tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9KnXfE9cVY9"
      },
      "source": [
        "# Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdBuIUPRcVY-"
      },
      "source": [
        "Lemmatization is the process of reducing inflected forms, sometimes derivationally related forms of a word to a common base form. This reduced form or root word is called a lemma. Lemmas have an advantage over simple stemming: Lemmas are always dictionary words. Lemmatizing can be a valuable data reduction technique because it aggregates various inflective forms of a word down to a single root."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afi7fWJpcVY-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "740581a7-af9a-4675-9956-ff7fe073bc12"
      },
      "source": [
        "# Demonstrate spaCy lemmatization with a verb form\n",
        "text = \"I am, you are, and he is.\" # All the verbs are variations on the verb \"to be\"\n",
        "\n",
        "# Note that the underscore following the attribute name in \n",
        "# the expression token.lemma_ provides the human readable form of the attribute.\n",
        "[token.lemma_ for token in nlp(text)]"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'be', ',', 'you', 'be', ',', 'and', 'he', 'be', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r1LTkjOCjOj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f99864c-0331-41e4-d199-24ddb1d1f2e7"
      },
      "source": [
        "# Look at the non-text form of the lemma:\n",
        "\n",
        "# 5.3: use token.lemma instead of token.lemma_\n",
        "[token.lemma for token in nlp(text)]\n",
        "\n",
        "# Write a comment describing what you see. These values are\n",
        "# ID numbers for spaCy's \"StringStore.\" More information here:\n",
        "# https://spacy.io/usage/spacy-101#vocab\n",
        "\n",
        "#The lemmatized token's hash values are displayed here\n",
        "#To save space during storing vocabulory in spacy, every string is\n",
        "#encoded in hash values"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4690420944186131903,\n",
              " 10382539506755952630,\n",
              " 2593208677638477497,\n",
              " 7624161793554793053,\n",
              " 10382539506755952630,\n",
              " 2593208677638477497,\n",
              " 2283656566040971221,\n",
              " 1655312771067108281,\n",
              " 10382539506755952630,\n",
              " 12646065887601541794]"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "oRc6MHIhcVY-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "934eddb6-5a66-4787-a1b4-8e901b94df80"
      },
      "source": [
        "# Here's another example\n",
        "text = \"Look! It looks like he looked.\"\n",
        "doc = nlp(text)\n",
        "for token in doc:\n",
        "    print(\"token:{} -> lemma:{}\".format(token.text,token.lemma_ ))"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token:Look -> lemma:look\n",
            "token:! -> lemma:!\n",
            "token:It -> lemma:it\n",
            "token:looks -> lemma:look\n",
            "token:like -> lemma:like\n",
            "token:he -> lemma:he\n",
            "token:looked -> lemma:look\n",
            "token:. -> lemma:.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vbqv-p5BG-mV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2891199c-395a-407f-a8d5-32679296fdfd"
      },
      "source": [
        "# Add your own example, this time using different forms of a noun\n",
        "\n",
        "# 5.4: Lemmatize two or more inflective forms of a noun. What about\n",
        "# irregular inflections (such as the plural of mouse)?\n",
        "\n",
        "text1 = \"The mice were nibbling at the computer mouse\"\n",
        "doc1 = nlp(text1)\n",
        "for token in doc1:\n",
        "    print(\"token:{} -> lemma:{}\".format(token.text,token.lemma_ ))"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token:The -> lemma:the\n",
            "token:mice -> lemma:mouse\n",
            "token:were -> lemma:be\n",
            "token:nibbling -> lemma:nibble\n",
            "token:at -> lemma:at\n",
            "token:the -> lemma:the\n",
            "token:computer -> lemma:computer\n",
            "token:mouse -> lemma:mouse\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text2 = \"The market had everything, ranging from knives to ponies\"\n",
        "doc2 = nlp(text2)\n",
        "for token in doc2:\n",
        "    print(\"token:{} -> lemma:{}\".format(token.text,token.lemma_ ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wy6UMc6V2hTc",
        "outputId": "a8c6066a-0933-4376-a248-10c2bd7cf4a6"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token:The -> lemma:the\n",
            "token:market -> lemma:market\n",
            "token:had -> lemma:have\n",
            "token:everything -> lemma:everything\n",
            "token:, -> lemma:,\n",
            "token:ranging -> lemma:range\n",
            "token:from -> lemma:from\n",
            "token:knives -> lemma:knife\n",
            "token:to -> lemma:to\n",
            "token:ponies -> lemma:pony\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPh2OEW9cVZD"
      },
      "source": [
        "# Token Extracting / Removing / Transforming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0_iGcUicVZE"
      },
      "source": [
        "Here's an overview of all of the bound methods and attributes that a token has. When creating an NLP pipeline, it is helpful not to have to write our own code to find out these things.\n",
        "\n",
        "|Attribute Name\t|Type|Description                                                                    |\n",
        "|:--------------------:|:---------:|:------------------------------------------------------------------------------------------------------------------------------------:|\n",
        "| lemma              | int     | Base form of the token, with no inflectional suffixes.                                                                             |\n",
        "| lemma_             | unicode | Base form of the token, with no inflectional suffixes.                                                                             |\n",
        "| norm               | int     | The token’s norm, i.e. a normalized form of the token text. Usually set in the language’s tokenizer exceptions or norm exceptions. |\n",
        "| norm_              | unicode | The token’s norm, i.e. a normalized form of the token text. Usually set in the language’s tokenizer exceptions or norm exceptions. |\n",
        "| lower              | int     | Lowercase form of the token.                                                                                                       |\n",
        "| lower_             | unicode | Lowercase form of the token text. Equivalent to Token.text.lower().                                                                |\n",
        "| shape              | int     | Transform of the tokens’s string, to show orthographic features. For example, “Xxxx” or “dd”.                                      |\n",
        "| shape_             | unicode | Transform of the tokens’s string, to show orthographic features. For example, “Xxxx” or “dd”.                                      |\n",
        "| prefix             | int     | Hash value of a length-N substring from the start of the token. Defaults to N=1.                                                   |\n",
        "| prefix_            | unicode | A length-N substring from the start of the token. Defaults to N=1.                                                                 |\n",
        "| suffix             | int     | Hash value of a length-N substring from the end of the token. Defaults to N=3.                                                     |\n",
        "| suffix_            | unicode | Length-N substring from the end of the token. Defaults to N=3.                                                                     |\n",
        "| is_alpha           | bool    | Does the token consist of alphabetic characters? Equivalent to token.text.isalpha().                                               |\n",
        "| is_ascii           | bool    | Does the token consist of ASCII characters? Equivalent to all(ord(c) < 128 for c in token.text).                                   |\n",
        "| is_digit           | bool    | Does the token consist of digits? Equivalent to token.text.isdigit().                                                              |\n",
        "| is_lower           | bool    | Is the token in lowercase? Equivalent to token.text.islower().                                                                     |\n",
        "| is_upper           | bool    | Is the token in uppercase? Equivalent to token.text.isupper().                                                                     |\n",
        "| is_title           | bool    | Is the token in titlecase? Equivalent to token.text.istitle().                                                                     |\n",
        "| is_punct           | bool    | Is the token punctuation?                                                                                                          |\n",
        "| is_left_punct      | bool    | Is the token a left punctuation mark, e.g. (?                                                                                      |\n",
        "| is_right_punct     | bool    | Is the token a right punctuation mark, e.g. )?                                                                                     |\n",
        "| is_space           | bool    | Does the token consist of whitespace characters? Equivalent to token.text.isspace().                                               |\n",
        "| is_bracket         | bool    | Is the token a bracket?                                                                                                            |\n",
        "| is_quote           | bool    | Is the token a quotation mark?                                                                                                     |\n",
        "| is_currency V2.0.8 | bool    | Is the token a currency symbol?                                                                                                    |\n",
        "| like_url           | bool    | Does the token resemble a URL?                                                                                                     |\n",
        "| like_num           | bool    | Does the token represent a number? e.g. “10.9”, “10”, “ten”, etc.                                                                  |\n",
        "| like_email         | bool    | Does the token resemble an email address?                                                                                          |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLANATg8cVZE"
      },
      "source": [
        "### Extracting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZySKnJrYcVZE"
      },
      "source": [
        "The list of attributes that spaCy makes available on the token object provide a variety of type tests. The is_ attributes allow testing for alpahnumeric, uppercase, title case, left punctuation mark, right punctuation mark, any punctuation mark, a bracket, a quote mark or a currency symbol. These are all very helpful in navigating within a string of tokens: Later we will show a search capability that allows us to include these in pattern matching.\n",
        "\n",
        "There are also three \"like\" attributes that show if a token looks like a web address, a numeric string, or an email address.\n",
        "\n",
        "Let's run some tests on a long and complex sentence from Wikipedia:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRQSZrZhcVZF"
      },
      "source": [
        "text='''An information retrieval technique using latent semantic structure was \n",
        "patented in 1988 (US Patent 4,839,853, now expired) by Scott Deerwester, \n",
        "Susan Dumais, George Furnas, Richard Harshman, Thomas Landauer, Karen Lochbaum \n",
        "and Lynn Streeter. In the context of its application to information retrieval, \n",
        "it is sometimes called latent semantic indexing (LSI).'''"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5TMwLVFcVZF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cc00e51-dd34-4c93-fb5a-b9e4739f2788"
      },
      "source": [
        "my_list=[] # Initialize a blank list\n",
        "doc = nlp(text) # Tokenize the text string\n",
        "for token in doc: # Check each token\n",
        "    if token.is_punct: # Run the bound method\n",
        "        my_list.append(token) # Append to the list\n",
        "\n",
        "for item in my_list: # Review each item in the list\n",
        "    print(item) # Print the item"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\n",
            ",\n",
            ")\n",
            ",\n",
            ",\n",
            ",\n",
            ",\n",
            ",\n",
            ".\n",
            ",\n",
            "(\n",
            ")\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na0scT-N5mYG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9acef150-6308-4c1c-bf34-b6b147af5932"
      },
      "source": [
        "# Now do something similar but use a list comprehension\n",
        "[tok for tok in doc if tok.is_left_punct]"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(, (]"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lchVjYGl6p69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32d9ba51-41c2-4fe8-a470-f47cb93fe4b7"
      },
      "source": [
        "# Add a line of code to display right punctuation\n",
        "\n",
        "# 5.5: Use the bound method to detect and print right puncutation\n",
        "[tok for tok in doc if tok.is_right_punct]"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[), )]"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNHFsCK064aJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27bfd98e-e760-492c-dabd-c96bb96560dd"
      },
      "source": [
        "# Add a line of code to detect tokens that seem like numbers\n",
        "\n",
        "# 5.6: Use the bound method to detect and display numbers\n",
        "[tok for tok in doc if tok.like_num]"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1988, 4,839,853]"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLuAkiwqsp4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "e98725bf-7d06-445a-aed7-d1738b69b45d"
      },
      "source": [
        "# For diagnostic purposes, it may be useful to examine these attributes\n",
        "# all together. Here's a code fragment that sets up a pandas df of token\n",
        "# attributes:\n",
        "\n",
        "import pandas as pd # Use a pandas DF\n",
        "\n",
        "# These will be out column names\n",
        "cols = (\"text\", \"lemma_\",\"is_punct\", \"is_stop\", \"is_alpha\",\"is_space\",\"lower_\")\n",
        "\n",
        "rows = [] # A blank list to hold the rows\n",
        "\n",
        "for t in doc: # Iterate through the tokens - will work for any length document\n",
        "    # build the next row\n",
        "    row = [t.text, t.lemma_,  t.is_punct,  t.is_stop,  t.is_alpha,  t.is_space,  t.lower_]\n",
        "    rows.append(row) # Append the row to the existing rows\n",
        "\n",
        "# Create the pandas data frame from the column names and the list of rows\n",
        "attri_pdf = pd.DataFrame(rows, columns=cols)\n",
        "\n",
        "attri_pdf # Gives a preview, but may not show all rows"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           text       lemma_  is_punct  is_stop  is_alpha  is_space  \\\n",
              "0            An           an     False     True      True     False   \n",
              "1   information  information     False    False      True     False   \n",
              "2     retrieval    retrieval     False    False      True     False   \n",
              "3     technique    technique     False    False      True     False   \n",
              "4         using          use     False     True      True     False   \n",
              "..          ...          ...       ...      ...       ...       ...   \n",
              "62     indexing     indexing     False    False      True     False   \n",
              "63            (            (      True    False     False     False   \n",
              "64          LSI          LSI     False    False      True     False   \n",
              "65            )            )      True    False     False     False   \n",
              "66            .            .      True    False     False     False   \n",
              "\n",
              "         lower_  \n",
              "0            an  \n",
              "1   information  \n",
              "2     retrieval  \n",
              "3     technique  \n",
              "4         using  \n",
              "..          ...  \n",
              "62     indexing  \n",
              "63            (  \n",
              "64          lsi  \n",
              "65            )  \n",
              "66            .  \n",
              "\n",
              "[67 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c541d327-4e23-4afd-a3a1-e03b92c5eeeb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>lemma_</th>\n",
              "      <th>is_punct</th>\n",
              "      <th>is_stop</th>\n",
              "      <th>is_alpha</th>\n",
              "      <th>is_space</th>\n",
              "      <th>lower_</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>An</td>\n",
              "      <td>an</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>an</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>information</td>\n",
              "      <td>information</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>information</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>retrieval</td>\n",
              "      <td>retrieval</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>retrieval</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>technique</td>\n",
              "      <td>technique</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>technique</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>using</td>\n",
              "      <td>use</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>using</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>indexing</td>\n",
              "      <td>indexing</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>indexing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>(</td>\n",
              "      <td>(</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>(</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>LSI</td>\n",
              "      <td>LSI</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>lsi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>)</td>\n",
              "      <td>)</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>67 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c541d327-4e23-4afd-a3a1-e03b92c5eeeb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c541d327-4e23-4afd-a3a1-e03b92c5eeeb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c541d327-4e23-4afd-a3a1-e03b92c5eeeb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACTokpuocVZG"
      },
      "source": [
        "In previous weeks we have considered stop words and why in some cases it makes sense to remove them from the token stream. Let's examine spaCy's stop word list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmEFYVF_cVZG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9cf685d-c15a-4092-eab6-62da3c5c8a13"
      },
      "source": [
        "import spacy\n",
        "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "len(spacy_stopwords)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "326"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYfeBc3ccVZH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "381434ff-db16-44b9-bf2b-81132715d92f"
      },
      "source": [
        "list(spacy_stopwords)[:8]"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['go', '’re', 'among', 'no', 'very', 'everyone', 'fifteen', 'whereupon']"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5x3AkUho6QE"
      },
      "source": [
        "It is good to know what is on a stop list because sometimes these lists contain words that we do not want to discard. Because spaCy tags each token with an attribute showing whether that token is a stop word, but does not discard the stop words, we have the opportunity to do diagnostics on the results. \n",
        "\n",
        "Let's process another piece of text from Wikipedia to focus on the stop words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOtvuhEvcVZH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "650e2220-293a-4be9-f5f7-34d547595f40"
      },
      "source": [
        "text = \"\"\"In natural language processing, the Latent Dirichlet Allocation (LDA) \n",
        "is a generative statistical model that allows sets of observations to be \n",
        "explained by unobserved groups that explain why some parts of the data are \n",
        "similar. For example, if observations are words collected into documents, \n",
        "it posits that each document is a mixture of a small number of topics and that \n",
        "each word's presence is attributable to one of the document's topics. LDA is \n",
        "an example of a topic model and belongs to the machine learning field and in \n",
        "a wider sense to the artificial intelligence field.\"\"\"\n",
        "\n",
        "doc = nlp(text)\n",
        "type(doc), len(doc)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(spacy.tokens.doc.Doc, 113)"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cb00Z7RFpZG5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c34905a6-456f-49df-b9ac-7d0cdba6cbdb"
      },
      "source": [
        "# Display the tokens that are stop words:\n",
        "print([token for token in doc if token.is_stop])"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[In, the, is, a, that, of, to, be, by, that, why, some, of, the, are, For, if, are, into, it, that, each, is, a, of, a, of, and, that, each, 's, is, to, one, of, the, 's, is, an, of, a, and, to, the, and, in, a, to, the]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note how spaCy has correctly tagged stop words even when they begin with a capital letter."
      ],
      "metadata": {
        "id": "QFF2Kk9ydf4o"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ny2AwIJcVZH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "339d3e7e-9eb4-4262-88fe-73eb196a0cc8"
      },
      "source": [
        "# Make a list of the tokens that are not stop-words\n",
        "no_stops = [token for token in doc if not token.is_stop]\n",
        "type(no_stops), len(no_stops)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(list, 64)"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V09k1Ar0ZZQ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61da8a1b-b065-4738-8b0d-0044d70e6e00"
      },
      "source": [
        "# Use slicing to view the first few non-stop words.\n",
        "no_stops[0:12]"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[natural,\n",
              " language,\n",
              " processing,\n",
              " ,,\n",
              " Latent,\n",
              " Dirichlet,\n",
              " Allocation,\n",
              " (,\n",
              " LDA,\n",
              " ),\n",
              " ,\n",
              " generative]"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCBYMAEgp4Z0"
      },
      "source": [
        "In some applications we may have uses for the punctuation tokens, but it is also good to know how to remove them. Conveniently, spaCy has also tagged every token with an indicator of whether it is punctuation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_F-0mSaalU6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fab3628-7810-4111-afe0-090ab2655aaa"
      },
      "source": [
        "# Also remove punctuation tokens\n",
        "no_stops_or_punct = [token for token in no_stops if not token.is_punct]\n",
        "type(no_stops_or_punct[0]), len(no_stops_or_punct)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(spacy.tokens.token.Token, 56)"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWC-WDivmMIj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08391607-ea1d-4b68-e635-6c71ebd880ee"
      },
      "source": [
        "# Use slicing to view the first few non-stop, non-punct words.\n",
        "no_stops_or_punct[0:10]"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[natural,\n",
              " language,\n",
              " processing,\n",
              " Latent,\n",
              " Dirichlet,\n",
              " Allocation,\n",
              " LDA,\n",
              " ,\n",
              " generative,\n",
              " statistical]"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07CDeLNlmfRW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "ca470c6d-1ae4-42c9-f875-9079841ab01d"
      },
      "source": [
        "# Hmm, no_stops_or_punct[7] seems to be blank. Write a few lines of\n",
        "# code to find out what it is/was. Hint: Try creating a pandas\n",
        "# data frame of token attributes like the one we made in an earlier\n",
        "# section of this lab.\n",
        "\n",
        "# 5.7: Investigate the mysterious token.\n",
        "# These will be out column names\n",
        "cols = (\"text\", \"lemma_\",\"is_punct\", \"is_stop\", \"is_alpha\",\"is_space\",\"lower_\")\n",
        "\n",
        "rows = [] # A blank list to hold the rows\n",
        "\n",
        "for t in no_stops_or_punct: # Iterate through the tokens - will work for any length document\n",
        "    # build the next row\n",
        "    row = [t.text, t.lemma_,  t.is_punct,  t.is_stop,  t.is_alpha,  t.is_space,  t.lower_]\n",
        "    rows.append(row) # Append the row to the existing rows\n",
        "\n",
        "# Create the pandas data frame from the column names and the list of rows\n",
        "attri_pdf = pd.DataFrame(rows, columns=cols)\n",
        "\n",
        "attri_pdf.head(10) # Gives a preview, but may not show all rows\n",
        "\n",
        "#The no_stops_or_punct[7] is \\n new line tag, that's why it shows empty"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          text       lemma_  is_punct  is_stop  is_alpha  is_space  \\\n",
              "0      natural      natural     False    False      True     False   \n",
              "1     language     language     False    False      True     False   \n",
              "2   processing   processing     False    False      True     False   \n",
              "3       Latent       Latent     False    False      True     False   \n",
              "4    Dirichlet    Dirichlet     False    False      True     False   \n",
              "5   Allocation   Allocation     False    False      True     False   \n",
              "6          LDA          LDA     False    False      True     False   \n",
              "7           \\n           \\n     False    False     False      True   \n",
              "8   generative   generative     False    False      True     False   \n",
              "9  statistical  statistical     False    False      True     False   \n",
              "\n",
              "        lower_  \n",
              "0      natural  \n",
              "1     language  \n",
              "2   processing  \n",
              "3       latent  \n",
              "4    dirichlet  \n",
              "5   allocation  \n",
              "6          lda  \n",
              "7           \\n  \n",
              "8   generative  \n",
              "9  statistical  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c1d5198e-4cb4-4fa7-bf2f-bc5c12e56fc4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>lemma_</th>\n",
              "      <th>is_punct</th>\n",
              "      <th>is_stop</th>\n",
              "      <th>is_alpha</th>\n",
              "      <th>is_space</th>\n",
              "      <th>lower_</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>natural</td>\n",
              "      <td>natural</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>natural</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>language</td>\n",
              "      <td>language</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>language</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>processing</td>\n",
              "      <td>processing</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>processing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Latent</td>\n",
              "      <td>Latent</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>latent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Dirichlet</td>\n",
              "      <td>Dirichlet</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>dirichlet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Allocation</td>\n",
              "      <td>Allocation</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>allocation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>LDA</td>\n",
              "      <td>LDA</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>lda</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>\\n</td>\n",
              "      <td>\\n</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>generative</td>\n",
              "      <td>generative</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>generative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>statistical</td>\n",
              "      <td>statistical</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>statistical</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c1d5198e-4cb4-4fa7-bf2f-bc5c12e56fc4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c1d5198e-4cb4-4fa7-bf2f-bc5c12e56fc4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c1d5198e-4cb4-4fa7-bf2f-bc5c12e56fc4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Discuss and Collaborate\n",
        "\n",
        "Task 5.7, just above, may require some ingenuity. Check in with someone else in the lab to get ideas on how to tackle this. When you have uncovered the result, discuss the implications with your partner."
      ],
      "metadata": {
        "id": "kSNJ-YEavskC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CkpzGjVcVZI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a1be49c-a137-454a-f50f-d97bad27abb9"
      },
      "source": [
        "# Another attribute on a token contains the lowercase version\n",
        "# of the token. Why does this attribute end with an underscore?\n",
        "lowercased = [ token.lower_ for token in no_stops_or_punct]\n",
        "lowercased[0:9]"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'latent',\n",
              " 'dirichlet',\n",
              " 'allocation',\n",
              " 'lda',\n",
              " '\\n',\n",
              " 'generative']"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVdSnFcvqiZy"
      },
      "source": [
        "When we called nlp() on the text object and created the tokens, spaCy also automatically guessed at the lemma for each token and stuck that in as an attribute. Knowing what you know about lemmatization, what does this imply about other processing that spaCy may have done to this text?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yH48zQg4bYNs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ed99d80-f1fb-41f4-ef02-3b4461cfd96a"
      },
      "source": [
        "# Make an additional list of lemma tokens\n",
        "lemma_list = [token.lemma_ for token in no_stops_or_punct] \n",
        "type(lemma_list[0]), len(set(lemma_list))"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(str, 39)"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHc9jkwLdTrz"
      },
      "source": [
        "# The output above suggests that some lemmas appear in the token\n",
        "# list more than one time. Use Counter from the collections package\n",
        "# to count instances of lemmas.\n",
        "from collections import Counter\n",
        "\n",
        "# 5.8: Instantiate a counter object with Counter(lemma_list). Assign this\n",
        "# to a new variable such as wc_lemmas\n",
        "wc_lemmas = Counter(lemma_list)\n"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.9: Display the frequency counts of the five most common lemmas. Hint:\n",
        "# a Counter has a bound method called most_common() that takes one \n",
        "# argument called \"n\"\n",
        "wc_lemmas.most_common(5)"
      ],
      "metadata": {
        "id": "282yXm5cdstt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce24b23a-2b25-4f56-9c3e-0a4949d350d1"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('\\n', 7), ('document', 3), ('topic', 3), ('LDA', 2), ('model', 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.10: Grab a new long string from Wikipedia or another source\n",
        "#      and remove stop words and punctuation, then lemmatize and\n",
        "#      count the frequencies of the top five lemmas. Write new code\n",
        "#      below, based on what you did above.\n",
        "\n",
        "\n",
        "text3 = \"\"\"Severe Tropical Cyclone Gabrielle was a tropical cyclone that affected New Zealand and Norfolk Island in February 2023.The fifth named storm of the 2022–23 Australian region cyclone season, and first severe tropical cyclone of the 2022–23 South Pacific cyclone season, Gabrielle was first noted as a developing tropical low on 6 February, while it was located to the south of the Solomon Islands, before it was classified as a tropical cyclone and named Gabrielle by the Australian Bureau of Meteorology.The system reached a Category 3 severe tropical cyclone, before moving into the South Pacific basin,then rapidly degenerated into a subtropical low during 11 February.Norfolk Island was placed under a red alert as Gabrielle approached, while heavy rain and wind warnings were issued across the North Island of New Zealand. States of emergency which were already in place in Auckland and the Coromandel as a result of the 2023 North Island floods were extended, and new states of emergency were declared in other areas. New Zealand began feeling the effects of the cyclone on 12 February with the impact still ongoing, with a national state of emergency being declared in the country on 14 February.\"\"\"\n",
        "#defining new sentence\n",
        "doc3 = nlp(text3)\n",
        "#removing stop words\n",
        "no_stops = [token for token in doc3 if not token.is_stop]\n",
        "#removing punctuations\n",
        "no_stops_or_punct1 = [token for token in no_stops if not token.is_punct]\n",
        "\n",
        "#defining dataframe\n",
        "cols = (\"text\", \"lemma_\",\"is_punct\", \"is_stop\", \"is_alpha\",\"is_space\",\"lower_\")\n",
        "\n",
        "rows = [] # A blank list to hold the rows\n",
        "\n",
        "for t in no_stops_or_punct1: # Iterate through the tokens - will work for any length document\n",
        "    # build the next row\n",
        "    row = [t.text, t.lemma_,  t.is_punct,  t.is_stop,  t.is_alpha,  t.is_space,  t.lower_]\n",
        "    rows.append(row) # Append the row to the existing rows\n",
        "\n",
        "# Create the pandas data frame from the column names and the list of rows\n",
        "cyclone_df = pd.DataFrame(rows, columns=cols)\n",
        "\n",
        "cyclone_df.head(10) # Gives a preview, but may not show all rows\n",
        "\n",
        "#lemmatising\n",
        "lemma_list1 = [token.lemma_ for token in no_stops_or_punct1] \n",
        "#counting lemmatising\n",
        "wc_lemmas1 = Counter(lemma_list1)\n",
        "#most common lemmas\n",
        "wc_lemmas1.most_common(5)"
      ],
      "metadata": {
        "id": "wTGnIRc4dudw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dd4cd3d-842d-425d-cbb6-05df97becbca"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('cyclone', 7),\n",
              " ('tropical', 5),\n",
              " ('February', 5),\n",
              " ('Gabrielle', 4),\n",
              " ('Island', 4)]"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAnZFrsRcVZJ"
      },
      "source": [
        "# Sentence Segmentation\n",
        "\n",
        "The spaCy doc object contains an element called \"sents\" that records the beginning and ending position (counting by tokens) of each sentence in the document. Finding sentence boundaries requires a substantial amount of algorithmic complexity, because the ending punctuation in strings such as U.S. or etc. may or may not indicate a sentence boundary. There are four strategies for sentence boundary detection in spaCy: dependency parser (default), statistical segmenter, rule-based segmenter, or custom function. Let's tokenize a fragment of Wikipedia text using the default (dependency parser) and then examine the resulting sentences. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-YbifqjcVZJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f3abfec3-1b29-45fb-aa42-059936430bf3"
      },
      "source": [
        "text = \"\"\"Sentence boundary disambiguation (SBD), also known as sentence breaking, sentence boundary detection, and sentence segmentation, is the problem in natural language processing of deciding where sentences begin and end. Natural language processing tools often require their input to be divided into sentences; however, sentence boundary identification can be challenging due to the potential ambiguity of punctuation marks. In written English, a period may indicate the end of a sentence, or may denote an abbreviation, a decimal point, an ellipsis, or an email address, among other possibilities. About 47% of the periods in the Wall Street Journal corpus denote abbreviations.[1] Question marks and exclamation marks can be similarly ambiguous due to use in emoticons, computer code, and slang.\"\"\"\n",
        "text[-26:] # Show the end of the string\n"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' computer code, and slang.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ng04k0RNcVZJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e19919e4-1c9e-47a0-bf4c-a72a0551802b"
      },
      "source": [
        "doc = nlp(text)\n",
        "type(doc.sents)"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "generator"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZM_0KnK2vYow"
      },
      "source": [
        "In Python, a generator is a special kind of iterator function that creates the requested elements on demand and \"on the fly.\" This is a helpful approach when working with large sets of data elements that it would be challenging to represent in memory all at once. It is as easy to create a generator as it is to create a list comprehension:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC5mlvHjwEHn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "239f399b-e361-4192-d512-c9921e1fb578"
      },
      "source": [
        "(n ** 2 for n in range(50)) # First try it without assigning it to an object"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object <genexpr> at 0x7f9262e68f20>"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4U566yuwZ8p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b8e9fc3-9ee1-43a1-b481-9119e54ee85c"
      },
      "source": [
        "sq_gen = (n ** 2 for n in range(20)) # This time, save the generator and check its type\n",
        "type(sq_gen)"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "generator"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42K93tfEwpUa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49256bde-16ad-48db-a32a-68aad454ee25"
      },
      "source": [
        "# Try running this cell twice. What happens the second time?\n",
        "# After the second run, what happens if you run the previous cell again?\n",
        "for num in sq_gen:\n",
        "    if num < 100:\n",
        "      print(num)\n",
        "\n",
        "#We need to create generator to iterate throgh the generator object"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "4\n",
            "9\n",
            "16\n",
            "25\n",
            "36\n",
            "49\n",
            "64\n",
            "81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GYAGCu3zCPQ"
      },
      "source": [
        "Looking back a couple of code blocks, doc.sents is a generator object, which means we can iterate through it's elements to find what we need. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMmFpAT3cVZJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40f3e16f-dbce-4c85-d085-a6c3eb44e602"
      },
      "source": [
        "for sent in doc.sents:\n",
        "    print(\"start_pos={}, end_pos={}, text:{}\".format(sent.start, sent.end, sent.text))"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start_pos=0, end_pos=36, text:Sentence boundary disambiguation (SBD), also known as sentence breaking, sentence boundary detection, and sentence segmentation, is the problem in natural language processing of deciding where sentences begin and end.\n",
            "start_pos=36, end_pos=67, text:Natural language processing tools often require their input to be divided into sentences; however, sentence boundary identification can be challenging due to the potential ambiguity of punctuation marks.\n",
            "start_pos=67, end_pos=103, text:In written English, a period may indicate the end of a sentence, or may denote an abbreviation, a decimal point, an ellipsis, or an email address, among other possibilities.\n",
            "start_pos=103, end_pos=139, text:About 47% of the periods in the Wall Street Journal corpus denote abbreviations.[1] Question marks and exclamation marks can be similarly ambiguous due to use in emoticons, computer code, and slang.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeO7rK2nzaHm"
      },
      "source": [
        "# Use the doc.sent generator object to iterate through the sentences. Create\n",
        "# a pandas data frame containing the starting and ending token numbers of each\n",
        "# sentence. Then use the data frame to fetch the first token from each sentence.\n",
        "import pandas as pd # Use a pandas DF\n",
        "\n",
        "# These will be our column names\n",
        "cols = (\"start\", \"end\")\n",
        "rows = [] # A blank list to hold the rows\n",
        "\n",
        "# 5.11: Iterate through the sentences, appending a row of start and end\n",
        "#        positions for each sentence.\n",
        "for sent in doc.sents: # Iterate through the tokens - will work for any length document\n",
        "    # build the next row\n",
        "    row = [sent.start, sent.end]\n",
        "    rows.append(row) # Append the row to the existing rows\n"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.12: Create a pandas data frame from the rows and column names\n",
        "\n",
        "start_df = pd.DataFrame(rows, columns=cols)\n",
        "\n"
      ],
      "metadata": {
        "id": "WBzTbo46eIfW"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.13: Display the pandas data frame\n",
        "start_df"
      ],
      "metadata": {
        "id": "DjHdv40IeITw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "4bed0acc-4eef-4e47-fbe7-72d4ddf11438"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   start  end\n",
              "0      0   36\n",
              "1     36   67\n",
              "2     67  103\n",
              "3    103  139"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a3bcae2d-4d3d-47e5-baba-b077617cb1ae\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>36</td>\n",
              "      <td>67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>67</td>\n",
              "      <td>103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>103</td>\n",
              "      <td>139</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a3bcae2d-4d3d-47e5-baba-b077617cb1ae')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a3bcae2d-4d3d-47e5-baba-b077617cb1ae button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a3bcae2d-4d3d-47e5-baba-b077617cb1ae');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.14: Iterate through the data frame and print the first token \n",
        "#         from each sentence.\n",
        "\n",
        "for i,row in start_df.iterrows():\n",
        "  f_token=doc[row['start']]\n",
        "  print(f_token)"
      ],
      "metadata": {
        "id": "-nVLymzGeIGA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc0e7b70-e8b2-4d89-c84c-286f85c01a31"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence\n",
            "Natural\n",
            "In\n",
            "About\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3SWM52scVZK"
      },
      "source": [
        "# Part of Speech Tagging\n",
        "\n",
        "POS tagging is a critical processing step in most pipelines, and much more difficult in some languages (e.g., Chinese) than in others. The standard spaCy pipeline always includes a \"tagger\" that assigns POS tags based on a statistical analysis of a training corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bIdEjLocVZK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6d1627e-6e76-4657-9275-e37ada7fc700"
      },
      "source": [
        "doc = nlp(\"I was reading an article about Berkeley Avenue in Reading, which was closed due to a police investigation.\")\n",
        "type(doc), len(doc)"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(spacy.tokens.doc.Doc, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbZPfIWacVZK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a208124b-cff4-46a7-9a2a-891f09b408aa"
      },
      "source": [
        "from tabulate import tabulate # To make a neat table\n",
        "\n",
        "tabdata = [ (token, token.tag_, token.pos_, spacy.explain(token.tag_)) for token in doc]\n",
        "\n",
        "print(tabulate(tabdata,  headers=[\"Token\", \"Token Tag\", \"POS\", \"Explanation\"]))\n"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token          Token Tag    POS    Explanation\n",
            "-------------  -----------  -----  -----------------------------------------\n",
            "I              PRP          PRON   pronoun, personal\n",
            "was            VBD          AUX    verb, past tense\n",
            "reading        VBG          VERB   verb, gerund or present participle\n",
            "an             DT           DET    determiner\n",
            "article        NN           NOUN   noun, singular or mass\n",
            "about          IN           ADP    conjunction, subordinating or preposition\n",
            "Berkeley       NNP          PROPN  noun, proper singular\n",
            "Avenue         NNP          PROPN  noun, proper singular\n",
            "in             IN           ADP    conjunction, subordinating or preposition\n",
            "Reading        NNP          PROPN  noun, proper singular\n",
            ",              ,            PUNCT  punctuation mark, comma\n",
            "which          WDT          PRON   wh-determiner\n",
            "was            VBD          AUX    verb, past tense\n",
            "closed         VBN          VERB   verb, past participle\n",
            "due            IN           ADP    conjunction, subordinating or preposition\n",
            "to             IN           ADP    conjunction, subordinating or preposition\n",
            "a              DT           DET    determiner\n",
            "police         NN           NOUN   noun, singular or mass\n",
            "investigation  NN           NOUN   noun, singular or mass\n",
            ".              .            PUNCT  punctuation mark, sentence closer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "497I80HCoK-m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0124db13-80c5-45e4-d580-3cf885e02f86"
      },
      "source": [
        "# Make a list of tokens for all of the proper nouns\n",
        "propnlist = [token for token in doc if token.pos_ == \"PROPN\"]\n",
        "\n",
        "[ (token, token.is_ascii, token.is_title) for token in propnlist]"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(Berkeley, True, True), (Avenue, True, True), (Reading, True, True)]"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hck-VPXBqtYb"
      },
      "source": [
        "# Write another sentence that includes a place name. Tokenize it,\n",
        "# display the POS tags, and excerpt the proper noun(s).\n",
        "\n",
        "# 5.15: Create a new text object for tokenizing.\n",
        "p_doc = nlp(\"The Australian territory of Norfolk Island was placed under a red alert as Gabrielle approached, with the center of the storm forecast to pass over or close to the island.\")\n"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.16: Tokenize the text object.\n",
        "tabdata = [ (token, token.tag_, token.pos_, spacy.explain(token.tag_)) for token in p_doc]\n"
      ],
      "metadata": {
        "id": "9D0ZCeESeagv"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.17: Display the POS tags for all tokens.\n",
        "print(tabulate(tabdata,  headers=[\"Token\", \"Token Tag\", \"POS\", \"Explanation\"]))\n"
      ],
      "metadata": {
        "id": "Ri8avzyseaOg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aee313f1-a502-4b85-bef4-0db73d6b5a8b"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token       Token Tag    POS    Explanation\n",
            "----------  -----------  -----  --------------------------------------------------\n",
            "The         DT           DET    determiner\n",
            "Australian  JJ           ADJ    adjective (English), other noun-modifier (Chinese)\n",
            "territory   NN           NOUN   noun, singular or mass\n",
            "of          IN           ADP    conjunction, subordinating or preposition\n",
            "Norfolk     NNP          PROPN  noun, proper singular\n",
            "Island      NNP          PROPN  noun, proper singular\n",
            "was         VBD          AUX    verb, past tense\n",
            "placed      VBN          VERB   verb, past participle\n",
            "under       IN           ADP    conjunction, subordinating or preposition\n",
            "a           DT           DET    determiner\n",
            "red         JJ           ADJ    adjective (English), other noun-modifier (Chinese)\n",
            "alert       NN           NOUN   noun, singular or mass\n",
            "as          IN           SCONJ  conjunction, subordinating or preposition\n",
            "Gabrielle   NNP          PROPN  noun, proper singular\n",
            "approached  VBD          VERB   verb, past tense\n",
            ",           ,            PUNCT  punctuation mark, comma\n",
            "with        IN           ADP    conjunction, subordinating or preposition\n",
            "the         DT           DET    determiner\n",
            "center      NN           NOUN   noun, singular or mass\n",
            "of          IN           ADP    conjunction, subordinating or preposition\n",
            "the         DT           DET    determiner\n",
            "storm       NN           NOUN   noun, singular or mass\n",
            "forecast    VBN          VERB   verb, past participle\n",
            "to          TO           PART   infinitival \"to\"\n",
            "pass        VB           VERB   verb, base form\n",
            "over        RB           ADV    adverb\n",
            "or          CC           CCONJ  conjunction, coordinating\n",
            "close       JJ           ADJ    adjective (English), other noun-modifier (Chinese)\n",
            "to          IN           ADP    conjunction, subordinating or preposition\n",
            "the         DT           DET    determiner\n",
            "island      NN           NOUN   noun, singular or mass\n",
            ".           .            PUNCT  punctuation mark, sentence closer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.18: Extract the proper nouns and display them.\n",
        "propnlist = [token for token in p_doc if token.pos_ == \"PROPN\"]\n",
        "[ (token, token.is_ascii, token.is_title) for token in propnlist]"
      ],
      "metadata": {
        "id": "iUlCPKgVeaB5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d86fa8f2-c32c-4e11-8ff0-77d7255515c7"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(Norfolk, True, True), (Island, True, True), (Gabrielle, True, True)]"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KXiIBy4cVZK"
      },
      "source": [
        "# Dependency Parsing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxQ23QB3cVZK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb147dcf-70eb-4205-d701-72f6ddc49220"
      },
      "source": [
        "# Let's take a closer look at the dependency structure:\n",
        "from tabulate import tabulate # To make a neat table\n",
        "\n",
        "tabdata = [ (token.text, token.tag_, token.dep_, token.head.text, token.head.tag_) for token in doc]\n",
        "\n",
        "print(tabulate(tabdata,  headers=[\"Token\", \"Token POS\", \"Dependency\", \"Head Token\", \"Head POS\"]))\n"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token          Token POS    Dependency    Head Token     Head POS\n",
            "-------------  -----------  ------------  -------------  ----------\n",
            "I              PRP          nsubj         reading        VBG\n",
            "was            VBD          aux           reading        VBG\n",
            "reading        VBG          ROOT          reading        VBG\n",
            "an             DT           det           article        NN\n",
            "article        NN           dobj          reading        VBG\n",
            "about          IN           prep          article        NN\n",
            "Berkeley       NNP          compound      Avenue         NNP\n",
            "Avenue         NNP          pobj          about          IN\n",
            "in             IN           prep          Avenue         NNP\n",
            "Reading        NNP          pobj          in             IN\n",
            ",              ,            punct         Avenue         NNP\n",
            "which          WDT          nsubjpass     closed         VBN\n",
            "was            VBD          auxpass       closed         VBN\n",
            "closed         VBN          relcl         Avenue         NNP\n",
            "due            IN           prep          closed         VBN\n",
            "to             IN           prep          due            IN\n",
            "a              DT           det           investigation  NN\n",
            "police         NN           compound      investigation  NN\n",
            "investigation  NN           pobj          due            IN\n",
            ".              .            punct         reading        VBG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PszlZv3HnG6"
      },
      "source": [
        "Take a close look at the output just above. For each token in the sentence, the text of the token is shown along with its part of speech. Then the dependency relation is shown. For example, the first token, \"I\", is the noun/subject of the sentence and is therefore dependent on the main verb, \"reading\", which is the gerund form of the verb to read. \n",
        "\n",
        "Take the time to examine each row of the output and make sure you understand the dependency relation that is being documented. And remember that spaCy's ability to diagram the relations in this way works because of the language model we originally loaded: \"en_core_web_sm\". Also important: The default sentence segmentation that we examined in a previous block works because spaCy's dependency parser accounts for all of the elements in a sentence, and therefore \"knows\" when the period character is closing a sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a_nCqwbJ3sV"
      },
      "source": [
        "# Grab another sentence from the web, but this time, cut off the sentence\n",
        "# before the end so that some key grammatical element is missing. Do paste\n",
        "# a period on the end, though, just to see if you can confuse spaCy.\n",
        "\n",
        "# 5.19: Cut and paste part of a sentence from the web into a text variable.\n",
        "text_var=nlp(\"\"\"Australian military and emergency personnel were on standby ready to.\"\"\")"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.20: Tokenize the sentence.\n",
        "tabdata = [ (token.text, token.tag_, token.dep_, token.head.text, token.head.tag_) for token in text_var]\n"
      ],
      "metadata": {
        "id": "rmfC_dDTeqtr"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.21: Generate a table showing the dependency relations in the sentence.\n",
        "print(tabulate(tabdata,  headers=[\"Token\", \"Token POS\", \"Dependency\", \"Head Token\", \"Head POS\"]))"
      ],
      "metadata": {
        "id": "UaaNJLYLeqgH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c621cb7a-49b2-4a78-d9a2-e924e20d84b8"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token       Token POS    Dependency    Head Token    Head POS\n",
            "----------  -----------  ------------  ------------  ----------\n",
            "Australian  JJ           amod          military      NN\n",
            "military    NN           nsubj         were          VBD\n",
            "and         CC           cc            military      NN\n",
            "emergency   NN           compound      personnel     NNS\n",
            "personnel   NNS          conj          military      NN\n",
            "were        VBD          ROOT          were          VBD\n",
            "on          IN           prep          were          VBD\n",
            "standby     RB           advmod        ready         JJ\n",
            "ready       JJ           amod          on            IN\n",
            "to          TO           xcomp         ready         JJ\n",
            ".           .            punct         were          VBD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.22: Add a comment to document any mistakes that spaCy made.\n",
        "\n",
        "#I deleted the last word - \"respond\" so spacy cannot detect \"ready to respond\" dependency\n",
        "#It is not able to create a dependency for \"ready\"\n",
        "#Ready is tagged as amod dependency - adjectival modifier to \"on\"\n",
        "#It should have actually been amod to \"respond\" which was deleted "
      ],
      "metadata": {
        "id": "JBklSoF8eqPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "1pOdKqdIcVZK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "cb3f788a-35f8-470e-9963-410f13e9a0e4"
      },
      "source": [
        "# As we saw in a previous lab, there is a graphical display module for \n",
        "# spaCy that supports drawing a figure of the dependency relations.\n",
        "from spacy import displacy\n",
        "displacy.render(text_var, style='dep', jupyter=True, options={'distance': 90})\n",
        "\n",
        "#Spacy is not making a mistake with the existing dependencies. "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"de9293145ed5430b828190a03ec2931c-0\" class=\"displacy\" width=\"950\" height=\"272.0\" direction=\"ltr\" style=\"max-width: none; height: 272.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Australian</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"140\">military</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"140\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"230\">and</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"230\">CCONJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"320\">emergency</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"320\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">personnel</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">were</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"590\">on</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"590\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"680\">standby</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"680\">ADV</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">ready</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"860\">to.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"860\">PART</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-de9293145ed5430b828190a03ec2931c-0-0\" stroke-width=\"2px\" d=\"M70,137.0 C70,92.0 130.0,92.0 130.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-de9293145ed5430b828190a03ec2931c-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,139.0 L62,127.0 78,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-de9293145ed5430b828190a03ec2931c-0-1\" stroke-width=\"2px\" d=\"M160,137.0 C160,2.0 500.0,2.0 500.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-de9293145ed5430b828190a03ec2931c-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M160,139.0 L152,127.0 168,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-de9293145ed5430b828190a03ec2931c-0-2\" stroke-width=\"2px\" d=\"M160,137.0 C160,92.0 220.0,92.0 220.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-de9293145ed5430b828190a03ec2931c-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M220.0,139.0 L228.0,127.0 212.0,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-de9293145ed5430b828190a03ec2931c-0-3\" stroke-width=\"2px\" d=\"M340,137.0 C340,92.0 400.0,92.0 400.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-de9293145ed5430b828190a03ec2931c-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M340,139.0 L332,127.0 348,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-de9293145ed5430b828190a03ec2931c-0-4\" stroke-width=\"2px\" d=\"M160,137.0 C160,47.0 405.0,47.0 405.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-de9293145ed5430b828190a03ec2931c-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M405.0,139.0 L413.0,127.0 397.0,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-de9293145ed5430b828190a03ec2931c-0-5\" stroke-width=\"2px\" d=\"M520,137.0 C520,92.0 580.0,92.0 580.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-de9293145ed5430b828190a03ec2931c-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M580.0,139.0 L588.0,127.0 572.0,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-de9293145ed5430b828190a03ec2931c-0-6\" stroke-width=\"2px\" d=\"M700,137.0 C700,92.0 760.0,92.0 760.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-de9293145ed5430b828190a03ec2931c-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M700,139.0 L692,127.0 708,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-de9293145ed5430b828190a03ec2931c-0-7\" stroke-width=\"2px\" d=\"M610,137.0 C610,47.0 765.0,47.0 765.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-de9293145ed5430b828190a03ec2931c-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M765.0,139.0 L773.0,127.0 757.0,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-de9293145ed5430b828190a03ec2931c-0-8\" stroke-width=\"2px\" d=\"M790,137.0 C790,92.0 850.0,92.0 850.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-de9293145ed5430b828190a03ec2931c-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M850.0,139.0 L858.0,127.0 842.0,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRBkxxybK9Rq"
      },
      "source": [
        "Now that you can see the dependency relations as a graph, do you notice any problems with the parsing? Did spaCy make any mistakes in connecting the various elements of the sentence?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gl7OK86pcVZK"
      },
      "source": [
        "# Named Entity Recognition\n",
        "\n",
        "Whenever spaCy finds a token that looks like a proper noun, it tags it as a predicted named entity. \"Predicted,\" because each spaCy language model has a trained classifier that makes predictions of whether or not a token might be a named entity and also what type of entity it is (e.g., an organization, a country, or something else."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3YNPOeIcVZK"
      },
      "source": [
        "The IOB tagging method is a a straightforward way of notating the status of tokens. Tokens can have one of the following four statuses:\n",
        "\n",
        "| TAG | ID | DESCRIPTION                           |\n",
        "|:-----:|:----:|:---------------------------------------:|\n",
        "| I   | 1  | Token is inside an entity.            |\n",
        "| O   | 2  | Token is outside an entity.           |\n",
        "| B   | 3  | Token begins an entity.               |\n",
        "|     | 0  | No entity tag is set (missing value). |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFstPHGLcVZL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "392b2395-1226-4228-ed25-da42c775fc74"
      },
      "source": [
        "text='''We’re bringing the celebration of Syracuse University’s 150 years of impact to Chicago'''\n",
        "doc = nlp(text)\n",
        "type(doc.ents)"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tuple"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNspkgjLLwwU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "488fc15c-395e-40c8-bdde-227e2302eaca"
      },
      "source": [
        "# So the list of entities is a tuple, which means we should be able to slice it.\n",
        "doc.ents[0]"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Syracuse University’s"
            ]
          },
          "metadata": {},
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "pkKIjUmycVZL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c631a79c-3213-41f9-d47a-9384c6d39025"
      },
      "source": [
        "# We can iterate through all of the entities in the document.\n",
        "for ent in doc.ents:\n",
        "    print(\"{}, [{},{}), {}\".format(ent.text, ent.start_char, ent.end_char, ent.label_))\n",
        "    \n",
        "    # For each entity, we can also access each entity as a span and \n",
        "    # iterate through its tokens\n",
        "    for token in ent.as_doc():        \n",
        "        print(\"    {} {} {}\".format(token, token.ent_iob_, token.ent_type_))"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Syracuse University’s, [34,55), ORG\n",
            "    Syracuse B ORG\n",
            "    University I ORG\n",
            "    ’s I ORG\n",
            "150 years, [56,65), DATE\n",
            "    150 B DATE\n",
            "    years I DATE\n",
            "Chicago, [79,86), GPE\n",
            "    Chicago B GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checkpoint! - Write the starting and ending characters of \"Chicago\" on the board\n",
        "\n",
        "Notice that for each of the three entities above, there's a numeric expression that shows the start and end of the character span where the entities can be found in the original string. Write the two numbers for \"Chicago\" on the white board.\n",
        "\n",
        "In the output shown above, ORG and DATE are pretty clear, but what does GPE stand for? Do a web search on \"spacy GPE\" to find out."
      ],
      "metadata": {
        "id": "YLnC8emVgrgV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A62VfRrvcVZL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3c1d862d-b692-487d-ac0f-00eba7c0d508"
      },
      "source": [
        "# The displacy module can also provide a graphical view of the named entities:\n",
        "from spacy import displacy\n",
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">We’re bringing the celebration of \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Syracuse University’s\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    150 years\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " of impact to \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Chicago\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              "</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLgDucPuPm97"
      },
      "source": [
        "# Now write or find another sentence that contains some named entities.\n",
        "\n",
        "# 5.23: Assign a new sentence containing some entities into a text variable.\n",
        "text_var2 = \"Ther is nothing wrong with Tesla's new electric car in Atlanta's show tonight\""
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.24: Tokenize the sentence.\n",
        "doc_new=nlp(text_var2)\n",
        "for ent in doc_new.ents:\n",
        "    print(\"{}, [{},{}), {}\".format(ent.text, ent.start_char, ent.end_char, ent.label_))\n",
        "    \n",
        "    # For each entity, we can also access each entity as a span and \n",
        "    # iterate through its tokens\n",
        "    for token in ent.as_doc():        \n",
        "        print(\"    {} {} {}\".format(token, token.ent_iob_, token.ent_type_))"
      ],
      "metadata": {
        "id": "P7xkGmque9nW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed32cd6a-ea5d-468b-dc3e-37dd154a9729"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla, [27,32), ORG\n",
            "    Tesla B ORG\n",
            "Atlanta, [55,62), GPE\n",
            "    Atlanta B GPE\n",
            "tonight, [70,77), TIME\n",
            "    tonight B TIME\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.24: Generate a displacy graphic with the named entities.\n",
        "from spacy import displacy\n",
        "displacy.render(doc_new, style=\"ent\", jupyter=True)"
      ],
      "metadata": {
        "id": "RTAdXNIpe9aq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2fdbbc48-7277-41aa-dbeb-229ad9b687a5"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Ther is nothing wrong with \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Tesla\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "'s new electric car in \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Atlanta\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              "'s show \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    tonight\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TIME</span>\n",
              "</mark>\n",
              "</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dowsinw_cVZL"
      },
      "source": [
        "# Word Vectors\n",
        "\n",
        "Word vectors are numeric representations of words in multidimensional space. Notable implementations of word vectors include GloVe, a technique that obtains vectors from a large word co-occurence matrix developed from a corpus, and Word2vec, an predictive neural network training process that does repetitive, incremental training over strings of tokens in a corpus. Word vectors have the interesting property that semantically similar words appear close to each other in multidimensional space. Given any two word vectors, we can calculate the similarity (or distance) between them.\n",
        "\n",
        "SpaCy language models come in several sizes. Small models do not contain word vectors - a space saving strategy. So if we want to use word vectors, we need to load the medium or large version of a language model as shown in the next cell. Note that because of the size of a large language model file, the next code cell may take about three minutes to complete. That's a lot of time and therefore a lot of data. Imagine how this might impact the operation of a deployed system."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yo5eag8a4QXs"
      },
      "source": [
        "import spacy.cli # Use the command line interface\n",
        "spacy.cli.download(\"en_core_web_lg\") # This imports the large model onto your virtual machines\n",
        "import en_core_web_lg # Now that it is downloaded, we can import it\n",
        "nlp_lg = en_core_web_lg.load() # Create an instance for further use\n",
        "type(nlp_lg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVq9THWEcVZM"
      },
      "source": [
        "# Let's get the vectors for three words to examine similarities\n",
        "apple = nlp_lg.vocab[\"apple\"]\n",
        "banana = nlp_lg.vocab[\"banana\"]\n",
        "car = nlp_lg.vocab[\"car\"]\n",
        "\n",
        "print(apple.vector) # Take a look at one of them"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJcrJest6x1F"
      },
      "source": [
        "# A spaCy lexeme, with a variety of attribute tests available\n",
        "type(apple), [m for m in dir(apple) if m[0:3] == \"is_\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9iCCiYeCD7C"
      },
      "source": [
        "# We can also confirm that a lexeme has a vector representation available\n",
        "banana.has_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_oVPIMlcVZM"
      },
      "source": [
        "# The .similarity() bound method allows us to generate a cosine similarity\n",
        "# score for two vectors.\n",
        "apple.similarity(banana)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24_ra9OncVZM"
      },
      "source": [
        "# The similarity is transitive, and by the calculation spaCy uses, seems\n",
        "# to result in values in the interval 0 to 1. Higher values indicate\n",
        "# closer similarity.\n",
        "banana.similarity(apple)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1-1r1LocVZM"
      },
      "source": [
        "apple.similarity(car)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhqm7JfYErQm"
      },
      "source": [
        "# A scatterplot of two similar vectors should indicate linearity\n",
        "import matplotlib.pyplot as plt # Make a simple plot\n",
        "plt.scatter(apple.vector, banana.vector) # Show the result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvogroAkFq0y"
      },
      "source": [
        "# A scatterplot of two different vectors should be a circular cloud\n",
        "plt.scatter(apple.vector, car.vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsT_flvAcVZN"
      },
      "source": [
        "# There are many options for a cosine similarity calculation,\n",
        "# including this one from scipy that we are not using here.\n",
        "\n",
        "#from scipy.spatial.distance import cosine"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CG5_VaPRcVZN"
      },
      "source": [
        "# With some help from numpy, we can calculate our own cosine scores\n",
        "# using a vector dot product, computed with np.dot()\n",
        "import numpy as np\n",
        "def cosine(x,y):\n",
        "    return np.dot(x,y) / (np.sqrt(np.dot(x,x)) * np.sqrt(np.dot(y,y)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myRRByyLcVZN"
      },
      "source": [
        "# One of the notable aspects of word vectors is the possibility of\n",
        "# doing simple math to create analogies.\n",
        "man = nlp_lg.vocab[\"man\"].vector\n",
        "woman = nlp_lg.vocab[\"woman\"].vector\n",
        "king = nlp_lg.vocab[\"king\"].vector\n",
        "queen = nlp_lg.vocab[\"queen\"].vector\n",
        "\n",
        "analogy = king - man + woman # Calculate the analogy with vector math\n",
        "cosine(analogy, queen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mbY34p5cVZN"
      },
      "source": [
        "# Let's compare the analogy to another vector\n",
        "cosine(analogy, car.vector) # Note how we needed to extract the vector for car"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVAZeKqNDquc"
      },
      "source": [
        "# Now you compute an anology and see what kind of results you get\n",
        "\n",
        "# 5.25: Obtain the vectors for Paris, France, Berlin, and Germany\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.26: Compute an analogy using three of these vectors\n"
      ],
      "metadata": {
        "id": "h9iB3pW8fnqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.27: Find the cosine similarity of the analogy with the fourth vector\n"
      ],
      "metadata": {
        "id": "omXVYktqfnZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.28: Find the cosine similarity of the analogy with banana\n"
      ],
      "metadata": {
        "id": "Re8hWYEOfnMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd7ce2MmcVZN"
      },
      "source": [
        "# Sentence Similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSAyZ7V3cVZN"
      },
      "source": [
        "As you have seen above, word vectors for similar words have similar patterns and though we don't know what each position in a vector signifies, these patterns support simple vector math for analogical reasoning. This suggests the possibility - which research seems to back up - that averaging two or more vectors provides a semantic summary of the vectors. Document and span objects in spaCy have attached vectors that repesents the average of all of the component word vectors. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNe-3gFMcVZN"
      },
      "source": [
        "doc1 = nlp_lg(\"The quick brown fox jumps over the lazy dog.\")\n",
        "doc2 = nlp_lg(\"The lazy dog jumps over the quick brown fox.\")\n",
        "doc1.similarity(doc1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cjp3UxW9cVZN"
      },
      "source": [
        "# We can also manually calculate the cosine similarity. It should be transitive\n",
        "# such that cosine(A,B) == cosine(B,A)\n",
        "doc1_vec = doc1.vector\n",
        "doc2_vec = doc2.vector\n",
        "cosine(doc2_vec, doc1_vec), cosine(doc1_vec, doc2_vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KF4OZHoHcVZN"
      },
      "source": [
        "# Let's get the similarity of the first sentence (doc1) with a new sentence.\n",
        "docdiff = nlp_lg(\"Four score and seven years ago, our fathers brought forth upon this continent a new nation.\")\n",
        "cosine(docdiff.vector, doc1_vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8quCzXHcVZO"
      },
      "source": [
        "# The semantics can be quite muddled by the averaging process. These two\n",
        "# sentences express the opposite sentiment\n",
        "doc3 = nlp_lg(\"I like snow\")\n",
        "doc4 = nlp_lg(\"I hate snow\")\n",
        "cosine(doc3.vector, doc4.vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Think through the implication of the result shown above. By creating an averaged word vector, we have given every word in a sentence the same weight in creating the new summary vector. Another exercise to try would be to take out the word snow and see how that changes the results."
      ],
      "metadata": {
        "id": "rMgDhnutf4UJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.29: Recalculate the previous cell after removing the word snow from both sentences.\n",
        "# Feel free to try any other experiments of interest. "
      ],
      "metadata": {
        "id": "2S0Ea8B9gTrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9us6kCycVZO"
      },
      "source": [
        "# And here we see part of why this is so.\n",
        "plt.scatter(nlp_lg.vocab[\"hate\"].vector, nlp_lg.vocab[\"like\"].vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tH0AAo2I4eW"
      },
      "source": [
        "# Now you create vector summaries of three sentences and look at the\n",
        "# cosine similarities for each pair.\n",
        "\n",
        "# 5.29: Create nlp_lg() objects for three sentences that you write yourself or\n",
        "# copy and paste from the web.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.30: Compute pairwise similarities for their vector summaries\n"
      ],
      "metadata": {
        "id": "uqy7MrhagfWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.31: Add a comment explaining what you see\n"
      ],
      "metadata": {
        "id": "YY2p4Zo1gg0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Putting it All Together\n",
        "\n",
        "If there is time left in the lab, put everything you have learned from the exercises above to use in processing two Wikipedia articles that discuss similar topics (for example, the Super Bowl and the Rose Bowl). Extract the text from two Wikipedia articles using the same code that you used for Homework 2. Then process both texts with spaCy. Extract all of the named entities and see which named entities the two articles have in common. Calculate a ratio of matching named entities to all detected entities."
      ],
      "metadata": {
        "id": "PNomt0IBgrMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.32: Extract Named Entities from Two Wikipedia Articles and Find Matches\n"
      ],
      "metadata": {
        "id": "EQJ5SJ_8h0By"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwo7l9BrcVZO"
      },
      "source": [
        "# Optional Advanced Topic: Processing pipelines\n",
        "\n",
        "Throughout this lab, we have been calling a function that we often referred to as nlp(). After loading a spaCy language model such as en_core_web_sm, we instantiate a pipeline object to conduct all of the steps that we will routinely want to accomplish with a document. \n",
        "\n",
        "The spaCy pipeline can be modified to change the default components or to add new components. Here's a list of the default components from the spaCy documentation:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMTUsVP5cVZP"
      },
      "source": [
        "| NAME      | COMPONENT         | CREATES                                             | DESCRIPTION                                      |\n",
        "|:-----------|:-------------------|:-----------------------------------------------------|:--------------------------------------------------|\n",
        "| tokenizer | Tokenizer         | Doc                                                 | Segment text into tokens.                        |\n",
        "| tagger    | Tagger            | Doc[i].tag                                          | Assign part-of-speech tags.                      |\n",
        "| parser    | DependencyParser  | Doc[i].head, Doc[i].dep, Doc.sents, Doc.noun_chunks | Assign dependency labels.                        |\n",
        "| ner       | EntityRecognizer  | Doc.ents, Doc[i].ent_iob, Doc[i].ent_type           | Detect and label named entities.                 |\n",
        "| textcat   | TextCategorizer   | Doc.cats                                            | Assign document labels.                          |\n",
        "| …         | custom components | `Doc._.xxx`, `Token._.xxx`, `Span._.xxx`                  | Assign custom attributes, methods or properties. |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BENxiG7XA3Qo"
      },
      "source": [
        "# We can also examine the pipeline for an instantiated object like this:\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "nlp.pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfta7uAsCxLv"
      },
      "source": [
        "You may notice in the list above that there is no tokenizer. In the spaCy pipeline model, it is assumed that tokenization was accomplished before pipeline processing begins. All pipeline elements receive a doc object, work on it and return a doc object. The tokenizer has a different kind of job becuase it receives a raw character string and returns a list of tokens. Thus, the language object has a different slot where the tokenizer is listed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi7w8BKTDF60"
      },
      "source": [
        "nlp.tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMnmTk3qDTfS"
      },
      "source": [
        "The spaCy pipeline was designed to balance simplicity and computational effort. Simplicity is important for getting started quickly with a language processing tasks, so the default pipeline contains all the stuff that most people need to address a realistic task. But if a component is not needed, it can save a lot of compute time to take a task out of the pipeline. Take a look at this example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7Z0Pp-4IB96"
      },
      "source": [
        "print(spacy.__version__) # Some version dependent stuff below"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hm6zUJcuD00G"
      },
      "source": [
        "# Let's skip the entity recognition and the dependency parsing\n",
        "nlp_simple = spacy.load(\"en_core_web_sm\", exclude=[\"parser\",\"ner\"])\n",
        "# Version 3 also adds facilities for enabling\n",
        "# and disabling pipeline elements on the fly.\n",
        "\n",
        "print(nlp_simple.pipeline) # SHow the pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYpDStT3IbTC"
      },
      "source": [
        "# Groucho Marx is an entity, but this pipeline doesn't detect it, because the\n",
        "# ner is disabled.\n",
        "simple_doc = nlp_simple(\"Groucho Marx shot an elephant in his underpants.\")\n",
        "\n",
        "[ent for ent in simple_doc.ents]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-Kj7lvGIgGY"
      },
      "source": [
        "# And the pipeline did not do dependency parsing\n",
        "[token.dep_ for token in simple_doc]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9ZKq8QkKSLy"
      },
      "source": [
        "# 5.32: Create a pipeline that contains a dependency parser, but no \n",
        "#       entity recognition. Process a sentence and show that the entity\n",
        "#       recognition is disabled.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kl9G0O6cVZR"
      },
      "source": [
        "# Adding Custom Pipeline Components\n",
        "\n",
        "A component receives a Doc object and can modify it. By adding a component to the pipeline, you’ll get access to the Doc at any point during processing – instead of only being able to modify it afterwards. You can control the position of the new component in the pipeline with the last, first, before, and after arguments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIywcoktcVZR"
      },
      "source": [
        "| ARGUMENT | TYPE | DESCRIPTION                                          |\n",
        "|----------|------|------------------------------------------------------|\n",
        "| doc      | Doc  | The Doc object processed by the previous component.  |\n",
        "| RETURNS  | Doc  | The Doc object processed by this pipeline component. |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTfX6L74cVZR"
      },
      "source": [
        "| ARGUMENT | TYPE    | DESCRIPTION                                                        |\n",
        "|----------|---------|--------------------------------------------------------------------|\n",
        "| last     | bool    | If set to True, component is added last in the pipeline (default). |\n",
        "| first    | bool    | If set to True, component is added first in the pipeline.          |\n",
        "| before   | unicode | String name of component to add the new component before.          |\n",
        "| after    | unicode | String name of component to add the new component after.           |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paTYGmxmcVZR"
      },
      "source": [
        "import spacy\n",
        "from spacy.tokens import Doc, Span, Token\n",
        "import json\n",
        "from spacy.language import Language\n",
        "\n",
        "# Here's a custom function for removing stopwords, starting with a decorator\n",
        "@Language.component(\"remstop_component\")\n",
        "def remove_stopwords(doc):\n",
        "    # A pipeline element would not normally contain this kind of diagnostic\n",
        "    # but we just want to show what the method received before processing.\n",
        "    print(\"Before stopwords_removal, this doc is: {}\".format(doc))\n",
        "    space_list = [t.whitespace_  for t in doc if not t.is_stop]\n",
        "    new_doc = Doc(doc.vocab,\n",
        "              words=[t.orth_ for t in doc if not t.is_stop],\n",
        "              spaces=space_list\n",
        "              )\n",
        "    return new_doc\n",
        "\n",
        "# Instantiate a default pipeline\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Add our stopword finder/remover\n",
        "nlp.add_pipe(\"remstop_component\", name=\"stopwords_removal\", first=True)\n",
        "\n",
        "# Show the pipeline\n",
        "print(nlp.pipe_names)  # ['stopwords_removal', 'tagger', 'parser', 'ner']\n",
        "\n",
        "# Process a sentence\n",
        "doc = nlp(\"This is a sentence.\")\n",
        "\n",
        "# See the result\n",
        "print(\"After stopwords_removal, this doc is: {}\".format(doc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHFJHfM6cVZS"
      },
      "source": [
        "# Attribute and Method Extensions\n",
        "\n",
        "For advanced users, it is possible to add new attributes and methods to spaCy objects. In a complex language processing system, additional attributes and methods could be used to annotate, control, and modify specialized features of a document or other spaCy object. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1aH52Q9cVZS"
      },
      "source": [
        "# Here we set a new attribute, called \"classified\" to indicate whether \n",
        "# a document's contents should be kept secret. \n",
        "Doc.set_extension(\"classified\", default=True)\n",
        "assert doc._.classified\n",
        "\n",
        "# Note that this block will throw an error if it is run more than once"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57QFD1UCNLyf"
      },
      "source": [
        "# Now we can set or retrieve the attribute\n",
        "doc._.classified = False\n",
        "doc._.classified"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mML9oyAqcVZT"
      },
      "source": [
        "# Here we set an extension that is a method rather than an attribute\n",
        "# Remember that a lambda function is a temporary function that we can use\n",
        "# to perform small tasks without having to use def.\n",
        "Doc.set_extension(\"experiment\", method=lambda doc, name: \"X_{}\".format(name))\n",
        "assert doc._.experiment(\"plane\") == \"X_plane\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-N6ESb3OLIx"
      },
      "source": [
        "# 5.33: Set a new extension method that reverses the letters of the input.\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}